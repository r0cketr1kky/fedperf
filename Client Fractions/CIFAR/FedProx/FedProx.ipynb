{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FedProx_CIFAR.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-0_nKpfq2h1R",
        "NY4eWzGiL6Mj",
        "jCOONkg-zV7Y",
        "ctjRsETiO1qO",
        "EUP_bcedcLwB",
        "XOchMapqvV_7",
        "hGqruzfzvV_8",
        "tUYyb4T-uXmF",
        "g_wYSRsdJ6s_",
        "Ri0FqXFeHW-V",
        "qfmcYUGWIwwV",
        "J8jzEyO0iywz"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgBTHvvzO-JD"
      },
      "source": [
        "# FedPerf - CIFAR + FedProx algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQR4DrZZDMy4"
      },
      "source": [
        "BASE_PATH = '/content/drive/MyDrive/FedPerf/CIFAR/FedProx'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKcpjZLrQQJV"
      },
      "source": [
        "%%capture output\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    import os\n",
        "    os.makedirs(BASE_PATH, exist_ok=True)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "except:\n",
        "    path = './'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0_nKpfq2h1R"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLLNM9X2JbQ8"
      },
      "source": [
        "%load_ext tensorboard\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms, utils, datasets\n",
        "from torchsummary import summary\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Check assigned GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "# set manual seed for reproducibility\n",
        "seed = 42\n",
        "\n",
        "# general reproducibility\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# gpu training specific\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY4eWzGiL6Mj"
      },
      "source": [
        "## Load the CIFAR Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc-HQXANvV_u"
      },
      "source": [
        "# torch.cuda.set_device('cuda:1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G649tjTXLL8F"
      },
      "source": [
        "# create transforms\n",
        "# We will just convert to tensor and normalize since no special transforms are mentioned in the paper\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Normalize the test set same as training set without augmentation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "cifar_data_train = datasets.CIFAR10('../data/cifar10/', train=True, download=True, transform=transform_train)\n",
        "cifar_data_test = datasets.CIFAR10('../data/cifar10/', train=False, download=True, transform=transform_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOtLLX66b1Ad"
      },
      "source": [
        "classes = np.array(list(cifar_data_train.class_to_idx.values()))\n",
        "classes_test = np.array(list(cifar_data_test.class_to_idx.values()))\n",
        "num_classes = len(classes_test)\n",
        "print(\"Classes: {} \\tType: {}\".format(classes, type(classes)))\n",
        "print(\"Classes Test: {} \\tType: {}\".format(classes_test, type(classes)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa4TJs-AVJ2O"
      },
      "source": [
        "print(\"Image Shape: {}\".format(cifar_data_train.data[0].shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCOONkg-zV7Y"
      },
      "source": [
        "## Visualize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9MK03TZw6Qs"
      },
      "source": [
        "def imshow(img):\n",
        "\t#img = img/2 + 0.5 #unnormalize the image\n",
        "\tplt.imshow(img, cmap='hsv') # convert from tensor to image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMJ0Kx4Kw-_H"
      },
      "source": [
        "def visualize(dataset):\n",
        "  figure = plt.figure(figsize=(25,4))\n",
        "  for i in range(20):\n",
        "    axis = figure.add_subplot(2, 20/2, i+1, xticks=[], yticks=[])\n",
        "    data = dataset.data[i]\n",
        "    #data = data.numpy()\n",
        "\n",
        "    target = dataset.targets[i]\n",
        "    #target = target.numpy()\n",
        "    imshow(data)\n",
        "    axis.set_title(target)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bPOwKg10Ro7"
      },
      "source": [
        "visualize(cifar_data_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKoh5Cf70UYu"
      },
      "source": [
        "visualize(cifar_data_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctjRsETiO1qO"
      },
      "source": [
        "## Partitioning the Data (IID and non-IID)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_v8lyrgO5dD"
      },
      "source": [
        "def iid_partition(dataset, clients):\n",
        "  \"\"\"\n",
        "  I.I.D paritioning of data over clients\n",
        "  Shuffle the data\n",
        "  Split it between clients\n",
        "  \n",
        "  params:\n",
        "    - dataset (torch.utils.Dataset): Dataset containing the CIFAR Images\n",
        "    - clients (int): Number of Clients to split the data between\n",
        "\n",
        "  returns:\n",
        "    - Dictionary of image indexes for each client\n",
        "  \"\"\"\n",
        "\n",
        "  num_items_per_client = int(len(dataset)/clients)\n",
        "  client_dict = {}\n",
        "  image_idxs = [i for i in range(len(dataset))]\n",
        "\n",
        "  for i in range(clients):\n",
        "    client_dict[i] = set(np.random.choice(image_idxs, num_items_per_client, replace=False))\n",
        "    image_idxs = list(set(image_idxs) - client_dict[i])\n",
        "\n",
        "  return client_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zMdliGMQoSl"
      },
      "source": [
        "def non_iid_partition(dataset, clients, total_shards, shards_size, num_shards_per_client):\n",
        "  \"\"\"\n",
        "  non I.I.D parititioning of data over clients\n",
        "  Sort the data by the digit label\n",
        "  Divide the data into N shards of size S\n",
        "  Each of the clients will get X shards\n",
        "\n",
        "  params:\n",
        "    - dataset (torch.utils.Dataset): Dataset containing the CIFAR Images\n",
        "    - clients (int): Number of Clients to split the data between\n",
        "    - total_shards (int): Number of shards to partition the data in\n",
        "    - shards_size (int): Size of each shard \n",
        "    - num_shards_per_client (int): Number of shards of size shards_size that each client receives\n",
        "\n",
        "  returns:\n",
        "    - Dictionary of image indexes for each client\n",
        "  \"\"\"\n",
        "  \n",
        "  shard_idxs = [i for i in range(total_shards)]\n",
        "  client_dict = {i: np.array([], dtype='int64') for i in range(clients)}\n",
        "  idxs = np.arange(len(dataset))\n",
        "  data_labels = np.array(dataset.targets)\n",
        "\n",
        "  # sort the labels\n",
        "  label_idxs = np.vstack((idxs, data_labels))\n",
        "  label_idxs = label_idxs[:, label_idxs[1,:].argsort()]\n",
        "  idxs = label_idxs[0,:]\n",
        "\n",
        "  # divide the data into total_shards of size shards_size\n",
        "  # assign num_shards_per_client to each client\n",
        "  for i in range(clients):\n",
        "    rand_set = set(np.random.choice(shard_idxs, num_shards_per_client, replace=False))\n",
        "    shard_idxs = list(set(shard_idxs) - rand_set)\n",
        "\n",
        "    for rand in rand_set:\n",
        "      client_dict[i] = np.concatenate((client_dict[i], idxs[rand*shards_size:(rand+1)*shards_size]), axis=0)\n",
        "  \n",
        "  return client_dict\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUP_bcedcLwB"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvoDNFKbZST5"
      },
      "source": [
        "class CIFAR_MLP(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CIFAR_MLP, self).__init__()\n",
        "    self.fc1 = nn.Linear(3 * 32 * 32, 2634)\n",
        "    self.fc2 = nn.Linear(2634, 2196) \n",
        "    self.fc3 = nn.Linear(2196, 1758)\n",
        "    self.fc4 = nn.Linear(1758, 1320)\n",
        "    self.fc5 = nn.Linear(1320, 882)\n",
        "    self.fc6 = nn.Linear(882, 444) \n",
        "    self.fc7 = nn.Linear(444, 10) \n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(-1, 3 * 32 * 32)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.relu(self.fc3(x))\n",
        "    x = F.relu(self.fc4(x))\n",
        "    x = F.relu(self.fc5(x))\n",
        "    x = F.relu(self.fc6(x))\n",
        "\n",
        "    x = self.fc7(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut1hZ8x3qYPZ"
      },
      "source": [
        "class CIFAR_CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CIFAR_CNN, self).__init__()\n",
        "\n",
        "    self.conv_layer = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "      nn.BatchNorm2d(32),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "      nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "      nn.BatchNorm2d(128),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      nn.Dropout2d(p=0.05),\n",
        "\n",
        "      nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "      nn.BatchNorm2d(256),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    )\n",
        "\n",
        "    self.fc_layer = nn.Sequential(\n",
        "      nn.Dropout(p=0.1),\n",
        "      nn.Linear(4096, 1024),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Linear(1024, 512),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Dropout(p=0.1),\n",
        "      nn.Linear(512, 10)\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):      \n",
        "    # conv layers\n",
        "    x = self.conv_layer(x)\n",
        "    \n",
        "    # flatten\n",
        "    x = x.view(x.size(0), -1)\n",
        "    \n",
        "    # fc layer\n",
        "    x = self.fc_layer(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVv4HA9HuLtr"
      },
      "source": [
        "### Print Model Summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H2jR2-Id4AA"
      },
      "source": [
        "cifar_mlp = CIFAR_MLP()\n",
        "cifar_cnn = CIFAR_CNN()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  cifar_mlp.cuda()\n",
        "  cifar_cnn.cuda()\n",
        "\n",
        "print(\"CIFAR MLP SUMMARY\")\n",
        "print(summary(cifar_mlp, (32,32,3)))\n",
        "\n",
        "print(\"\\nCIFAR CNN SUMMARY\")\n",
        "print(summary(cifar_cnn, (3, 32,32)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf_8XEXa-gZ7"
      },
      "source": [
        "## FedProx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOchMapqvV_7"
      },
      "source": [
        "### Systems Heterogeneity Simulations\n",
        "\n",
        "Generate epochs for selected clients based on percentage of devices that corresponds to heterogeneity. \n",
        "\n",
        "Assign x number of epochs (chosen unifirmly at random between [1, E]) to 0%, 50% or 90% of the selected devices, respectively. Settings where 0% devices perform fewer than E epochs of work correspond to the environments without system heterogeneity, while 90% of the devices sending their partial solutions corresponds to highly heterogenous system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRQZRPaZvV_7"
      },
      "source": [
        "def GenerateLocalEpochs(percentage, size, max_epochs):\n",
        "  ''' Method generates list of epochs for selected clients\n",
        "  to replicate system heteroggeneity\n",
        "\n",
        "  Params:\n",
        "    percentage: percentage of clients to have fewer than E epochs\n",
        "    size:       total size of the list\n",
        "    max_epochs: maximum value for local epochs\n",
        "  \n",
        "  Returns:\n",
        "    List of size epochs for each Client Update\n",
        "\n",
        "  '''\n",
        "\n",
        "  # if percentage is 0 then each client runs for E epochs\n",
        "  if percentage == 0:\n",
        "      return np.array([max_epochs]*size)\n",
        "  else:\n",
        "    # get the number of clients to have fewer than E epochs\n",
        "    heterogenous_size = int((percentage/100) * size)\n",
        "\n",
        "    # generate random uniform epochs of heterogenous size between 1 and E\n",
        "    epoch_list = np.random.randint(1, max_epochs, heterogenous_size)\n",
        "\n",
        "    # the rest of the clients will have E epochs\n",
        "    remaining_size = size - heterogenous_size\n",
        "    rem_list = [max_epochs]*remaining_size\n",
        "\n",
        "    epoch_list = np.append(epoch_list, rem_list, axis=0)\n",
        "    \n",
        "    # shuffle the list and return\n",
        "    np.random.shuffle(epoch_list)\n",
        "\n",
        "    return epoch_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGqruzfzvV_8"
      },
      "source": [
        "## Federated Averaging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-aNdQsQ-Kvp"
      },
      "source": [
        "### Local Training (Client Update)\n",
        "\n",
        "Local training for the model on client side"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX6OsQyO-Gz7"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, dataset, idxs):\n",
        "      self.dataset = dataset\n",
        "      self.idxs = list(idxs)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.idxs)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "      image, label = self.dataset[self.idxs[item]]\n",
        "      return image, label\n",
        "\n",
        "\n",
        "class ClientUpdate(object):\n",
        "  def __init__(self, dataset, batchSize, learning_rate, epochs, idxs, mu, algorithm):\n",
        "    self.train_loader = DataLoader(CustomDataset(dataset, idxs), batch_size=batchSize, shuffle=True)\n",
        "    self.algorithm = algorithm\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epochs = epochs\n",
        "\n",
        "  def train(self, model):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    proximal_criterion = nn.MSELoss(reduction='mean')\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.5)\n",
        "\n",
        "    # use the weights of global model for proximal term calculation\n",
        "    global_model = copy.deepcopy(model)\n",
        "\n",
        "    # calculate local training time\n",
        "    start_time = time.time()\n",
        "\n",
        "\n",
        "    e_loss = []\n",
        "    for epoch in range(1, self.epochs+1):\n",
        "\n",
        "      train_loss = 0.0\n",
        "\n",
        "      model.train()\n",
        "      for data, labels in self.train_loader:\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "        # clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # make a forward pass\n",
        "        output = model(data)\n",
        "\n",
        "        # calculate the loss + the proximal term\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "        if self.algorithm == 'fedprox':\n",
        "          proximal_term = 0.0\n",
        "\n",
        "          # iterate through the current and global model parameters\n",
        "          for w, w_t in zip(model.parameters(), global_model.parameters()) :\n",
        "            # update the proximal term \n",
        "            #proximal_term += torch.sum(torch.abs((w-w_t)**2))\n",
        "            proximal_term += (w-w_t).norm(2)\n",
        "\n",
        "          loss = criterion(output, labels) + (mu/2)*proximal_term\n",
        "        else:\n",
        "          loss = criterion(output, labels)\n",
        "    \n",
        "        # do a backwards pass\n",
        "        loss.backward()\n",
        "        # perform a single optimization step\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "\n",
        "      # average losses\n",
        "      train_loss = train_loss/len(self.train_loader.dataset)\n",
        "      e_loss.append(train_loss)\n",
        "\n",
        "    total_loss = sum(e_loss)/len(e_loss)\n",
        "\n",
        "    return model.state_dict(), total_loss, (time.time() - start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ukgh1DVHE2Ds"
      },
      "source": [
        "### Server Side Training\n",
        "\n",
        "Following Algorithm 1 from the paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NF1e33BgpeL"
      },
      "source": [
        "def training(model, rounds, batch_size, lr, ds, ds_test, data_dict, C, K, E, mu, percentage, plt_title, plt_color, target_test_accuracy, algorithm=\"fedprox\", history=[], logdir='.'):\n",
        "  \"\"\"\n",
        "  Function implements the Federated Averaging Algorithm from the FedAvg paper.\n",
        "  Specifically, this function is used for the server side training and weight update\n",
        "\n",
        "  Params:\n",
        "    - model:           PyTorch model to train\n",
        "    - rounds:          Number of communication rounds for the client update\n",
        "    - batch_size:      Batch size for client update training\n",
        "    - lr:              Learning rate used for client update training\n",
        "    - ds:              Dataset used for training\n",
        "    - ds_test:         Dataset used for testing\n",
        "    - data_dict:       Type of data partition used for training (IID or non-IID)\n",
        "    - C:               Fraction of clients randomly chosen to perform computation on each round\n",
        "    - K:               Total number of clients\n",
        "    - E:               Number of training passes each client makes over its local dataset per round\n",
        "    - tb_writer_name:  Directory name to save the tensorboard logs\n",
        "  Returns:\n",
        "    - model:           Trained model on the server\n",
        "  \"\"\"\n",
        "\n",
        "  # global model weights\n",
        "  global_weights = model.state_dict()\n",
        "\n",
        "  # training loss\n",
        "  # train_accuracy = []\n",
        "  train_loss = []\n",
        "  test_acc = []\n",
        "  test_loss = []\n",
        "\n",
        "  # store last loss for convergence\n",
        "  last_loss = 0.0\n",
        "\n",
        "  # total time taken \n",
        "  total_time = 0\n",
        "  start = time.time()\n",
        "\n",
        "  print(f\"System heterogeneity set to {percentage}% stragglers.\\n\")\n",
        "  print(f\"Picking {max(int(C*K),1 )} random clients per round.\\n\")\n",
        "\n",
        "  for curr_round in range(1, rounds+1):\n",
        "    w, local_loss, lst_local_train_time = [], [], []\n",
        "\n",
        "    m = max(int(C*K), 1)\n",
        "\n",
        "    heterogenous_epoch_list = GenerateLocalEpochs(percentage, size=m, max_epochs=E)\n",
        "    heterogenous_epoch_list = np.array(heterogenous_epoch_list)\n",
        "\n",
        "    S_t = np.random.choice(range(K), m, replace=False)\n",
        "    S_t = np.array(S_t)\n",
        "    print('Clients: {}/{} -> {}'.format(len(S_t), K, S_t))\n",
        "    \n",
        "    # For Federated Averaging, drop all the clients that are stragglers\n",
        "    if algorithm == 'fedavg':\n",
        "      stragglers_indices = np.argwhere(heterogenous_epoch_list < E)\n",
        "      heterogenous_epoch_list = np.delete(heterogenous_epoch_list, stragglers_indices)\n",
        "      S_t = np.delete(S_t, stragglers_indices)\n",
        "\n",
        "    # for k, epoch in zip(S_t, heterogenous_epoch_list):\n",
        "    for i in tqdm(range(len(S_t))):\n",
        "      k = S_t[i]\n",
        "      epoch = heterogenous_epoch_list[i]\n",
        "      local_update = ClientUpdate(dataset=ds, batchSize=batch_size, learning_rate=lr, epochs=epoch, idxs=data_dict[k], mu=mu, algorithm=algorithm)\n",
        "      weights, loss, local_train_time = local_update.train(model=copy.deepcopy(model))\n",
        "\n",
        "      w.append(copy.deepcopy(weights))\n",
        "      local_loss.append(copy.deepcopy(loss))\n",
        "      lst_local_train_time.append(local_train_time)\n",
        "\n",
        "    # calculate time to update the global weights\n",
        "    global_start_time = time.time()\n",
        "\n",
        "    # updating the global weights\n",
        "    weights_avg = copy.deepcopy(w[0])\n",
        "    for k in weights_avg.keys():\n",
        "      for i in range(1, len(w)):\n",
        "        weights_avg[k] += w[i][k]\n",
        "\n",
        "      weights_avg[k] = torch.div(weights_avg[k], len(w))\n",
        "\n",
        "    global_weights = weights_avg\n",
        "    global_end_time = time.time()\n",
        "\n",
        "    # calculate total time \n",
        "    total_time += (global_end_time - global_start_time) + sum(lst_local_train_time)/len(lst_local_train_time)\n",
        "\n",
        "    # move the updated weights to our model state dict\n",
        "    model.load_state_dict(global_weights)\n",
        "\n",
        "    # loss\n",
        "    loss_avg = sum(local_loss) / len(local_loss)\n",
        "    print('Round: {}... \\tAverage Loss: {}'.format(curr_round, round(loss_avg, 3)))\n",
        "    train_loss.append(loss_avg)\n",
        "\n",
        "    # test accuracy\n",
        "    # criterion = nn.CrossEntropyLoss()\n",
        "    # test_accuracy_current, test_loss_current = testing(copy.deepcopy(model), ds_test, 128, criterion, num_classes, classes_test)\n",
        "    # test_accuracy.append(test_accuracy_current)\n",
        "    # test_loss.append(test_loss_current)\n",
        "\n",
        "    # testing\n",
        "    test_scores = testing(model, ds_test, batch_size * 2, nn.CrossEntropyLoss(), num_classes, classes_test)\n",
        "    test_scores['train_loss'] = loss_avg\n",
        "    t_loss, t_accuracy = test_scores['loss'], test_scores['accuracy']\n",
        "    test_acc.append(t_accuracy)\n",
        "    test_loss.append(t_loss)\n",
        "    history.append(test_scores)\n",
        "\n",
        "    # break if we achieve the target test accuracy\n",
        "#     if test_accuracy_current >= target_test_accuracy:\n",
        "#       rounds = curr_round\n",
        "#       break\n",
        "\n",
        "    # break if we achieve convergence, i.e., loss between two consecutive rounds is <0.0001\n",
        "    if algorithm == 'fedprox' and abs(loss_avg - last_loss) < 0.0001:\n",
        "      rounds = curr_round\n",
        "      break\n",
        "    \n",
        "    # update the last loss\n",
        "    last_loss = loss_avg\n",
        "\n",
        "  end = time.time()\n",
        "  # plot train loss\n",
        "  fig, ax = plt.subplots()\n",
        "  x_axis = np.arange(1, rounds+1)\n",
        "  y_axis = np.array(train_loss)\n",
        "  ax.plot(x_axis, y_axis)\n",
        "\n",
        "  ax.set(xlabel='Number of Rounds', ylabel='Train Loss', title=plt_title)\n",
        "  ax.grid()\n",
        "  # fig.savefig(plt_title+'_Train_loss.jpg', format='jpg')\n",
        "  plt.show()\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  x_axis = np.arange(1, rounds+1)\n",
        "  y_axis = np.array(test_loss)\n",
        "  ax.plot(x_axis, y_axis, 'tab:'+plt_color)\n",
        "\n",
        "  ax.set(xlabel='Number of Rounds', ylabel='Test Loss', title=plt_title)\n",
        "  ax.grid()\n",
        "  # fig.savefig(plt_title+'_Test_loss.jpg', format='jpg')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  x_axis = np.arange(1, rounds+1)\n",
        "  y_axis = np.array(test_acc)\n",
        "  ax.plot(x_axis, y_axis, 'tab:'+plt_color)\n",
        "\n",
        "  ax.set(xlabel='Number of Rounds', ylabel='Test Accuracy', title=plt_title)\n",
        "  ax.grid()\n",
        "  # fig.savefig(plt_title+'_Test_Accuracy.jpg', format='jpg')\n",
        "  plt.show()\n",
        "  \n",
        "  print(\"Training Done!\")\n",
        "  print(\"Total time taken to Train: {}\\n\\n\".format(end-start))\n",
        "  \n",
        "  # return model, train_loss, test_accuracy, test_loss\n",
        "  return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUYyb4T-uXmF"
      },
      "source": [
        "## Testing Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCcIZmO5uan9"
      },
      "source": [
        "def testing(model, dataset, bs, criterion, num_classes, classes, print_info=False):\n",
        "  #test loss \n",
        "  test_loss = 0.0\n",
        "  correct_class = list(0. for i in range(num_classes))\n",
        "  total_class = list(0. for i in range(num_classes))\n",
        "\n",
        "  test_loader = DataLoader(dataset, batch_size=bs)\n",
        "  l = len(test_loader)\n",
        "  model.eval()\n",
        "  print('running validation...')\n",
        "  # for data, labels in test_loader:\n",
        "  for i, (data, labels) in enumerate(tqdm(test_loader)):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    test_loss += loss.item()*data.size(0)\n",
        "\n",
        "    _, pred = torch.max(output, 1)\n",
        "\n",
        "    # For F1Score\n",
        "    y_true = np.append(y_true, labels.data.view_as(pred).cpu().numpy()) if i != 0 else labels.data.view_as(pred).cpu().numpy()\n",
        "    y_hat = np.append(y_hat, pred.cpu().numpy()) if i != 0 else pred.cpu().numpy()\n",
        "\n",
        "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
        "\n",
        "    #test accuracy for each object class\n",
        "    for i in range(num_classes):\n",
        "      label = labels.data[i]\n",
        "      correct_class[label] += correct[i].item()\n",
        "      total_class[label] += 1\n",
        "    \n",
        "  # avg test loss\n",
        "  test_loss = test_loss/len(test_loader.dataset)\n",
        "  test_accuracy = 100. * np.sum(correct_class) / np.sum(total_class)\n",
        "  print(\"Test Loss: {:.6f}\\n\".format(test_loss))\n",
        "\n",
        "  # Avg F1 Score\n",
        "  f1_macro = f1_score(y_true, y_hat, average='macro')\n",
        "  # F1-Score -> weigthed to consider class imbalance\n",
        "  f1_weighted =  f1_score(y_true, y_hat, average='weighted')\n",
        "  print(\"F1 Score: {:.6f} (macro) {:.6f} (weighted) %\\n\".format(f1_macro, f1_weighted))\n",
        "\n",
        "  if print_info:\n",
        "    for i in range(num_classes):\n",
        "      if total_class[i]>0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % \n",
        "              (classes[i], 100 * correct_class[i] / total_class[i],\n",
        "              np.sum(correct_class[i]), np.sum(total_class[i])))\n",
        "      else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "  print('\\nFinal Test  Accuracy: {:.3f} ({}/{})'.format(\n",
        "        test_accuracy,\n",
        "        np.sum(correct_class), np.sum(total_class)))\n",
        "  \n",
        "  # return test_accuracy, test_loss\n",
        "  return {'loss': test_loss, 'accuracy': test_accuracy, 'f1_macro': f1_macro, 'f1_weighted': f1_weighted}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_wYSRsdJ6s_"
      },
      "source": [
        "## Plot utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6sYLbo3J6dd"
      },
      "source": [
        "def plot_scores(history, base_dir, title):\n",
        "    accuracies = [x['accuracy'] for x in history]\n",
        "    f1_macro = [x['f1_macro'] for x in history]\n",
        "    f1_weighted = [x['f1_weighted'] for x in history]\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(accuracies, 'tab:orange')\n",
        "    ax.set(xlabel='Rounds', ylabel='Test Accuracy', title=title)\n",
        "    ax.grid()\n",
        "    fig.savefig(f'{base_dir}/{title}_Test_Accuracy.jpg', format='jpg', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(f1_macro, 'tab:orange')\n",
        "    ax.set(xlabel='Rounds', ylabel='Test F1 (macro)', title=title)\n",
        "    ax.grid()\n",
        "    fig.savefig(f'{base_dir}/{title}_Test_F1_Macro.jpg', format='jpg', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(f1_weighted, 'tab:orange')\n",
        "    ax.set(xlabel='Rounds', ylabel='Test F1 (weighted)', title=title)\n",
        "    ax.grid()\n",
        "    fig.savefig(f'{base_dir}/{title}_Test_F1_Weighted.jpg', format='jpg', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_losses(history, base_dir, title):\n",
        "    val_losses = [x['loss'] for x in history]\n",
        "    train_losses = [x['train_loss'] for x in history]\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(train_losses, 'tab:orange')\n",
        "    ax.set(xlabel='Rounds', ylabel='Train Loss', title=title)\n",
        "    ax.grid()\n",
        "    fig.savefig(f'{base_dir}/{title}_Train_Loss.jpg', format='jpg', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(val_losses, 'tab:orange')\n",
        "    ax.set(xlabel='Rounds', ylabel='Test Loss', title=title)\n",
        "    ax.grid()\n",
        "    fig.savefig(f'{base_dir}/{title}_Test_Loss.jpg', format='jpg', dpi=300)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri0FqXFeHW-V"
      },
      "source": [
        "## Logger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMBtx9P5c1V_"
      },
      "source": [
        "log_dict = {}\n",
        "NUM_REPEAT = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfmcYUGWIwwV"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1WfefwkIx-M"
      },
      "source": [
        "# number of training rounds\n",
        "rounds = 50\n",
        "# client fraction\n",
        "C = 0.7\n",
        "# number of clients\n",
        "K = 100\n",
        "# number of training passes on local dataset for each roung\n",
        "E = 5\n",
        "# batch size\n",
        "batch_size = 10\n",
        "# learning Rate\n",
        "lr = 0.05\n",
        "# proximal term constant\n",
        "mu = 0.01\n",
        "# percentage of clients to have fewer than E epochs\n",
        "percentage = 50\n",
        "\n",
        "# target_test_accuracy\n",
        "target_test_accuracy=80.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv31hG-0JJIm"
      },
      "source": [
        "hyperparams = {'rounds': rounds,\n",
        "               'C': C,\n",
        "               'K': K,\n",
        "               'E': E,\n",
        "               'mu': mu,\n",
        "               'batch_size': batch_size,\n",
        "               'lr': lr,\n",
        "               'percentage': percentage,\n",
        "               'target_test_accuracy': target_test_accuracy\n",
        "               }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avVkMOw0JVNF"
      },
      "source": [
        "PARAM = f'client_fraction/{C}'\n",
        "EXP_DIR = f'{BASE_PATH}/{PARAM}'\n",
        "os.makedirs(EXP_DIR, exist_ok=True)\n",
        "EXP_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hO5oV6aXqeh"
      },
      "source": [
        "## CIFAR CNN on IID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfAqQtX6KG4F"
      },
      "source": [
        "title = \"CIFAR CNN on IID Dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flSQv_P4zCfx"
      },
      "source": [
        "### Train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZalcKZtEseA"
      },
      "source": [
        "train_loss_multiple_runs = [None] * NUM_REPEAT\n",
        "test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
        "test_loss_multiple_runs = [None] * NUM_REPEAT\n",
        "test_f1_macro_multiple_runs = [None] * NUM_REPEAT\n",
        "test_f1_micro_multiple_runs = [None] * NUM_REPEAT\n",
        "\n",
        "exp_history = []\n",
        "\n",
        "for exp_num in range(NUM_REPEAT):\n",
        "  print(\"Experiment Run Number: \", exp_num)\n",
        "\n",
        "  # load model\n",
        "  cifar_cnn = CIFAR_CNN()\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    cifar_cnn.cuda()\n",
        "\n",
        "  # data partition dictionary\n",
        "  iid_dict = iid_partition(cifar_data_train, K)\n",
        "\n",
        "  test_history = []\n",
        "  # cifar_cnn_iid_trained, train_loss, test_accuracy, test_loss = training(cifar_cnn, rounds, batch_size, lr, cifar_data_train, cifar_data_test, iid_dict, C, K, E, mu, percentage, \"CIFAR CNN on IID Dataset\", \"orange\", target_test_accuracy)\n",
        "  cifar_cnn_iid_trained, test_history = training(cifar_cnn,\n",
        "                                                 rounds, batch_size, lr,\n",
        "                                                 cifar_data_train, cifar_data_test, iid_dict,\n",
        "                                                 C, K, E, mu,\n",
        "                                                 percentage,\n",
        "                                                 title, \"orange\",\n",
        "                                                 target_test_accuracy,\n",
        "                                                 algorithm=\"fedprox\",\n",
        "                                                 history=test_history,\n",
        "                                                 )\n",
        "  \n",
        "  exp_history.append(test_history)\n",
        "\n",
        "  train_loss_multiple_runs[exp_num] = [scores['train_loss'] for scores in test_history]\n",
        "  test_accuracy_multiple_runs[exp_num] = [scores['accuracy'] for scores in test_history]\n",
        "  test_loss_multiple_runs[exp_num] = [scores['loss'] for scores in test_history]\n",
        "  test_f1_macro_multiple_runs[exp_num] = [scores['f1_macro'] for scores in test_history]\n",
        "  test_f1_micro_multiple_runs[exp_num] = [scores['f1_weighted'] for scores in test_history]\n",
        "\n",
        "  del cifar_cnn_iid_trained\n",
        "  torch.cuda.empty_cache()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNwC82przF6G"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB97BFs9we9w"
      },
      "source": [
        "# criterion = nn.CrossEntropyLoss()\n",
        "# acc, loss = testing(cifar_cnn_iid_trained, cifar_data_test, 128, criterion, num_classes, classes_test, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIC3XVZyKw1i"
      },
      "source": [
        "df = None\n",
        "for i, e in enumerate(exp_history):\n",
        "    if i == 0:\n",
        "        df = pd.json_normalize(e)\n",
        "        continue\n",
        "    df = df + pd.json_normalize(e)\n",
        "    \n",
        "df_avg = df / len(exp_history)\n",
        "avg_history = df_avg.to_dict(orient='records')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArE4Jic2KzY3"
      },
      "source": [
        "plot_losses(history=avg_history, base_dir=EXP_DIR, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVGZAyrzKz1N"
      },
      "source": [
        "plot_scores(history=avg_history, base_dir=EXP_DIR, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdQZEZmHHeqt"
      },
      "source": [
        "log_dict[title] = {'train_loss': train_loss_multiple_runs,\n",
        "                   'test_loss': test_loss_multiple_runs,\n",
        "                   'test_accuracy': test_accuracy_multiple_runs,\n",
        "                   'test_f1_macro': test_f1_macro_multiple_runs,\n",
        "                   'test_f1_micro': test_f1_micro_multiple_runs,\n",
        "                   'hyperparams': hyperparams,\n",
        "                   }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0f8O0W0MHF6"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open(f'{EXP_DIR}/results_iid.pkl', 'wb') as file:\n",
        "  pickle.dump(log_dict, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF8MdSIUYcnl"
      },
      "source": [
        "## CIFAR CNN on Non IID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBhrxPPcMKCc"
      },
      "source": [
        "log_dict = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYybT5IfMiEU"
      },
      "source": [
        "title = 'CIFAR CNN on Non-IID Dataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6wXX7JW11bx"
      },
      "source": [
        "### Train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1nCOcMBN9qs"
      },
      "source": [
        "# Skewness\n",
        "# 100, 200, 250, 2 - Low\n",
        "# 100, 500, 120, 5 - Medium\n",
        "# 100, 1000, 60, 10 - High\n",
        "clients = K\n",
        "num_shards_per_client = 2\n",
        "total_shards = 200\n",
        "shards_size = 250\n",
        "\n",
        "hyperparams['shards'] = num_shards_per_client\n",
        "\n",
        "clients, total_shards, shards_size, num_shards_per_client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCD3kBCKYfBK"
      },
      "source": [
        "train_loss_multiple_runs = [None] * NUM_REPEAT\n",
        "test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
        "test_loss_multiple_runs = [None] * NUM_REPEAT\n",
        "test_f1_macro_multiple_runs = [None] * NUM_REPEAT\n",
        "test_f1_micro_multiple_runs = [None] * NUM_REPEAT\n",
        "\n",
        "exp_history = []\n",
        "\n",
        "for exp_num in range(NUM_REPEAT):\n",
        "  print(\"Experiment Run Number: \", exp_num)\n",
        "\n",
        "  # load model\n",
        "  cifar_cnn = CIFAR_CNN()\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    cifar_cnn.cuda()\n",
        "\n",
        "  # dict containing different type of data partition\n",
        "  niid_dict = non_iid_partition(dataset=cifar_data_train,\n",
        "                                clients=K,\n",
        "                                total_shards=total_shards,\n",
        "                                shards_size=shards_size,\n",
        "                                num_shards_per_client=num_shards_per_client)\n",
        "\n",
        "  test_history = []\n",
        "  cifar_cnn_non_iid_trained, test_history = training(cifar_cnn,\n",
        "                                                     rounds, batch_size, lr,\n",
        "                                                     cifar_data_train, cifar_data_test, niid_dict,\n",
        "                                                     C, K, E, mu, percentage,\n",
        "                                                     title, \"orange\",\n",
        "                                                     target_test_accuracy,\n",
        "                                                     algorithm=\"fedprox\",\n",
        "                                                     history=test_history,\n",
        "                                                     )\n",
        "  \n",
        "  exp_history.append(test_history)\n",
        "\n",
        "  train_loss_multiple_runs[exp_num] = [scores['train_loss'] for scores in test_history]\n",
        "  test_accuracy_multiple_runs[exp_num] = [scores['accuracy'] for scores in test_history]\n",
        "  test_loss_multiple_runs[exp_num] = [scores['loss'] for scores in test_history]\n",
        "  test_f1_macro_multiple_runs[exp_num] = [scores['f1_macro'] for scores in test_history]\n",
        "  test_f1_micro_multiple_runs[exp_num] = [scores['f1_weighted'] for scores in test_history]\n",
        "\n",
        "  del cifar_cnn_non_iid_trained\n",
        "  torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C68J-Kk14dB"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yQ9GiAZ15jE"
      },
      "source": [
        "# criterion = nn.CrossEntropyLoss()\n",
        "# acc, loss = testing(cifar_cnn_non_iid_trained, cifar_data_test, 128, criterion, num_classes, classes_test, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8l_rBBmSObDc"
      },
      "source": [
        "df = None\n",
        "for i, e in enumerate(exp_history):\n",
        "    if i == 0:\n",
        "        df = pd.json_normalize(e)\n",
        "        continue\n",
        "    df = df + pd.json_normalize(e)\n",
        "    \n",
        "df_avg = df / len(exp_history)\n",
        "avg_history = df_avg.to_dict(orient='records')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTVnY1rLOcsI"
      },
      "source": [
        "plot_losses(history=avg_history, base_dir=EXP_DIR, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKoU5fgvOdT4"
      },
      "source": [
        "plot_scores(history=avg_history, base_dir=EXP_DIR, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxMcxgLhLvX-"
      },
      "source": [
        "log_dict[title] = {'train_loss': train_loss_multiple_runs,\n",
        "                   'test_loss': test_loss_multiple_runs,\n",
        "                   'test_accuracy': test_accuracy_multiple_runs,\n",
        "                   'test_f1_macro': test_f1_macro_multiple_runs,\n",
        "                   'test_f1_micro': test_f1_micro_multiple_runs,\n",
        "                   'hyperparams': hyperparams,\n",
        "                   }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBi4PuFJMTsi"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open(f'{EXP_DIR}/results_niid.pkl', 'wb') as file:\n",
        "  pickle.dump(log_dict, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptMieSimVxid"
      },
      "source": [
        "## CIFAR MLP on IID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz3f_fOxVxie"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZGAZPhtVxie"
      },
      "source": [
        "# train_loss_multiple_runs = [None] * NUM_REPEAT\n",
        "# test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
        "# test_loss_multiple_runs = [None] * NUM_REPEAT\n",
        "\n",
        "# for exp_num in range(NUM_REPEAT):\n",
        "#   print(\"Experiment Run Number: \", exp_num)\n",
        "\n",
        "#   # number of training rounds\n",
        "#   rounds = 50\n",
        "#   # client fraction\n",
        "#   C = 0.1\n",
        "#   # number of clients\n",
        "#   K = 100\n",
        "#   # number of training passes on local dataset for each round\n",
        "#   E = 5\n",
        "#   # batch size\n",
        "#   batch_size = 10\n",
        "#   # learning Rate\n",
        "#   lr=0.05\n",
        "#   # proximal term constant\n",
        "#   mu = 0.01\n",
        "#   # percentage of clients to have fewer than E epochs\n",
        "#   percentage = 50\n",
        "#   # target_test_accuracy\n",
        "#   target_test_accuracy=50.0\n",
        "\n",
        "#   # dict containing different type of data partition\n",
        "#   data_dict = iid_partition(cifar_data_train, 100)\n",
        "#   # load model\n",
        "#   cifar_mlp = CIFAR_MLP()\n",
        "\n",
        "#   if torch.cuda.is_available():\n",
        "#     cifar_mlp.cuda()\n",
        "\n",
        "#   cifar_mlp_iid_trained, train_loss, test_accuracy, test_loss = training(cifar_mlp, rounds, batch_size, lr, cifar_data_train, cifar_data_test, data_dict, C, K, E, mu, percentage, \"CIFAR MLP on IID Dataset\", \"orange\", target_test_accuracy)\n",
        "  \n",
        "#   train_loss_multiple_runs[exp_num] = train_loss\n",
        "#   test_accuracy_multiple_runs[exp_num] = test_accuracy\n",
        "#   test_loss_multiple_runs[exp_num] = test_loss\n",
        "\n",
        "#   del cifar_mlp_iid_trained\n",
        "#   torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTBsL3-72PPd"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9G4j5L62OrS"
      },
      "source": [
        "# criterion = nn.CrossEntropyLoss()\n",
        "# acc, loss = testing(cifar_mlp_iid_trained, cifar_data_test, 128, criterion, num_classes, classes_test, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWCdJFRCL_f2"
      },
      "source": [
        "# hyperparams = {'rounds': rounds,\n",
        "#                'C': C,\n",
        "#                'K': K,\n",
        "#                'E': E,\n",
        "#                'batch_size': batch_size,\n",
        "#                'lr': lr,\n",
        "#                'mu': mu,\n",
        "#                'percentage': percentage,\n",
        "#                'target_test_accuracy': target_test_accuracy,\n",
        "#                }\n",
        "\n",
        "# log_dict['CIFAR MLP on IID'] = {'train_loss': train_loss_multiple_runs, \n",
        "#                                 'test_loss': test_loss_multiple_runs, \n",
        "#                                 'test_accuracy': test_accuracy_multiple_runs,\n",
        "#                                 'hyperparams': hyperparams,\n",
        "#                                 }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8jzEyO0iywz"
      },
      "source": [
        "## CIFAR MLP on Non IID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJFepr3y2bF-"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBKO44Hgi1Uh"
      },
      "source": [
        "# train_loss_multiple_runs = [None] * NUM_REPEAT\n",
        "# test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
        "# test_loss_multiple_runs = [None] * NUM_REPEAT\n",
        "\n",
        "# for exp_num in range(NUM_REPEAT):\n",
        "#   print(\"Experiment Run Number: \", exp_num)\n",
        "  \n",
        "#   # number of training rounds\n",
        "#   rounds = 50\n",
        "#   # client fraction\n",
        "#   C = 0.1\n",
        "#   # number of clients\n",
        "#   K = 100\n",
        "#   # number of training passes on local dataset for each roung\n",
        "#   E = 5\n",
        "#   # batch size\n",
        "#   batch_size = 10\n",
        "#   # learning Rate\n",
        "#   lr=0.05\n",
        "#   # proximal term constant\n",
        "#   mu = 0.01\n",
        "#   # percentage of clients to have fewer than E epochs\n",
        "#   percentage = 50\n",
        "#   # target_test_accuracy\n",
        "#   target_test_accuracy=50.0\n",
        "\n",
        "#   # dict containing different type of data partition\n",
        "#   data_dict = non_iid_partition(cifar_data_train, 100, 200, 250, 2)\n",
        "#   # load model\n",
        "#   cifar_mlp = CIFAR_MLP()\n",
        "\n",
        "#   if torch.cuda.is_available():\n",
        "#     cifar_mlp.cuda()\n",
        "\n",
        "#   cifar_mlp_non_iid_trained, train_loss, test_accuracy, test_loss = training(cifar_mlp, rounds, batch_size, lr, cifar_data_train, cifar_data_test, data_dict, C, K, E, mu, percentage, \"CIFAR MLP on Non-IID Dataset\", \"green\", target_test_accuracy)\n",
        "\n",
        "#   train_loss_multiple_runs[exp_num] = train_loss\n",
        "#   test_accuracy_multiple_runs[exp_num] = test_accuracy\n",
        "#   test_loss_multiple_runs[exp_num] = test_loss\n",
        "\n",
        "#   del cifar_mlp_non_iid_trained\n",
        "#   torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmuQYPbF2mes"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tKMlJyF2nGN"
      },
      "source": [
        "# criterion = nn.CrossEntropyLoss()\n",
        "# acc, loss = testing(cifar_mlp_non_iid_trained, cifar_data_test, 128, criterion, num_classes, classes_test, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W5krYcSMQiu"
      },
      "source": [
        "# hyperparams = {'rounds': rounds,\n",
        "#                'C': C,\n",
        "#                'K': K,\n",
        "#                'E': E,\n",
        "#                'batch_size': batch_size,\n",
        "#                'lr': lr,\n",
        "#                'mu': mu,\n",
        "#                'percentage': percentage,\n",
        "#                'target_test_accuracy': target_test_accuracy,\n",
        "#                }\n",
        "\n",
        "# log_dict['CIFAR MLP on Non IID'] = {'train_loss': train_loss_multiple_runs, \n",
        "#                                 'test_loss': test_loss_multiple_runs, \n",
        "#                                 'test_accuracy': test_accuracy_multiple_runs,\n",
        "#                                 'hyperparams': hyperparams,\n",
        "#                                 }"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}