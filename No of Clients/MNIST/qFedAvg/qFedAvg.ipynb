{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qFedAvg_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "x5UTD8mqIDWl",
        "Bs9Wgjda2lQ9",
        "ctjRsETiO1qO",
        "xP0TiV5n-aoj",
        "W82ANFbZ-aol",
        "g_wYSRsdJ6s_",
        "p-8XOZlR9wbV",
        "0vQVNPmM93M7",
        "JrTGp6vd-DKa",
        "hGoeL-HpDv7L",
        "ttnrJ9FnDxFR",
        "ExsUNb2MACKt"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v_R7FXY7_fi"
      },
      "source": [
        "# FedPerf - MNIST + qFedAvg algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5UTD8mqIDWl"
      },
      "source": [
        "## Setup & Dependencies Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vTQbtlX731V"
      },
      "source": [
        "%%capture\n",
        "!pip install torchsummaryX unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHlNfr2R8EvM"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "import copy\n",
        "from functools import reduce\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import time\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.sampler import Sampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchsummary import summary\n",
        "from torchsummaryX import summary as summaryx\n",
        "from torchvision import transforms, utils, datasets\n",
        "from tqdm.notebook import tqdm\n",
        "from unidecode import unidecode\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Check assigned GPU\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "# set manual seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# general reproducibility\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# gpu training specific\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvWSM6mh8LZr"
      },
      "source": [
        "## Mount GDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh9Lj2-lIIfY"
      },
      "source": [
        "BASE_DIR = '/content/drive/MyDrive/FedPerf/MNIST/qFedAvg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "518ez9Oz8LNE"
      },
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    os.makedirs(BASE_DIR, exist_ok=True)\n",
        "except:\n",
        "    print(\"WARNING: Results won't be stored on GDrive\")\n",
        "    BASE_DIR = './'\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs9Wgjda2lQ9"
      },
      "source": [
        "## Load the MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArhLGDSd2lRD"
      },
      "source": [
        "# create transforms\n",
        "# We will just convert to tensor and normalize since no special transforms are mentioned in the paper\n",
        "transforms_mnist = transforms.Compose([\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n",
        "\n",
        "mnist_data_train = datasets.MNIST('./data/mnist/', train=True, download=True, transform=transforms_mnist)\n",
        "mnist_data_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=transforms_mnist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm9usjn2vFkL"
      },
      "source": [
        "classes = np.array(list(mnist_data_train.class_to_idx.values()))\n",
        "classes_test = np.array(list(mnist_data_test.class_to_idx.values()))\n",
        "num_classes = len(classes_test)\n",
        "print(\"Classes: {} \\tType: {}\".format(classes, type(classes)))\n",
        "print(\"Classes Test: {} \\tType: {}\".format(classes_test, type(classes)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lvJt3Ofv2SO"
      },
      "source": [
        "print(\"Image Shape: {}\".format(mnist_data_train.data[0].size()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctjRsETiO1qO"
      },
      "source": [
        "## Partitioning the Data (IID and non-IID)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_v8lyrgO5dD"
      },
      "source": [
        "def iid_partition(dataset, clients):\n",
        "  \"\"\"\n",
        "  I.I.D paritioning of data over clients\n",
        "  Shuffle the data\n",
        "  Split it between clients\n",
        "  \n",
        "  params:\n",
        "    - dataset (torch.utils.Dataset): Dataset containing the CIFAR Images\n",
        "    - clients (int): Number of Clients to split the data between\n",
        "\n",
        "  returns:\n",
        "    - Dictionary of image indexes for each client\n",
        "  \"\"\"\n",
        "\n",
        "  num_items_per_client = int(len(dataset)/clients)\n",
        "  client_dict = {}\n",
        "  image_idxs = [i for i in range(len(dataset))]\n",
        "\n",
        "  for i in range(clients):\n",
        "    client_dict[i] = set(np.random.choice(image_idxs, num_items_per_client, replace=False))\n",
        "    image_idxs = list(set(image_idxs) - client_dict[i])\n",
        "\n",
        "  return client_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zMdliGMQoSl"
      },
      "source": [
        "def non_iid_partition(dataset, clients, total_shards, shards_size, num_shards_per_client):\n",
        "  \"\"\"\n",
        "  non I.I.D parititioning of data over clients\n",
        "  Sort the data by the digit label\n",
        "  Divide the data into N shards of size S\n",
        "  Each of the clients will get X shards\n",
        "\n",
        "  params:\n",
        "    - dataset (torch.utils.Dataset): Dataset containing the CIFAR Images\n",
        "    - clients (int): Number of Clients to split the data between\n",
        "    - total_shards (int): Number of shards to partition the data in\n",
        "    - shards_size (int): Size of each shard \n",
        "    - num_shards_per_client (int): Number of shards of size shards_size that each client receives\n",
        "\n",
        "  returns:\n",
        "    - Dictionary of image indexes for each client\n",
        "  \"\"\"\n",
        "  \n",
        "  shard_idxs = [i for i in range(total_shards)]\n",
        "  client_dict = {i: np.array([], dtype='int64') for i in range(clients)}\n",
        "  idxs = np.arange(len(dataset))\n",
        "  data_labels = np.array(dataset.targets)\n",
        "\n",
        "  # sort the labels\n",
        "  label_idxs = np.vstack((idxs, data_labels))\n",
        "  label_idxs = label_idxs[:, label_idxs[1,:].argsort()]\n",
        "  idxs = label_idxs[0,:]\n",
        "\n",
        "  # divide the data into total_shards of size shards_size\n",
        "  # assign num_shards_per_client to each client\n",
        "  for i in range(clients):\n",
        "    rand_set = set(np.random.choice(shard_idxs, num_shards_per_client, replace=False))\n",
        "    shard_idxs = list(set(shard_idxs) - rand_set)\n",
        "\n",
        "    for rand in rand_set:\n",
        "      client_dict[i] = np.concatenate((client_dict[i], idxs[rand*shards_size:(rand+1)*shards_size]), axis=0)\n",
        "  \n",
        "  return client_dict\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP0TiV5n-aoj"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrp0DTn7-aok"
      },
      "source": [
        "class MNIST_2NN(nn.Module):\n",
        "  \"\"\"\n",
        "  A simple multilayer-perceptron with 2-hidden layers with 200 units each\n",
        "  using ReLu activations\n",
        "\n",
        "  Total Expected Params: 199,210\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(MNIST_2NN, self).__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(28*28, 200)\n",
        "    self.fc2 = nn.Linear(200, 200)\n",
        "    self.fc3 = nn.Linear(200, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    out = self.fc3(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyHlCF_g-aol"
      },
      "source": [
        "class MNIST_CNN(nn.Module):\n",
        "  \"\"\"\n",
        "  CNN with two 5x5 convolution lauers(the first with 32 channels, second with 64,\n",
        "  each followed with 2x2 max pooling), a fully connected layer with 512 uunits and \n",
        "  ReLu activation, and the final Softmax output layer\n",
        "\n",
        "  Total Expected Params: 1,663,370\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(MNIST_CNN, self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
        "    \n",
        "    self.pool = nn.MaxPool2d(2,2)\n",
        "    self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    self.fc1 = nn.Linear(1024, 512)\n",
        "    self.out = nn.Linear(512, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = self.dropout(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.out(x)\n",
        "    out = F.log_softmax(x, dim=1)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W82ANFbZ-aol"
      },
      "source": [
        "### Print Model Summaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB64pKxP-aom"
      },
      "source": [
        "mnist_mlp = MNIST_2NN()\n",
        "mnist_cnn = MNIST_CNN()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  mnist_mlp.cuda()\n",
        "  mnist_cnn.cuda()\n",
        "\n",
        "print(\"MNIST MLP SUMMARY\")\n",
        "print(summary(mnist_mlp, (28,28)))\n",
        "\n",
        "print(\"\\nMNIST CNN SUMMARY\")\n",
        "print(summary(mnist_cnn, (1, 28,28)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_wYSRsdJ6s_"
      },
      "source": [
        "## Plot utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6sYLbo3J6dd"
      },
      "source": [
        "def plot_scores(history, base_dir, title):\n",
        "    accuracies = [x['accuracy'] for x in history]\n",
        "    f1_macro = [x['f1_macro'] for x in history]\n",
        "    f1_weighted = [x['f1_weighted'] for x in history]\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(accuracies, 'tab:orange')\n",
        "    ax.set(xlabel='Rounds', ylabel='Test Accuracy', title=title)\n",
        "    ax.grid()\n",
        "    fig.savefig(f'{base_dir}/{title}_Test_Accuracy.jpg', format='jpg', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(f1_macro, 'tab:orange')\n",
        "    ax.set(xlabel='Rounds', ylabel='Test F1 (macro)', title=title)\n",
        "    ax.grid()\n",
        "    fig.savefig(f'{base_dir}/{title}_Test_F1_Macro.jpg', format='jpg', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(f1_weighted, 'tab:orange')\n",
        "    ax.set(xlabel='Rounds', ylabel='Test F1 (weighted)', title=title)\n",
        "    ax.grid()\n",
        "    fig.savefig(f'{base_dir}/{title}_Test_F1_Weighted.jpg', format='jpg', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_losses(history, base_dir, title):\n",
        "    val_losses = [x['loss'] for x in history]\n",
        "    train_losses = [x['train_loss'] for x in history]\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(train_losses, 'tab:orange')\n",
        "    ax.set(xlabel='Rounds', ylabel='Train Loss', title=title)\n",
        "    ax.grid()\n",
        "    fig.savefig(f'{base_dir}/{title}_Train_Loss.jpg', format='jpg', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(val_losses, 'tab:orange')\n",
        "    ax.set(xlabel='Rounds', ylabel='Test Loss', title=title)\n",
        "    ax.grid()\n",
        "    fig.savefig(f'{base_dir}/{title}_Test_Loss.jpg', format='jpg', dpi=300)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbJUc6X89jFw"
      },
      "source": [
        "## qFedAvg Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-8XOZlR9wbV"
      },
      "source": [
        "### Local Training (Client Update)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83Jn60Gt8KlY"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, dataset, idxs):\n",
        "      self.dataset = dataset\n",
        "      self.idxs = list(idxs)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.idxs)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "      data, label = self.dataset[self.idxs[item]]\n",
        "      return data, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOn7VSei8KjB"
      },
      "source": [
        "class ClientUpdate(object):\n",
        "  def __init__(self, dataset, batch_size, learning_rate, epochs, idxs, q=None):\n",
        "    if hasattr(dataset, 'dataloader'):\n",
        "        self.train_loader = dataset.dataloader(batch_size=batch_size, shuffle=True)\n",
        "    else:\n",
        "        self.train_loader = DataLoader(CustomDataset(dataset, idxs), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.epochs = epochs\n",
        "    self.q = q\n",
        "    if not self.q:\n",
        "        # TODO: Client itself adjust fairness \n",
        "        pass\n",
        "    self.mu = 1e-10\n",
        "\n",
        "  def train(self, model):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.5)\n",
        "\n",
        "    e_loss = []\n",
        "    model_weights = copy.deepcopy(model.state_dict())\n",
        "    for epoch in range(1, self.epochs+1):\n",
        "\n",
        "      train_loss = 0.0\n",
        "      model.train()\n",
        "      # for data, labels in tqdm(self.train_loader):\n",
        "      for data, labels in self.train_loader:\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "          data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "        # clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # make a forward pass\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, labels)\n",
        "        # do a backwards pass\n",
        "        loss.backward()\n",
        "        # perform a single optimization step\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "\n",
        "      # average losses\n",
        "      train_loss = train_loss/len(self.train_loader.dataset)\n",
        "      e_loss.append(train_loss)\n",
        "\n",
        "    total_loss = sum(e_loss)/len(e_loss)\n",
        "\n",
        "    # delta weights\n",
        "    model_weights_new = copy.deepcopy(model.state_dict())\n",
        "    L = 1.0 / self.learning_rate\n",
        "\n",
        "    delta_weights, delta, h = {}, {}, {}\n",
        "    loss_q = np.float_power(total_loss + self.mu, self.q)\n",
        "\n",
        "    # updating the global weights\n",
        "    for k in model_weights_new.keys():\n",
        "      delta_weights[k] = (model_weights[k] - model_weights_new[k]) * L\n",
        "      delta[k] =  loss_q * delta_weights[k]\n",
        "      # Estimation of the local Lipchitz constant\n",
        "      h[k] = (self.q * np.float_power(total_loss + self.mu, self.q - 1) * torch.pow(torch.norm(delta_weights[k]), 2)) + (L * loss_q)\n",
        "\n",
        "    return delta, h, total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vQVNPmM93M7"
      },
      "source": [
        "### Server Side Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhOfx0HSCX35"
      },
      "source": [
        "def client_sampling(n, m, weights=None, with_replace=False):\n",
        "    pk = None\n",
        "    if weights:\n",
        "        total_weights = np.sum(np.asarray(weights))\n",
        "        pk = [w * 1.0 / total_weights for w in weights]\n",
        "\n",
        "    return np.random.choice(range(n), m, replace=with_replace, p=pk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCJZnkY08Kgs"
      },
      "source": [
        "def training(model, rounds, batch_size, lr, ds, data_dict, test_ds, C, K, E, q=0, sampling='uniform', tb_logger=None,\n",
        "             test_history=[], perf_fig_file='loss.jpg'):\n",
        "  \"\"\"\n",
        "  Function implements the Federated Averaging Algorithm from the FedAvg paper.\n",
        "  Specifically, this function is used for the server side training and weight update\n",
        "\n",
        "  Params:\n",
        "    - model:           PyTorch model to train\n",
        "    - rounds:          Number of communication rounds for the client update\n",
        "    - batch_size:      Batch size for client update training\n",
        "    - lr:              Learning rate used for client update training\n",
        "    - ds:              Dataset used for training\n",
        "    - data_dict:       Type of data partition used for training (IID or non-IID)\n",
        "    - test_ds          Dataset used for global testing\n",
        "    - C:               Fraction of clients randomly chosen to perform computation on each round\n",
        "    - K:               Total number of clients\n",
        "    - E:               Number of training passes each client makes over its local dataset per round\n",
        "    - q:               Parameter that tunes the amount of fairness we wish to impose (default: 0 -> vanilla FedAvg objective)\n",
        "    - sampling         Uniform or weighted (default: uniform)\n",
        "    - tb_logger:       Tensorboard SummaryWriter\n",
        "    - test_history:    Test Scores history log\n",
        "    - perf_fig_file    File for storing final performance plot (loss)\n",
        "  Returns:\n",
        "    - model:           Trained model on the server\n",
        "  \"\"\"\n",
        "\n",
        "  # global model weights\n",
        "  global_weights = model.state_dict()\n",
        "\n",
        "  # training loss\n",
        "  train_loss = []\n",
        "\n",
        "  # client weights by total samples\n",
        "  p_k = None\n",
        "  if sampling == 'weighted':\n",
        "    p_k = [len(data_dict[c]) for c in data_dict] if ds else [len(data_dict[c]['train_ds']) for c in data_dict]\n",
        "\n",
        "  # Time log\n",
        "  start_time = time.time()\n",
        "\n",
        "  users_id = list(data_dict.keys())\n",
        "\n",
        "  # Orchestrate training\n",
        "  for curr_round in range(1, rounds+1):\n",
        "    deltas, hs, local_loss = [], [], []\n",
        "\n",
        "    m = max(int(C*K), 1)    \n",
        "    S_t = client_sampling(K, m, weights=p_k, with_replace=False)\n",
        "\n",
        "    print('Round: {} Picking {}/{} clients: {}'.format(curr_round, m, K, S_t))\n",
        "\n",
        "    global_weights = model.state_dict()\n",
        "\n",
        "    for k in tqdm(S_t):\n",
        "      key = users_id[k]\n",
        "      ds_ = ds if ds else data_dict[key]['train_ds']\n",
        "      idxs = data_dict[key] if ds else None\n",
        "      # print(f'Client {k}: {len(idxs) if idxs else len(ds_)} samples')\n",
        "      local_update = ClientUpdate(dataset=ds_, batch_size=batch_size, learning_rate=lr, epochs=E, idxs=idxs, q=q)\n",
        "    #   weights, loss = local_update.train(model=copy.deepcopy(model))\n",
        "      delta_k, h_k, loss = local_update.train(model=copy.deepcopy(model))\n",
        "\n",
        "      deltas.append(copy.deepcopy(delta_k))\n",
        "      hs.append(copy.deepcopy(h_k))\n",
        "      local_loss.append(copy.deepcopy(loss))\n",
        "\n",
        "      if tb_logger:\n",
        "        tb_logger.add_scalar(f'Round/S{k}', loss, curr_round)\n",
        "\n",
        "    # Perform qFedAvg\n",
        "    h_sum = copy.deepcopy(hs[0])\n",
        "    delta_sum = copy.deepcopy(deltas[0])\n",
        "    \n",
        "    for k in h_sum.keys():\n",
        "        for i in range(1, len(hs)):\n",
        "            h_sum[k] += hs[i][k]\n",
        "            delta_sum[k] += deltas[i][k]\n",
        "\n",
        "    new_weights = {}\n",
        "    for k in delta_sum.keys():\n",
        "        for i in range(len(deltas)):\n",
        "            new_weights[k] = delta_sum[k] / h_sum[k]\n",
        "\n",
        "    # Updating global model weights\n",
        "    for k in global_weights.keys():\n",
        "        # print(k, type(global_weights[k]), type(new_weights[k]))\n",
        "        if 'num_batches_tracked' in k:\n",
        "            continue\n",
        "        global_weights[k] -= new_weights[k]\n",
        "\n",
        "    # move the updated weights to our model state dict\n",
        "    model.load_state_dict(global_weights)\n",
        "\n",
        "    # loss\n",
        "    loss_avg = sum(local_loss) / len(local_loss)\n",
        "    print('Round: {}... \\tAverage Loss: {}'.format(curr_round, round(loss_avg, 3)))\n",
        "    train_loss.append(loss_avg)\n",
        "\n",
        "    if tb_logger:\n",
        "        tb_logger.add_scalar('Train/Loss', loss_avg, curr_round)\n",
        "        # tb_logger.add_scalar(f'Train/Datapoints', total_datapoints, curr_round)\n",
        "    \n",
        "    # if curr_round % eval_every == 0:\n",
        "    test_scores = testing(model, test_ds, batch_size, nn.CrossEntropyLoss(), num_classes, classes_test)\n",
        "    test_scores['train_loss'] = loss_avg\n",
        "    test_history.append(test_scores)\n",
        "    if tb_logger:\n",
        "        tb_logger.add_scalar(f'Test/Loss', test_scores['loss'], curr_round)\n",
        "        tb_logger.add_scalars(f'Test/Scores', {\n",
        "            'accuracy': test_scores['accuracy'], 'f1_macro': test_scores['f1_macro'], 'f1_weighted': test_scores['f1_weighted']\n",
        "        }, curr_round)\n",
        "  \n",
        "  end_time = time.time()\n",
        "  \n",
        "  fig, ax = plt.subplots()\n",
        "  x_axis = np.arange(1, rounds+1)\n",
        "  y_axis = np.array(train_loss)\n",
        "  ax.plot(x_axis, y_axis, 'tab:orange')\n",
        "\n",
        "  ax.set(xlabel='# Rounds', ylabel='Train Loss',\n",
        "       title=\"Model's Performance with q: {}\".format(q))\n",
        "  ax.grid()\n",
        "  #fig.savefig(perf_fig_file, format='jpg')\n",
        "\n",
        "  print(\"Training Done! Total time: {}\".format(round(end_time - start_time, 3)))\n",
        "  return model, test_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrTGp6vd-DKa"
      },
      "source": [
        "### Testing Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEnPyGYb8KeO"
      },
      "source": [
        "def testing(model, dataset, bs, criterion, num_classes, classes, print_all=False):\n",
        "  #test loss \n",
        "  test_loss = 0.0\n",
        "  y_true, y_hat = None, None\n",
        "\n",
        "  correct_class = list(0 for i in range(num_classes))\n",
        "  total_class = list(0 for i in range(num_classes))\n",
        "\n",
        "  if hasattr(dataset, 'dataloader'):\n",
        "    test_loader = dataset.dataloader(batch_size=bs, shuffle=False)\n",
        "  else:\n",
        "    test_loader = DataLoader(dataset, batch_size=bs, shuffle=False)\n",
        "\n",
        "  l = len(test_loader)\n",
        "\n",
        "  model.eval()\n",
        "  for i, (data, labels) in enumerate(tqdm(test_loader)):\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      data, labels = data.cuda(), labels.cuda()\n",
        "\n",
        "    output = model(data)\n",
        "    loss = criterion(output, labels)\n",
        "    test_loss += loss.item()*data.size(0)\n",
        "\n",
        "    _, pred = torch.max(output, dim=1)\n",
        "\n",
        "    # For F1Score\n",
        "    y_true = np.append(y_true, labels.data.view_as(pred).cpu().numpy()) if i != 0 else labels.data.view_as(pred).cpu().numpy()\n",
        "    y_hat = np.append(y_hat, pred.cpu().numpy()) if i != 0 else pred.cpu().numpy()\n",
        "\n",
        "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
        "\n",
        "    #test accuracy for each object class\n",
        "    # for i in range(num_classes):\n",
        "    #   label = labels.data[i]\n",
        "    #   correct_class[label] += correct[i].item()\n",
        "    #   total_class[label] += 1\n",
        "\n",
        "    for i, lbl in enumerate(labels.data):\n",
        "      try:\n",
        "        # print(type(lbl))\n",
        "        # correct_class[lbl.data[0]] += correct.data[i]\n",
        "        correct_class[lbl.item()] += correct[i]\n",
        "        total_class[lbl.item()] += 1\n",
        "      except:\n",
        "          print('Error', lbl, i)\n",
        "    \n",
        "  # avg test loss\n",
        "  test_loss = test_loss/len(test_loader.dataset)\n",
        "  print(\"Test Loss: {:.6f}\\n\".format(test_loss))\n",
        "\n",
        "  # Avg F1 Score\n",
        "  f1_macro = f1_score(y_true, y_hat, average='macro')\n",
        "  # F1-Score -> weigthed to consider class imbalance\n",
        "  f1_weighted =  f1_score(y_true, y_hat, average='weighted')\n",
        "  print(\"F1 Score: {:.6f} (macro) {:.6f} (weighted) %\\n\".format(f1_macro, f1_weighted))\n",
        "\n",
        "  # print test accuracy\n",
        "  if print_all:\n",
        "    for i in range(num_classes):\n",
        "        if total_class[i] > 0:\n",
        "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % \n",
        "                    (classes[i], 100 * correct_class[i] / total_class[i],\n",
        "                    np.sum(correct_class[i]), np.sum(total_class[i])))\n",
        "        else:\n",
        "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "  overall_accuracy = np.sum(correct_class) / np.sum(total_class)\n",
        "\n",
        "  print('\\nFinal Test  Accuracy: {:.3f} ({}/{})'.format(overall_accuracy, np.sum(correct_class), np.sum(total_class)))\n",
        "\n",
        "  return {'loss': test_loss, 'accuracy': overall_accuracy, 'f1_macro': f1_macro, 'f1_weighted': f1_weighted}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evpa7UZO-Hii"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IadhhL4knbOc"
      },
      "source": [
        "class Hyperparameters():\n",
        "\n",
        "    def __init__(self, total_clients):\n",
        "        # number of training rounds\n",
        "        self.rounds = 50\n",
        "        # client fraction\n",
        "        self.C = 0.7\n",
        "        # number of clients\n",
        "        self.K = total_clients\n",
        "        # number of training passes on local dataset for each roung\n",
        "        self.E = 5\n",
        "        # batch size\n",
        "        self.batch_size = 10  # fedprox\n",
        "        # learning Rate\n",
        "        self.lr = 0.05\n",
        "        # fairness\n",
        "        self.q = 0.001  # qFFL\n",
        "        # sampling\n",
        "        # self.sampling = 'uniform'\n",
        "        self.sampling = 'weighted'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUmhoN-zGZp_"
      },
      "source": [
        "hparams = Hyperparameters(total_clients=100)\n",
        "hparams.__dict__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfNgps35XnxY"
      },
      "source": [
        "# Sweeping parameter\n",
        "PARAM_NAME = 'clients_fraction'\n",
        "PARAM_VALUE = hparams.C\n",
        "exp_id = f'{PARAM_NAME}/{PARAM_VALUE}'\n",
        "exp_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmXNlFlYcl2H"
      },
      "source": [
        "EXP_DIR = f'{BASE_DIR}/{exp_id}'\n",
        "os.makedirs(EXP_DIR, exist_ok=True)\n",
        "\n",
        "# tb_logger = SummaryWriter(log_dir)\n",
        "# print(f'TBoard logger created at: {log_dir}')\n",
        "EXP_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGoeL-HpDv7L"
      },
      "source": [
        "### MNIST CNN on IID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u68TlfO67cgL"
      },
      "source": [
        "exp_log = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keTKYLOc759-"
      },
      "source": [
        "title = 'MNIST CNN on IID Dataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61pZ-UY6D8nB"
      },
      "source": [
        "# train_ds, data_dict, test_ds = iid_partition(corpus, seq_length, val_split=True)\n",
        "iid_dict = iid_partition(mnist_data_train, hparams.K)\n",
        "\n",
        "total_clients = len(iid_dict.keys())\n",
        "'Total users:', total_clients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymcomht5GdBO"
      },
      "source": [
        "def run_experiment(run_id):\n",
        "\n",
        "    mnist_cnn = MNIST_CNN()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        mnist_cnn.cuda()\n",
        "    \n",
        "    test_history = []\n",
        "\n",
        "    mnist_cnn_iid_trained, test_history = training(mnist_cnn,\n",
        "                                              hparams.rounds, hparams.batch_size, hparams.lr,\n",
        "                                              mnist_data_train, iid_dict, mnist_data_test,\n",
        "                                              hparams.C, hparams.K, hparams.E, hparams.q,\n",
        "                                              sampling=hparams.sampling,\n",
        "                                              test_history=test_history,\n",
        "                                              # tb_logger=tb_logger,\n",
        "                                              # perf_fig_file=f'{BASE_DIR}/loss.jpg'\n",
        "                                              ) \n",
        "    \n",
        "    final_scores = testing(mnist_cnn_iid_trained, mnist_data_test, hparams.batch_size * 2, nn.CrossEntropyLoss(), num_classes, classes_test)\n",
        "    print(f'\\n\\n========================================================\\n\\n')\n",
        "    print(f'Final scores for Exp {run_id} \\n {final_scores}')\n",
        "\n",
        "    log = {\n",
        "        'history': test_history,\n",
        "        'hyperparams': hparams.__dict__\n",
        "    }\n",
        "\n",
        "    with open(f'{EXP_DIR}/results_iid_cnn_{run_id}.pkl', 'wb') as file:\n",
        "        pickle.dump(log, file)\n",
        "\n",
        "    return test_history\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhPK_WL9GZhV"
      },
      "source": [
        "exp_history = list()\n",
        "for run_id in range(2):  # TOTAL RUNS\n",
        "    print(f'============== RUNNING EXPERIMENT #{run_id} ==============')\n",
        "    exp_history.append(run_experiment(run_id))\n",
        "    print(f'\\n\\n========================================================\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4Rxo_HZn10G"
      },
      "source": [
        "exp_log[title] = {\n",
        "    'history': exp_history,\n",
        "    'hyperparams': hparams.__dict__\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQGqrZ1_n-e3"
      },
      "source": [
        "df = None\n",
        "for i, e in enumerate(exp_history):\n",
        "    if i == 0:\n",
        "        df = pd.json_normalize(e)\n",
        "        continue\n",
        "    df = df + pd.json_normalize(e)\n",
        "    \n",
        "df_avg = df / len(exp_history)\n",
        "avg_history = df_avg.to_dict(orient='records')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3SO1Cga5tLE"
      },
      "source": [
        "plot_scores(history=avg_history, base_dir=EXP_DIR, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwo7EWRh5uQx"
      },
      "source": [
        "plot_losses(history=avg_history, base_dir=EXP_DIR, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KPWHl8128He"
      },
      "source": [
        "# with open(f'{EXP_DIR}/results_iid.pkl', 'wb') as file:\n",
        "#     pickle.dump(exp_log, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttnrJ9FnDxFR"
      },
      "source": [
        "### MNIST CNN Non-IID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T9hXMvT2_Ud"
      },
      "source": [
        "exp_log = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3cGeO20ofEZ"
      },
      "source": [
        "title = 'MNIST CNN on Non-IID Dataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAZaKDj98o15"
      },
      "source": [
        "# Skewness\n",
        "# 100, 200, 250, 2 - Low\n",
        "# 100, 500, 120, 5 - Medium\n",
        "# 100, 1000, 60, 10 - High\n",
        "clients = hparams.K\n",
        "num_shards_per_client = 2\n",
        "total_shards = 200\n",
        "shards_size = 250\n",
        "\n",
        "hparams.shards = num_shards_per_client\n",
        "\n",
        "clients, total_shards, shards_size, num_shards_per_client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQlDBOdJEAaX"
      },
      "source": [
        "niid_dict = non_iid_partition(dataset=mnist_data_train,\n",
        "                                clients=hparams.K,\n",
        "                                total_shards=total_shards,\n",
        "                                shards_size=shards_size,\n",
        "                                num_shards_per_client=num_shards_per_client)\n",
        "\n",
        "total_clients = len(niid_dict.keys())\n",
        "'Total users:', total_clients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu1A3GUjGHWy"
      },
      "source": [
        "def run_experiment(run_id):\n",
        "\n",
        "    mnist_cnn = MNIST_CNN()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        mnist_cnn.cuda()\n",
        "    \n",
        "    test_history = []\n",
        "\n",
        "    mnist_cnn_niid_trained, test_history = training(mnist_cnn,\n",
        "                                              hparams.rounds, hparams.batch_size, hparams.lr,\n",
        "                                              mnist_data_train, niid_dict, mnist_data_test,\n",
        "                                              hparams.C, hparams.K, hparams.E, hparams.q,\n",
        "                                              sampling=hparams.sampling,\n",
        "                                              test_history=test_history,\n",
        "                                              # tb_logger=tb_logger,\n",
        "                                              # perf_fig_file=f'{BASE_DIR}/loss.jpg'\n",
        "                                              ) \n",
        "    \n",
        "    final_scores = testing(mnist_cnn_niid_trained, mnist_data_test, hparams.batch_size * 2, nn.CrossEntropyLoss(), num_classes, classes_test)\n",
        "    print(f'\\n\\n========================================================\\n\\n')\n",
        "    print(f'Final scores for Exp {run_id} \\n {final_scores}')\n",
        "\n",
        "    log = {\n",
        "        'history': test_history,\n",
        "        'hyperparams': hparams.__dict__\n",
        "    }\n",
        "\n",
        "    with open(f'{EXP_DIR}/results_niid_cnn_{run_id}.pkl', 'wb') as file:\n",
        "        pickle.dump(log, file)\n",
        "\n",
        "    return test_history\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaaamxrWGKct"
      },
      "source": [
        "exp_history = list()\n",
        "for run_id in range(2):  # TOTAL RUNS\n",
        "    print(f'============== RUNNING EXPERIMENT #{run_id} ==============')\n",
        "    exp_history.append(run_experiment(run_id))\n",
        "    print(f'\\n\\n========================================================\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftL-MoHxwe5C"
      },
      "source": [
        "exp_log[title] = {\n",
        "    'history': exp_history,\n",
        "    'hyperparams': hparams.__dict__\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jceXDZXOwezj"
      },
      "source": [
        "df = None\n",
        "for i, e in enumerate(exp_history):\n",
        "    if i == 0:\n",
        "        df = pd.json_normalize(e)\n",
        "        continue\n",
        "    df = df + pd.json_normalize(e)\n",
        "    \n",
        "df_avg = df / len(exp_history)\n",
        "avg_history = df_avg.to_dict(orient='records')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u7uTHwJ6KXE"
      },
      "source": [
        "plot_scores(history=avg_history, base_dir=EXP_DIR, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um4eBO8O6KLX"
      },
      "source": [
        "plot_losses(history=avg_history, base_dir=EXP_DIR, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExsUNb2MACKt"
      },
      "source": [
        "### MNIST MLP on IID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "injvzL4sACKv"
      },
      "source": [
        "exp_log = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "worp1iTZACKw"
      },
      "source": [
        "title = 'MNIST MLP on IID Dataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPzEF1yVACKw"
      },
      "source": [
        "# train_ds, data_dict, test_ds = iid_partition(corpus, seq_length, val_split=True)\n",
        "iid_dict = iid_partition(mnist_data_train, hparams.K)\n",
        "\n",
        "total_clients = len(iid_dict.keys())\n",
        "'Total users:', total_clients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE8YGu3eACKx"
      },
      "source": [
        "def run_experiment(run_id):\n",
        "\n",
        "    mnist_mlp = MNIST_2NN()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        mnist_mlp.cuda()\n",
        "    \n",
        "    test_history = []\n",
        "\n",
        "    mnist_mlp_iid_trained, test_history = training(mnist_mlp,\n",
        "                                              hparams.rounds, hparams.batch_size, hparams.lr,\n",
        "                                              mnist_data_train, iid_dict, mnist_data_test,\n",
        "                                              hparams.C, hparams.K, hparams.E, hparams.q,\n",
        "                                              sampling=hparams.sampling,\n",
        "                                              test_history=test_history,\n",
        "                                              # tb_logger=tb_logger,\n",
        "                                              # perf_fig_file=f'{BASE_DIR}/loss.jpg'\n",
        "                                              ) \n",
        "    \n",
        "    final_scores = testing(mnist_mlp_iid_trained, mnist_data_test, hparams.batch_size * 2, nn.CrossEntropyLoss(), num_classes, classes_test)\n",
        "    print(f'\\n\\n========================================================\\n\\n')\n",
        "    print(f'Final scores for Exp {run_id} \\n {final_scores}')\n",
        "\n",
        "    log = {\n",
        "        'history': test_history,\n",
        "        'hyperparams': hparams.__dict__\n",
        "    }\n",
        "\n",
        "    with open(f'{EXP_DIR}/results_iid_mlp_{run_id}.pkl', 'wb') as file:\n",
        "        pickle.dump(log, file)\n",
        "\n",
        "    return test_history\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfeaTiU6ACKx"
      },
      "source": [
        "exp_history = list()\n",
        "for run_id in range(2):  # TOTAL RUNS\n",
        "    print(f'============== RUNNING EXPERIMENT #{run_id} ==============')\n",
        "    exp_history.append(run_experiment(run_id))\n",
        "    print(f'\\n\\n========================================================\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx5pLzLLACKy"
      },
      "source": [
        "exp_log[title] = {\n",
        "    'history': exp_history,\n",
        "    'hyperparams': hparams.__dict__\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB_MiMRNACKy"
      },
      "source": [
        "df = None\n",
        "for i, e in enumerate(exp_history):\n",
        "    if i == 0:\n",
        "        df = pd.json_normalize(e)\n",
        "        continue\n",
        "    df = df + pd.json_normalize(e)\n",
        "    \n",
        "df_avg = df / len(exp_history)\n",
        "avg_history = df_avg.to_dict(orient='records')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6dwSVX5ACKy"
      },
      "source": [
        "plot_scores(history=avg_history, base_dir=EXP_DIR, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trw8BVxNACKy"
      },
      "source": [
        "plot_losses(history=avg_history, base_dir=EXP_DIR, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nG-HXdIyACKz"
      },
      "source": [
        "# with open(f'{EXP_DIR}/results_iid.pkl', 'wb') as file:\n",
        "#     pickle.dump(exp_log, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBuvfxx3AWV2"
      },
      "source": [
        "### MNIST CNN Non-IID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf9d3BtAAWV3"
      },
      "source": [
        "exp_log = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwo1iUPrAWV3"
      },
      "source": [
        "title = 'MNIST MLP on Non-IID Dataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH7AaWtiAWV4"
      },
      "source": [
        "# Skewness\n",
        "# 100, 200, 250, 2 - Low\n",
        "# 100, 500, 120, 5 - Medium\n",
        "# 100, 1000, 60, 10 - High\n",
        "clients = hparams.K\n",
        "num_shards_per_client = 2\n",
        "total_shards = 200\n",
        "shards_size = 250\n",
        "\n",
        "hparams.shards = num_shards_per_client\n",
        "\n",
        "clients, total_shards, shards_size, num_shards_per_client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ileJ4GyAWV4"
      },
      "source": [
        "niid_dict = non_iid_partition(dataset=mnist_data_train,\n",
        "                                clients=hparams.K,\n",
        "                                total_shards=total_shards,\n",
        "                                shards_size=shards_size,\n",
        "                                num_shards_per_client=num_shards_per_client)\n",
        "\n",
        "total_clients = len(niid_dict.keys())\n",
        "'Total users:', total_clients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYAmDeTRAWV4"
      },
      "source": [
        "def run_experiment(run_id):\n",
        "\n",
        "    mnist_mlp = MNIST_2NN()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        mnist_mlp.cuda()\n",
        "    \n",
        "    test_history = []\n",
        "\n",
        "    mnist_mlp_niid_trained, test_history = training(mnist_mlp,\n",
        "                                              hparams.rounds, hparams.batch_size, hparams.lr,\n",
        "                                              mnist_data_train, niid_dict, mnist_data_test,\n",
        "                                              hparams.C, hparams.K, hparams.E, hparams.q,\n",
        "                                              sampling=hparams.sampling,\n",
        "                                              test_history=test_history,\n",
        "                                              # tb_logger=tb_logger,\n",
        "                                              # perf_fig_file=f'{BASE_DIR}/loss.jpg'\n",
        "                                              ) \n",
        "    \n",
        "    final_scores = testing(mnist_mlp_niid_trained, mnist_data_test, hparams.batch_size * 2, nn.CrossEntropyLoss(), num_classes, classes_test)\n",
        "    print(f'\\n\\n========================================================\\n\\n')\n",
        "    print(f'Final scores for Exp {run_id} \\n {final_scores}')\n",
        "\n",
        "    log = {\n",
        "        'history': test_history,\n",
        "        'hyperparams': hparams.__dict__\n",
        "    }\n",
        "\n",
        "    with open(f'{EXP_DIR}/results_niid_mlp_{run_id}.pkl', 'wb') as file:\n",
        "        pickle.dump(log, file)\n",
        "\n",
        "    return test_history\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDVOuVkxAWV5"
      },
      "source": [
        "exp_history = list()\n",
        "for run_id in range(2):  # TOTAL RUNS\n",
        "    print(f'============== RUNNING EXPERIMENT #{run_id} ==============')\n",
        "    exp_history.append(run_experiment(run_id))\n",
        "    print(f'\\n\\n========================================================\\n\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMqoIqjLAWV5"
      },
      "source": [
        "exp_log[title] = {\n",
        "    'history': exp_history,\n",
        "    'hyperparams': hparams.__dict__\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZELjYeQbAWV6"
      },
      "source": [
        "df = None\n",
        "for i, e in enumerate(exp_history):\n",
        "    if i == 0:\n",
        "        df = pd.json_normalize(e)\n",
        "        continue\n",
        "    df = df + pd.json_normalize(e)\n",
        "    \n",
        "df_avg = df / len(exp_history)\n",
        "avg_history = df_avg.to_dict(orient='records')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NP91nStAWV7"
      },
      "source": [
        "plot_scores(history=avg_history, base_dir=EXP_DIR, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAYLR1JpAWV7"
      },
      "source": [
        "plot_losses(history=avg_history, base_dir=EXP_DIR, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N80BpTFy6aR7"
      },
      "source": [
        "### Pickle Experiment Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGlR8COy6aCN"
      },
      "source": [
        "# with open(f'{EXP_DIR}/results_niid.pkl', 'wb') as file:\n",
        "#     pickle.dump(exp_log, file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}