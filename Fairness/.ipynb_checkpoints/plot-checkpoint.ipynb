{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, RegularPolygon\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.projections.polar import PolarAxes\n",
    "from matplotlib.projections import register_projection\n",
    "from matplotlib.spines import Spine\n",
    "from matplotlib.transforms import Affine2D\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 15})\n",
    "\n",
    "\n",
    "methods = [\"FedAvg\", \"FedMed\", \"FedProx\", \"qFedAvg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fairness(dataset, NUM_REPS):\n",
    "    ROUNDS = 50\n",
    "    NUM_CLIENTS = 100\n",
    "    exp_matches = [\n",
    "        \" CNN on IID\",\n",
    "        \" CNN on Non IID\",\n",
    "        \" MLP on IID\",\n",
    "        \" MLP on Non IID\",\n",
    "        \" LSTM on IID\",\n",
    "        \" LSTM on Non IID\",\n",
    "    ]\n",
    "\n",
    "    for exp_match in exp_matches:\n",
    "        plt.figure(figsize=[8, 6])\n",
    "        for method in methods:\n",
    "            pickle_files = glob.glob(f\"./{dataset}/{method}/*.pkl\")\n",
    "            print(pickle_files)\n",
    "\n",
    "            if not pickle_files:\n",
    "                continue\n",
    "\n",
    "            for pickle_file in pickle_files:\n",
    "                with open(pickle_file, \"rb\") as file:\n",
    "                    log_dict = pickle.load(file)\n",
    "\n",
    "                for experiment in log_dict.keys():\n",
    "                    if experiment.endswith(exp_match):\n",
    "                        print(experiment)\n",
    "\n",
    "                        if \"Non IID\" in experiment:\n",
    "                            IS_IID = \"Non_IID\"\n",
    "                        else:\n",
    "                            IS_IID = \"IID\"\n",
    "\n",
    "                        final_accuracy = [0] * NUM_CLIENTS\n",
    "                        final_accuracy_count = [0] * NUM_CLIENTS\n",
    "\n",
    "                        for rep in range(NUM_REPS):\n",
    "                            for client in range(NUM_CLIENTS):\n",
    "                                if not log_dict[experiment][\"test_accuracy_clients\"][rep][client]:\n",
    "                                    continue\n",
    "\n",
    "                                final_accuracy[client] += log_dict[experiment][\"test_accuracy_clients\"][rep][client][\n",
    "                                    -1\n",
    "                                ][1]\n",
    "                                final_accuracy_count[client] += 1\n",
    "\n",
    "                        final_accuracy = [\n",
    "                            accuracy / count if count else 0\n",
    "                            for accuracy, count in zip(final_accuracy, final_accuracy_count)\n",
    "                        ]\n",
    "\n",
    "                        if dataset == \"Shakespeare\":\n",
    "                            final_accuracy = [accuracy * 100 for accuracy in final_accuracy]\n",
    "\n",
    "                        method_name = pickle_file.split(\"/\")[-1].split(\".\")[0].replace(\"Fairness_\", \"\")\n",
    "\n",
    "                        plt.grid(True)\n",
    "                        plt.hist(final_accuracy, bins=20)\n",
    "\n",
    "                        # plt.title(f\"Fairness\")\n",
    "                        plt.xlabel(\"Test accuracy\")\n",
    "                        plt.ylabel(\"Number of clients\")\n",
    "\n",
    "                        plt.tight_layout()\n",
    "                        plt.savefig(\n",
    "                            f\"./results/{dataset}/Histograms/{IS_IID}/{experiment}_{method_name}.svg\",\n",
    "                            format=\"svg\",\n",
    "                            dpi=1000,\n",
    "                        )\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fairness_stacked_error_bar_plot(dataset, NUM_REPS):\n",
    "    ROUNDS = 50\n",
    "    NUM_CLIENTS = 100\n",
    "\n",
    "    final_entropy_mean_tracker = {}\n",
    "    final_entropy_std_tracker = {}\n",
    "    final_entropy_max_tracker = {}\n",
    "    final_entropy_min_tracker = {}\n",
    "\n",
    "    for method in methods:\n",
    "        pickle_files = glob.glob(f\"./{dataset}/{method}/*.pkl\")\n",
    "        print(pickle_files)\n",
    "\n",
    "        if not pickle_files:\n",
    "            continue\n",
    "\n",
    "        for pickle_file in pickle_files:\n",
    "            with open(pickle_file, \"rb\") as file:\n",
    "                log_dict = pickle.load(file)\n",
    "\n",
    "            for experiment in log_dict.keys():\n",
    "                print(experiment)\n",
    "                final_accuracy = [0] * NUM_CLIENTS\n",
    "                entropy_runs = [None] * NUM_REPS\n",
    "\n",
    "                for rep in range(NUM_REPS):\n",
    "                    for client in range(NUM_CLIENTS):\n",
    "                        if not log_dict[experiment][\"test_accuracy_clients\"][rep][client]:\n",
    "                            continue\n",
    "\n",
    "                        final_accuracy[client] = log_dict[experiment][\"test_accuracy_clients\"][rep][client][-1][1]\n",
    "\n",
    "                    if dataset == \"Shakespeare\":\n",
    "                        final_accuracy = [accuracy * 100 for accuracy in final_accuracy]\n",
    "\n",
    "                    # print(method, experiment, len(final_accuracy), final_accuracy)\n",
    "\n",
    "                    # histogram = np.histogram(final_accuracy, bins=1000, range=(0, 100), density=True)\n",
    "                    # data_distribution = histogram[0]\n",
    "                    # entropy = -(data_distribution * np.ma.log(np.abs(data_distribution))).sum()\n",
    "\n",
    "                    histogram = np.asarray(np.histogram(final_accuracy, bins=1000, range=(0, 100), density=False)[0])\n",
    "                    data_distribution = histogram / histogram.sum()\n",
    "                    entropy = -(data_distribution * np.ma.log2(np.abs(data_distribution))).sum()\n",
    "\n",
    "                    entropy_runs[rep] = entropy\n",
    "\n",
    "                method_name = pickle_file.split(\"/\")[-1].split(\".\")[0].replace(\"Fairness_\", \"\")\n",
    "\n",
    "                histogram = np.asarray([1 / histogram.shape[0] for _ in range(histogram.shape[0])])\n",
    "                data_distribution = histogram / histogram.sum()\n",
    "                max_entropy = -(data_distribution * np.ma.log2(np.abs(data_distribution))).sum()\n",
    "\n",
    "                entropy_runs = np.array(entropy_runs)\n",
    "                final_entropy_mean_tracker[f\"{method_name}-{experiment}\"] = np.mean(entropy_runs, axis=0)\n",
    "                final_entropy_std_tracker[f\"{method_name}-{experiment}\"] = np.std(entropy_runs, axis=0)\n",
    "                final_entropy_max_tracker[f\"{method_name}-{experiment}\"] = np.max(entropy_runs, axis=0)\n",
    "                final_entropy_min_tracker[f\"{method_name}-{experiment}\"] = np.min(entropy_runs, axis=0)\n",
    "\n",
    "    exp_matches = [\" on IID\", \" on Non IID\"]\n",
    "\n",
    "    for exp_match in exp_matches:\n",
    "        plt.figure(figsize=[8, 6])\n",
    "        if \"Non IID\" in exp_match:\n",
    "            IS_IID = \"Non_IID\"\n",
    "        else:\n",
    "            IS_IID = \"IID\"\n",
    "\n",
    "        final_acc_mean_list = []\n",
    "        final_acc_std_list = []\n",
    "        final_acc_max_list = []\n",
    "        final_acc_min_list = []\n",
    "        key_list = []\n",
    "        for key in final_entropy_mean_tracker:\n",
    "            if \"MLP\" in key:\n",
    "                continue\n",
    "\n",
    "            if key.endswith(exp_match):\n",
    "                key_list.append(\n",
    "                    key.replace(f\"-{dataset}\", \"\")\n",
    "                    .replace(exp_match, \"\")\n",
    "                    .replace(\"uniform\", \"u\")\n",
    "                    .replace(\"weighted\", \"w\")\n",
    "                )\n",
    "                final_acc_mean_list.append(final_entropy_mean_tracker[key])\n",
    "                final_acc_std_list.append(final_entropy_std_tracker[key])\n",
    "                final_acc_max_list.append(final_entropy_max_tracker[key])\n",
    "                final_acc_min_list.append(final_entropy_min_tracker[key])\n",
    "                # print(key, final_entropy_mean_tracker[key])\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.errorbar(\n",
    "            np.arange(len(key_list)), np.array(final_acc_mean_list), np.array(final_acc_std_list), fmt=\"ok\", lw=3\n",
    "        )\n",
    "        plt.errorbar(\n",
    "            np.arange(len(key_list)),\n",
    "            np.array(final_acc_mean_list),\n",
    "            [\n",
    "                np.array(final_acc_mean_list) - np.array(final_acc_min_list),\n",
    "                np.array(final_acc_max_list) - np.array(final_acc_mean_list),\n",
    "            ],\n",
    "            fmt=\".k\",\n",
    "            ecolor=\"black\",\n",
    "            lw=1,\n",
    "        )\n",
    "\n",
    "        plt.errorbar(\n",
    "            np.arange(len(key_list)),\n",
    "            np.array([max_entropy for _ in range(len(key_list))]),\n",
    "            [\n",
    "                np.array([0 for _ in range(len(key_list))]),\n",
    "                np.array([0 for _ in range(len(key_list))]),\n",
    "            ],\n",
    "            fmt=\"D\",\n",
    "            color=\"red\",\n",
    "            lw=1,\n",
    "        )\n",
    "\n",
    "        plt.xticks(np.arange(len(key_list)), key_list, rotation=\"45\", ha=\"right\")\n",
    "        # plt.title(f\"Fairness\")\n",
    "        plt.xlabel(\"Algorithms\")\n",
    "        plt.ylabel(\"Entropy\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            f\"./results/{dataset}/Stacked_Error_Bar/{IS_IID}/{dataset}{exp_match}.svg\", format=\"svg\", dpi=1000\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fairness_histogram(dataset, NUM_REPS, exp_match):\n",
    "    ROUNDS = 50\n",
    "    NUM_CLIENTS = 100\n",
    "\n",
    "    method_match = exp_match.split(\"-\")[0].split(\"_\")[0]\n",
    "    method_exp_match = exp_match.split(\"-\")[0]\n",
    "\n",
    "    for method in methods:\n",
    "        if method != method_match:\n",
    "            continue\n",
    "\n",
    "        pickle_files = glob.glob(f\"./{dataset}/{method}/*.pkl\")\n",
    "        if not pickle_files:\n",
    "            continue\n",
    "\n",
    "        for pickle_file in pickle_files:\n",
    "            if method_exp_match not in pickle_file:\n",
    "                continue\n",
    "\n",
    "            with open(pickle_file, \"rb\") as file:\n",
    "                log_dict = pickle.load(file)\n",
    "\n",
    "            for experiment in log_dict.keys():\n",
    "                if exp_match.endswith(experiment):\n",
    "                    # if 'Non IID' in experiment:\n",
    "                    #     IS_IID = 'Non_IID'\n",
    "                    # else:\n",
    "                    #     IS_IID = 'IID'\n",
    "                    final_accuracy = [0] * NUM_CLIENTS\n",
    "                    final_accuracy_count = [0] * NUM_CLIENTS\n",
    "\n",
    "                    for rep in range(NUM_REPS):\n",
    "                        for client in range(NUM_CLIENTS):\n",
    "                            if not log_dict[experiment][\"test_accuracy_clients\"][rep][client]:\n",
    "                                continue\n",
    "\n",
    "                            final_accuracy[client] += log_dict[experiment][\"test_accuracy_clients\"][rep][client][-1][1]\n",
    "                            final_accuracy_count[client] += 1\n",
    "\n",
    "                    final_accuracy = [\n",
    "                        accuracy / count if count else 0\n",
    "                        for accuracy, count in zip(final_accuracy, final_accuracy_count)\n",
    "                    ]\n",
    "\n",
    "                    if dataset == \"Shakespeare\":\n",
    "                        final_accuracy = [accuracy * 100 for accuracy in final_accuracy]\n",
    "\n",
    "                    return final_accuracy\n",
    "\n",
    "\n",
    "def plot_fairness_stacked_error_bar_plot_with_distribution(dataset, NUM_REPS):\n",
    "    ROUNDS = 50\n",
    "    NUM_CLIENTS = 100\n",
    "\n",
    "    final_entropy_mean_tracker = {}\n",
    "    final_entropy_std_tracker = {}\n",
    "    final_entropy_max_tracker = {}\n",
    "    final_entropy_min_tracker = {}\n",
    "    histogram_tracker = {}\n",
    "\n",
    "    for method in methods:\n",
    "        pickle_files = glob.glob(f\"./{dataset}/{method}/*.pkl\")\n",
    "        print(pickle_files)\n",
    "\n",
    "        if not pickle_files:\n",
    "            continue\n",
    "\n",
    "        for pickle_file in pickle_files:\n",
    "            with open(pickle_file, \"rb\") as file:\n",
    "                log_dict = pickle.load(file)\n",
    "\n",
    "            for experiment in log_dict.keys():\n",
    "                print(experiment)\n",
    "                final_accuracy = [0] * NUM_CLIENTS\n",
    "                entropy_runs = [None] * NUM_REPS\n",
    "\n",
    "                for rep in range(NUM_REPS):\n",
    "                    for client in range(NUM_CLIENTS):\n",
    "                        if not log_dict[experiment][\"test_accuracy_clients\"][rep][client]:\n",
    "                            continue\n",
    "\n",
    "                        final_accuracy[client] = log_dict[experiment][\"test_accuracy_clients\"][rep][client][-1][1]\n",
    "\n",
    "                    if dataset == \"Shakespeare\":\n",
    "                        final_accuracy = [accuracy * 100 for accuracy in final_accuracy]\n",
    "\n",
    "                    # print(method, experiment, len(final_accuracy), final_accuracy)\n",
    "\n",
    "                    # histogram = np.histogram(final_accuracy, bins=1000, range=(0, 100), density=True)\n",
    "                    # data_distribution = histogram[0]\n",
    "                    # entropy = -(data_distribution * np.ma.log(np.abs(data_distribution))).sum()\n",
    "\n",
    "                    histogram = np.asarray(np.histogram(final_accuracy, bins=1000, range=(0, 100), density=False)[0])\n",
    "                    data_distribution = histogram / histogram.sum()\n",
    "                    entropy = -(data_distribution * np.ma.log2(np.abs(data_distribution))).sum()\n",
    "\n",
    "                    entropy_runs[rep] = entropy\n",
    "\n",
    "                method_name = pickle_file.split(\"/\")[-1].split(\".\")[0].replace(\"Fairness_\", \"\")\n",
    "\n",
    "                histogram = np.asarray([1 / histogram.shape[0] for _ in range(histogram.shape[0])])\n",
    "                data_distribution = histogram / histogram.sum()\n",
    "                max_entropy = -(data_distribution * np.ma.log2(np.abs(data_distribution))).sum()\n",
    "\n",
    "                entropy_runs = np.array(entropy_runs)\n",
    "\n",
    "                method_name_shorted = method_name.replace(\"uniform\", \"u\").replace(\"weighted\", \"w\")\n",
    "                histogram_tracker[f\"{method_name_shorted}-{experiment}\"] = get_fairness_histogram(\n",
    "                    dataset, NUM_REPS, f\"{method_name}-{experiment}\"\n",
    "                )\n",
    "                final_entropy_mean_tracker[f\"{method_name_shorted}-{experiment}\"] = np.mean(entropy_runs, axis=0)\n",
    "                final_entropy_std_tracker[f\"{method_name_shorted}-{experiment}\"] = np.std(entropy_runs, axis=0)\n",
    "                final_entropy_max_tracker[f\"{method_name_shorted}-{experiment}\"] = np.max(entropy_runs, axis=0)\n",
    "                final_entropy_min_tracker[f\"{method_name_shorted}-{experiment}\"] = np.min(entropy_runs, axis=0)\n",
    "\n",
    "    exp_matches = [\" on IID\", \" on Non IID\"]\n",
    "\n",
    "    for exp_match in exp_matches:\n",
    "        plt.figure(figsize=[8, 6])\n",
    "        if \"Non IID\" in exp_match:\n",
    "            IS_IID = \"Non_IID\"\n",
    "        else:\n",
    "            IS_IID = \"IID\"\n",
    "\n",
    "        final_acc_mean_list = []\n",
    "        final_acc_std_list = []\n",
    "        final_acc_max_list = []\n",
    "        final_acc_min_list = []\n",
    "        key_list = []\n",
    "        for key in final_entropy_mean_tracker:\n",
    "            if \"MLP\" in key:\n",
    "                continue\n",
    "\n",
    "            if key.endswith(exp_match):\n",
    "                key_list.append(\n",
    "                    key.replace(f\"-{dataset}\", \"\")\n",
    "                    .replace(exp_match, \"\")\n",
    "                    .replace(\"uniform\", \"u\")\n",
    "                    .replace(\"weighted\", \"w\")\n",
    "                )\n",
    "                final_acc_mean_list.append(final_entropy_mean_tracker[key])\n",
    "                final_acc_std_list.append(final_entropy_std_tracker[key])\n",
    "                final_acc_max_list.append(final_entropy_max_tracker[key])\n",
    "                final_acc_min_list.append(final_entropy_min_tracker[key])\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.margins(0.1)\n",
    "\n",
    "        plt.errorbar(\n",
    "            np.arange(len(key_list)), np.array(final_acc_mean_list), np.array(final_acc_std_list), fmt=\"ok\", lw=3\n",
    "        )\n",
    "        plt.errorbar(\n",
    "            np.arange(len(key_list)),\n",
    "            np.array(final_acc_mean_list),\n",
    "            [\n",
    "                np.array(final_acc_mean_list) - np.array(final_acc_min_list),\n",
    "                np.array(final_acc_max_list) - np.array(final_acc_mean_list),\n",
    "            ],\n",
    "            fmt=\".k\",\n",
    "            ecolor=\"black\",\n",
    "            lw=1,\n",
    "        )\n",
    "\n",
    "        plt.errorbar(\n",
    "            np.arange(len(key_list)),\n",
    "            np.array([max_entropy for _ in range(len(key_list))]),\n",
    "            [\n",
    "                np.array([0 for _ in range(len(key_list))]),\n",
    "                np.array([0 for _ in range(len(key_list))]),\n",
    "            ],\n",
    "            fmt=\"D\",\n",
    "            color=\"red\",\n",
    "            lw=1,\n",
    "        )\n",
    "\n",
    "        plt.xticks(np.arange(len(key_list)), key_list, rotation=\"45\", ha=\"right\")\n",
    "\n",
    "        ax = plt.gca()\n",
    "        plt.gcf().canvas.draw()\n",
    "        ticks = [tick for tick in plt.gca().get_xticklabels()]\n",
    "\n",
    "        for i, t in enumerate(ticks):\n",
    "            method = t.get_text().split()[0]\n",
    "            model = t.get_text().split()[1]\n",
    "\n",
    "            # bbox = t.get_window_extent().transformed(plt.gca().transData.inverted())\n",
    "\n",
    "            # if dataset == \"MNIST\":\n",
    "            #     ax_ins  = ax.inset_axes([i * 0.064 + 0.06, 0.58, 0.05, 0.25])\n",
    "            # else:\n",
    "            #     ax_ins  = ax.inset_axes([i * 0.1395 + 0.03, 0.55, 0.1, 0.25])\n",
    "\n",
    "            # print(histogram_tracker[f\"{method}-{dataset} {model}{exp_match}\"])\n",
    "            ax_ins = ax.inset_axes([i * 0.1395 + 0.03, 0.55, 0.1, 0.25])\n",
    "            ax_ins.hist(histogram_tracker[f\"{method}-{dataset} {model}{exp_match}\"], bins=25)\n",
    "            ax_ins.set_xticks([0, 100])\n",
    "            ax_ins.xaxis.set_visible(False)\n",
    "            ax_ins.yaxis.set_visible(False)\n",
    "\n",
    "        # plt.title(f\"Fairness\")\n",
    "        plt.xlabel(\"Algorithms\")\n",
    "        plt.ylabel(\"Entropy\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            f\"./results/{dataset}/Stacked_Error_Bar_With_Dist/{IS_IID}/{dataset}{exp_match}.svg\",\n",
    "            format=\"svg\",\n",
    "            dpi=1000,\n",
    "        )\n",
    "        plt.savefig(\n",
    "            f\"../results/Fairness/{dataset}/{IS_IID}.svg\",\n",
    "            format=\"svg\",\n",
    "            dpi=1000,\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fairness(\"MNIST\", 5)\n",
    "plot_fairness(\"CIFAR\", 5)\n",
    "plot_fairness(\"Shakespeare\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fairness_stacked_error_bar_plot(\"MNIST\", 5)\n",
    "plot_fairness_stacked_error_bar_plot(\"CIFAR\", 5)\n",
    "plot_fairness_stacked_error_bar_plot(\"Shakespeare\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fairness_stacked_error_bar_plot_with_distribution(\"MNIST\", 5)\n",
    "plot_fairness_stacked_error_bar_plot_with_distribution(\"CIFAR\", 5)\n",
    "plot_fairness_stacked_error_bar_plot_with_distribution(\"Shakespeare\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_Asgn",
   "language": "python",
   "name": "dl_asgn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
