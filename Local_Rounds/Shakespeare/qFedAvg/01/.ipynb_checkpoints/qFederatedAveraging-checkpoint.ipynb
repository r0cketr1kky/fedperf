{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tushar-semwal/fedperf/blob/main/Vineeth/Local_Rounds/Shakesphere/qFedAvg/qFederatedAveraging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WQ6Rq0UiG6ev"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummaryX in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (1.3.0)\n",
      "Requirement already satisfied: unidecode in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (1.2.0)\n",
      "Requirement already satisfied: pandas in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (from torchsummaryX) (0.24.2)\n",
      "Requirement already satisfied: torch in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (from torchsummaryX) (1.7.1+cu101)\n",
      "Requirement already satisfied: numpy in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (from torchsummaryX) (1.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (from pandas->torchsummaryX) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2011k in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (from pandas->torchsummaryX) (2021.1)\n",
      "Requirement already satisfied: typing-extensions in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (from torch->torchsummaryX) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas->torchsummaryX) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummaryX unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yKcpjZLrQQJV"
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    import os\n",
    "    path = '/content/drive/MyDrive/Colab Notebooks/OpenMined/'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "except:\n",
    "    path = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0_nKpfq2h1R"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DLLNM9X2JbQ8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 27 10:50:38 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 23%   42C    P2    66W / 250W |   1101MiB / 11178MiB |     12%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "| 32%   54C    P2   121W / 250W |   1574MiB / 11178MiB |     53%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    163076      C   ...3/envs/QSGD-PT/bin/python     1099MiB |\n",
      "|    1   N/A  N/A     73673      C   ...3/envs/QSGD-PT/bin/python      791MiB |\n",
      "|    1   N/A  N/A     77229      C   ...3/envs/QSGD-PT/bin/python      781MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import copy\n",
    "from functools import reduce\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from torchsummaryX import summary as summaryx\n",
    "from torchvision import transforms, utils, datasets\n",
    "from tqdm.notebook import tqdm\n",
    "from unidecode import unidecode\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Check assigned GPU\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "\n",
    "# set manual seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# general reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# gpu training specific\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY4eWzGiL6Mj"
   },
   "source": [
    "## Load the Shakespeare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hf03LRxof7Zj"
   },
   "outputs": [],
   "source": [
    "!rm -Rf data\n",
    "!mkdir -p data scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ngygA4-Fgobx"
   },
   "outputs": [],
   "source": [
    "GENERATE_DATASET = False  # If False, download the dataset provided by the q-FFL paper\n",
    "DATA_DIR = 'data/'\n",
    "# Dataset generation params\n",
    "SAMPLES_FRACTION = 1.  # If using an already generated dataset\n",
    "# SAMPLES_FRACTION = 0.2  # Fraction of total samples in the dataset - FedProx default script\n",
    "# SAMPLES_FRACTION = 0.05  # Fraction of total samples in the dataset - qFFL\n",
    "TRAIN_FRACTION = 0.8  # Train set size\n",
    "MIN_SAMPLES = 0  # Min samples per client (for filtering purposes) - FedProx\n",
    "# MIN_SAMPLES = 64  # Min samples per client (for filtering purposes) - qFFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nUmwJgJygoYD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-27 10:50:38--  http://www.gutenberg.org/files/100/100-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5757108 (5.5M) [text/plain]\n",
      "Saving to: ‘data/shakespeare.txt’\n",
      "\n",
      "data/shakespeare.tx 100%[===================>]   5.49M  1.02MB/s    in 20s     \n",
      "\n",
      "2021-04-27 10:51:00 (283 KB/s) - ‘data/shakespeare.txt’ saved [5757108/5757108]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download raw dataset\n",
    "# !wget https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt -O data/shakespeare.txt\n",
    "!wget --adjust-extension http://www.gutenberg.org/files/100/100-0.txt -O data/shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4dCvx80BgoVr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1n46Mftp3_ahRi1Z6jYhEriyLtdRDS1tD\n",
      "To: /home/vineeth/code/OM/Vineeth/Local_Rounds/Shakespeare/qFedAvg/01/shakespeare.zip\n",
      "2.96MB [00:02, 1.04MB/s]\n",
      "Archive:  shakespeare.zip\n",
      "   creating: shakespeare_paper/\n",
      "   creating: shakespeare_paper/test/\n",
      "  inflating: shakespeare_paper/test/all_data_niid_2_keep_0_test_8.json  \n",
      "   creating: shakespeare_paper/train/\n",
      "  inflating: shakespeare_paper/train/all_data_niid_2_keep_0_train_8.json  \n"
     ]
    }
   ],
   "source": [
    "if not GENERATE_DATASET:\n",
    "    !rm -Rf data/train data/test\n",
    "    !gdown --id 1n46Mftp3_ahRi1Z6jYhEriyLtdRDS1tD  # Download Shakespeare dataset used by the FedProx paper\n",
    "    !unzip shakespeare.zip\n",
    "    !mv -f shakespeare_paper/train data/\n",
    "    !mv -f shakespeare_paper/test data/\n",
    "    !rm -R shakespeare_paper/ shakespeare.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "a4pzFvPvhQhq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Length: 90\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    data = list(unidecode(f.read()))\n",
    "    corpus = list(set(list(data)))\n",
    "print('Corpus Length:', len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cce_-qnxhD4n"
   },
   "source": [
    "#### Dataset Preprocessing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Rt13M4IcgoTV"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "if GENERATE_DATASET:\n",
    "    # Download dataset generation scripts\n",
    "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/data/shakespeare/preprocess/preprocess_shakespeare.py -O scripts/preprocess_shakespeare.py\n",
    "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/data/shakespeare/preprocess/shake_utils.py -O scripts/shake_utils.py\n",
    "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/data/shakespeare/preprocess/gen_all_data.py -O scripts/gen_all_data.py\n",
    "\n",
    "    # Download data preprocessing scripts\n",
    "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/utils/sample.py -O scripts/sample.py\n",
    "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/utils/remove_users.py -O scripts/remove_users.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EIEyRW27goPo"
   },
   "outputs": [],
   "source": [
    "# Running scripts\n",
    "if GENERATE_DATASET:\n",
    "    !mkdir -p data/raw_data data/all_data data/train data/test\n",
    "    !python scripts/preprocess_shakespeare.py data/shakespeare.txt data/raw_data\n",
    "    !python scripts/gen_all_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mq8V6v_4hhhD"
   },
   "source": [
    "#### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "H2SjEBKoWDxv"
   },
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, x, y, corpus, seq_length):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.corpus = corpus\n",
    "        self.corpus_size = len(self.corpus)\n",
    "        super(ShakespeareDataset, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__} - (length: {self.__len__()})'\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        input_seq = self.x[i]\n",
    "        next_char = self.y[i]\n",
    "        # print('\\tgetitem', i, input_seq, next_char)\n",
    "        input_value = self.text2charindxs(input_seq).long()\n",
    "        target_value = self.get_label_from_char(next_char)\n",
    "        return input_value, target_value\n",
    "\n",
    "    def text2charindxs(self, text):\n",
    "        tensor = torch.zeros(len(text), dtype=torch.int32)\n",
    "        for i, c in enumerate(text):\n",
    "            tensor[i] = self.get_label_from_char(c)\n",
    "        return tensor\n",
    "\n",
    "    def get_label_from_char(self, c):\n",
    "        return self.corpus.index(c)\n",
    "\n",
    "    def get_char_from_label(self, l):\n",
    "        return self.corpus[l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fgJtS62lYAN"
   },
   "source": [
    "##### Federated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5DqL5pTmgn5X"
   },
   "outputs": [],
   "source": [
    "class ShakespeareFedDataset(ShakespeareDataset):\n",
    "    def __init__(self, x, y, corpus, seq_length):\n",
    "        super(ShakespeareFedDataset, self).__init__(x, y, corpus, seq_length)\n",
    "\n",
    "    def dataloader(self, batch_size, shuffle=True):\n",
    "        return DataLoader(self,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XelbyPsDlfgb"
   },
   "source": [
    "## Partitioning & Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOBblyFGlwlU"
   },
   "source": [
    "### IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "cSZFWKmsgn1p"
   },
   "outputs": [],
   "source": [
    "def iid_partition_(dataset, clients):\n",
    "  \"\"\"\n",
    "  I.I.D paritioning of data over clients\n",
    "  Shuffle the data\n",
    "  Split it between clients\n",
    "  \n",
    "  params:\n",
    "    - dataset (torch.utils.Dataset): Dataset\n",
    "    - clients (int): Number of Clients to split the data between\n",
    "\n",
    "  returns:\n",
    "    - Dictionary of image indexes for each client\n",
    "  \"\"\"\n",
    "\n",
    "  num_items_per_client = int(len(dataset)/clients)\n",
    "  client_dict = {}\n",
    "  image_idxs = [i for i in range(len(dataset))]\n",
    "\n",
    "  for i in range(clients):\n",
    "    client_dict[i] = set(np.random.choice(image_idxs, num_items_per_client, replace=False))\n",
    "    image_idxs = list(set(image_idxs) - client_dict[i])\n",
    "\n",
    "  return client_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-lGwDyhSll9h"
   },
   "outputs": [],
   "source": [
    "def iid_partition(corpus, seq_length=80, val_split=False):\n",
    "\n",
    "    train_file = [os.path.join(DATA_DIR, 'train', f) for f in os.listdir(f'{DATA_DIR}/train') if f.endswith('.json')][0]\n",
    "    test_file = [os.path.join(DATA_DIR, 'test', f) for f in os.listdir(f'{DATA_DIR}/test') if f.endswith('.json')][0]\n",
    "\n",
    "    with open(train_file, 'r') as file:\n",
    "        data_train = json.loads(unidecode(file.read()))\n",
    "\n",
    "    with open(test_file, 'r') as file:\n",
    "        data_test = json.loads(unidecode(file.read()))\n",
    "\n",
    "    \n",
    "    total_samples_train = sum(data_train['num_samples'])\n",
    "\n",
    "    data_dict = {}\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    x_test, y_test = [], []\n",
    "    # x_val, y_val = [], []\n",
    "\n",
    "    users = list(zip(data_train['users'], data_train['num_samples']))\n",
    "    # random.shuffle(users)\n",
    "\n",
    "\n",
    "\n",
    "    total_samples = int(sum(data_train['num_samples']) * SAMPLES_FRACTION)\n",
    "    print('Objective', total_samples, '/', sum(data_train['num_samples']))\n",
    "    sample_count = 0\n",
    "    \n",
    "    for i, (author_id, samples) in enumerate(users):\n",
    "\n",
    "        if sample_count >= total_samples:\n",
    "            print('Max samples reached', sample_count, '/', total_samples)\n",
    "            break\n",
    "\n",
    "        if samples < MIN_SAMPLES: # or data_train['num_samples'][i] > 10000:\n",
    "            print('SKIP', author_id, samples)\n",
    "            continue\n",
    "        else:\n",
    "            udata_train = data_train['user_data'][author_id]\n",
    "            max_samples = samples if (sample_count + samples) <= total_samples else (sample_count + samples - total_samples) \n",
    "            \n",
    "            sample_count += max_samples\n",
    "            # print('sample_count', sample_count)\n",
    "\n",
    "            x_train.extend(data_train['user_data'][author_id]['x'][:max_samples])\n",
    "            y_train.extend(data_train['user_data'][author_id]['y'][:max_samples])\n",
    "\n",
    "            author_data = data_test['user_data'][author_id]\n",
    "            test_size = int(len(author_data['x']) * SAMPLES_FRACTION)\n",
    "\n",
    "            if val_split:\n",
    "                x_test.extend(author_data['x'][:int(test_size / 2)])\n",
    "                y_test.extend(author_data['y'][:int(test_size / 2)])\n",
    "                # x_val.extend(author_data['x'][int(test_size / 2):])\n",
    "                # y_val.extend(author_data['y'][int(test_size / 2):int(test_size)])\n",
    "\n",
    "            else:\n",
    "                x_test.extend(author_data['x'][:int(test_size)])\n",
    "                y_test.extend(author_data['y'][:int(test_size)])\n",
    "\n",
    "    train_ds = ShakespeareDataset(x_train, y_train, corpus, seq_length)\n",
    "    test_ds = ShakespeareDataset(x_test, y_test, corpus, seq_length)\n",
    "    # val_ds = ShakespeareDataset(x_val, y_val, corpus, seq_length)\n",
    "\n",
    "    data_dict = iid_partition_(train_ds, clients=len(users))\n",
    "\n",
    "    return train_ds, data_dict, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFvc8mLoouKa"
   },
   "source": [
    "### Non-IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GZ76WsCZot9s"
   },
   "outputs": [],
   "source": [
    "def noniid_partition(corpus, seq_length=80, val_split=False):\n",
    "\n",
    "    train_file = [os.path.join(DATA_DIR, 'train', f) for f in os.listdir(f'{DATA_DIR}/train') if f.endswith('.json')][0]\n",
    "    test_file = [os.path.join(DATA_DIR, 'test', f) for f in os.listdir(f'{DATA_DIR}/test') if f.endswith('.json')][0]\n",
    "\n",
    "    with open(train_file, 'r') as file:\n",
    "        data_train = json.loads(unidecode(file.read()))\n",
    "\n",
    "    with open(test_file, 'r') as file:\n",
    "        data_test = json.loads(unidecode(file.read()))\n",
    "\n",
    "    \n",
    "    total_samples_train = sum(data_train['num_samples'])\n",
    "\n",
    "    data_dict = {}\n",
    "\n",
    "    x_test, y_test = [], []\n",
    "\n",
    "    users = list(zip(data_train['users'], data_train['num_samples']))\n",
    "    # random.shuffle(users)\n",
    "\n",
    "    total_samples = int(sum(data_train['num_samples']) * SAMPLES_FRACTION)\n",
    "    print('Objective', total_samples, '/', sum(data_train['num_samples']))\n",
    "    sample_count = 0\n",
    "    \n",
    "    for i, (author_id, samples) in enumerate(users):\n",
    "\n",
    "        if sample_count >= total_samples:\n",
    "            print('Max samples reached', sample_count, '/', total_samples)\n",
    "            break\n",
    "\n",
    "        if samples < MIN_SAMPLES: # or data_train['num_samples'][i] > 10000:\n",
    "            print('SKIP', author_id, samples)\n",
    "            continue\n",
    "        else:\n",
    "            udata_train = data_train['user_data'][author_id]\n",
    "            max_samples = samples if (sample_count + samples) <= total_samples else (sample_count + samples - total_samples) \n",
    "            \n",
    "            sample_count += max_samples\n",
    "            # print('sample_count', sample_count)\n",
    "\n",
    "            x_train = data_train['user_data'][author_id]['x'][:max_samples]\n",
    "            y_train = data_train['user_data'][author_id]['y'][:max_samples]\n",
    "\n",
    "            train_ds = ShakespeareFedDataset(x_train, y_train, corpus, seq_length)\n",
    "\n",
    "            x_val, y_val = None, None\n",
    "            val_ds = None\n",
    "            author_data = data_test['user_data'][author_id]\n",
    "            test_size = int(len(author_data['x']) * SAMPLES_FRACTION)\n",
    "            if val_split:\n",
    "                x_test += author_data['x'][:int(test_size / 2)]\n",
    "                y_test += author_data['y'][:int(test_size / 2)]\n",
    "                x_val = author_data['x'][int(test_size / 2):]\n",
    "                y_val = author_data['y'][int(test_size / 2):int(test_size)]\n",
    "\n",
    "                val_ds = ShakespeareFedDataset(x_val, y_val, corpus, seq_length)\n",
    "\n",
    "            else:\n",
    "                x_test += author_data['x'][:int(test_size)]\n",
    "                y_test += author_data['y'][:int(test_size)]\n",
    "\n",
    "            data_dict[author_id] = {\n",
    "                'train_ds': train_ds,\n",
    "                'val_ds': val_ds\n",
    "            }\n",
    "\n",
    "    test_ds = ShakespeareFedDataset(x_test, y_test, corpus, seq_length)\n",
    "\n",
    "    return data_dict, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWVOxcAao2_t"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQQQ2mLeo6EA"
   },
   "source": [
    "### Shakespeare LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2mGXTrXRot7R"
   },
   "outputs": [],
   "source": [
    "class ShakespeareLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, classes, lstm_layers=2, dropout=0.1, batch_first=True):\n",
    "        super(ShakespeareLSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.classes = classes\n",
    "        self.no_layers = lstm_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=self.classes,\n",
    "                                      embedding_dim=self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, \n",
    "                            hidden_size=self.hidden_dim,\n",
    "                            num_layers=self.no_layers,\n",
    "                            batch_first=batch_first, \n",
    "                            dropout=dropout if self.no_layers > 1 else 0.)\n",
    "        self.fc = nn.Linear(hidden_dim, self.classes)\n",
    "\n",
    "    def forward(self, x, hc=None):\n",
    "        batch_size = x.size(0)\n",
    "        x_emb = self.embedding(x)\n",
    "        self.lstm.flatten_parameters()\n",
    "        out, (ht, ct) = self.lstm(x_emb.view(batch_size, -1, self.embedding_dim), hc)\n",
    "        dense = self.fc(ht[-1])\n",
    "        return dense\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(self.no_layers, batch_size, self.hidden_dim)),\n",
    "                Variable(torch.zeros(self.no_layers, batch_size, self.hidden_dim)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QsuJlVipMc8"
   },
   "source": [
    "#### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "n_Vb0BYpot5I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shakespeare LSTM SUMMARY\n",
      "==========================================================\n",
      "            Kernel Shape   Output Shape  Params  Mult-Adds\n",
      "Layer                                                     \n",
      "0_embedding      [8, 90]    [10, 80, 8]     720        720\n",
      "1_lstm                 -  [10, 80, 256]  798720     794624\n",
      "2_fc           [256, 90]       [10, 90]   23130      23040\n",
      "----------------------------------------------------------\n",
      "                      Totals\n",
      "Total params          822570\n",
      "Trainable params      822570\n",
      "Non-trainable params       0\n",
      "Mult-Adds             818384\n",
      "==========================================================\n",
      "            Kernel Shape   Output Shape  Params  Mult-Adds\n",
      "Layer                                                     \n",
      "0_embedding      [8, 90]    [10, 80, 8]     720        720\n",
      "1_lstm                 -  [10, 80, 256]  798720     794624\n",
      "2_fc           [256, 90]       [10, 90]   23130      23040\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "seq_length = 80 # mcmahan17a, fedprox, qFFL\n",
    "\n",
    "shakespeare_lstm = ShakespeareLSTM(input_dim=seq_length,  \n",
    "                                   embedding_dim=8,  # mcmahan17a, fedprox, qFFL\n",
    "                                   hidden_dim=256,  # mcmahan17a, fedprox impl\n",
    "                                   # hidden_dim=100,  # fedprox paper\n",
    "                                   classes=len(corpus),\n",
    "                                   lstm_layers=2,\n",
    "                                   dropout=0.1,\n",
    "                                   batch_first=True\n",
    "                                   )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  shakespeare_lstm.cuda()\n",
    "\n",
    "hc = shakespeare_lstm.init_hidden(batch_size)\n",
    "\n",
    "x_sample = torch.zeros((batch_size, seq_length),\n",
    "                       dtype=torch.long,\n",
    "                       device=(torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')))\n",
    "\n",
    "x_sample[0][0] = 1\n",
    "x_sample\n",
    "\n",
    "print(\"\\nShakespeare LSTM SUMMARY\")\n",
    "print(summaryx(shakespeare_lstm, x_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn7egnzTpeks"
   },
   "source": [
    "## FedAvg Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFFAfTOwpk4j"
   },
   "source": [
    "### Plot Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oyYjWa6IpnTY"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "367THsiTpo-C"
   },
   "outputs": [],
   "source": [
    "def plot_scores(history, exp_id, title, suffix):\n",
    "    accuracies = [x['accuracy'] for x in history]\n",
    "    f1_macro = [x['f1_macro'] for x in history]\n",
    "    f1_weighted = [x['f1_weighted'] for x in history]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(accuracies, 'tab:orange')\n",
    "    ax.set(xlabel='Rounds', ylabel='Test Accuracy', title=title)\n",
    "    ax.grid()\n",
    "    fig.savefig(f'{BASE_DIR}/{exp_id}/Test_Accuracy_{suffix}.jpg', format='jpg', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(f1_macro, 'tab:orange')\n",
    "    ax.set(xlabel='Rounds', ylabel='Test F1 (macro)', title=title)\n",
    "    ax.grid()\n",
    "    fig.savefig(f'{BASE_DIR}/{exp_id}/Test_F1_Macro_{suffix}.jpg', format='jpg')\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(f1_weighted, 'tab:orange')\n",
    "    ax.set(xlabel='Rounds', ylabel='Test F1 (weighted)', title=title)\n",
    "    ax.grid()\n",
    "    fig.savefig(f'{BASE_DIR}/{exp_id}/Test_F1_Weighted_{suffix}.jpg', format='jpg')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_losses(history, exp_id, title, suffix):\n",
    "    val_losses = [x['loss'] for x in history]\n",
    "    train_losses = [x['train_loss'] for x in history]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(train_losses, 'tab:orange')\n",
    "    ax.set(xlabel='Rounds', ylabel='Train Loss', title=title)\n",
    "    ax.grid()\n",
    "    fig.savefig(f'{BASE_DIR}/{exp_id}/Train_Loss_{suffix}.jpg', format='jpg')\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(val_losses, 'tab:orange')\n",
    "    ax.set(xlabel='Rounds', ylabel='Test Loss', title=title)\n",
    "    ax.grid()\n",
    "    fig.savefig(f'{BASE_DIR}/{exp_id}/Test_Loss_{suffix}.jpg', format='jpg')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ9PZM0Gp9ve"
   },
   "source": [
    "### Local Training (Client Update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "EDJFltwdotzZ"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, dataset, idxs):\n",
    "      self.dataset = dataset\n",
    "      self.idxs = list(idxs)\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.idxs)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "      data, label = self.dataset[self.idxs[item]]\n",
    "      return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HtRzU5Yepddq"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ClientUpdate(object):\n",
    "  def __init__(self, dataset, batch_size, learning_rate, epochs, idxs, q=None):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    if hasattr(dataset, 'dataloader'):\n",
    "        self.train_loader = dataset.dataloader(batch_size=batch_size, shuffle=True)\n",
    "    else:\n",
    "        self.train_loader = DataLoader(CustomDataset(dataset, idxs), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    self.learning_rate = learning_rate\n",
    "    self.epochs = epochs\n",
    "    self.q = q\n",
    "    if not self.q:\n",
    "        # TODO: Client itself adjust fairness \n",
    "        pass\n",
    "    self.mu = 1e-10\n",
    "\n",
    "  def train(self, model):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.5)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    e_loss = []\n",
    "    model_weights = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(1, self.epochs+1):\n",
    "\n",
    "      train_loss = 0.0\n",
    "\n",
    "      model.train()\n",
    "      # for data, labels in tqdm(self.train_loader):\n",
    "      for data, labels in self.train_loader:\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "          data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # make a forward pass\n",
    "        # print('input', data.size())\n",
    "        output = model(data)\n",
    "        # print('output', output.size())\n",
    "        # print('labels', labels.size())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, labels)\n",
    "        # do a backwards pass\n",
    "        loss.backward()\n",
    "        # perform a single optimization step\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "      # average losses\n",
    "      train_loss = train_loss/len(self.train_loader.dataset)\n",
    "      e_loss.append(train_loss)\n",
    "\n",
    "\n",
    "    total_loss = sum(e_loss)/len(e_loss)\n",
    "\n",
    "    # delta weights\n",
    "    model_weights_new = copy.deepcopy(model.state_dict())\n",
    "    L = 1.0 / self.learning_rate\n",
    "\n",
    "    delta_weights, delta, h = {}, {}, {}\n",
    "    loss_q = np.float_power(total_loss + self.mu, self.q)\n",
    "    # updating the global weights\n",
    "    for k in model_weights_new.keys():\n",
    "      delta_weights[k] = (model_weights[k] - model_weights_new[k]) * L\n",
    "      delta[k] =  loss_q * delta_weights[k]\n",
    "      # Estimation of the local Lipchitz constant\n",
    "      h[k] = (self.q * np.float_power(total_loss + self.mu, self.q - 1) * torch.pow(torch.norm(delta_weights[k]), 2)) + (L * loss_q)\n",
    "\n",
    "    return delta, h, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3crFDN0xqGu6"
   },
   "source": [
    "### Server Side Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IZghjoL5_Hbv"
   },
   "outputs": [],
   "source": [
    "def client_sampling(n, m, weights=None, with_replace=False):\n",
    "    pk = None\n",
    "    if weights:\n",
    "        total_weights = np.sum(np.asarray(weights))\n",
    "        pk = [w * 1.0 / total_weights for w in weights]\n",
    "\n",
    "    return np.random.choice(range(n), m, replace=with_replace, p=pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "c085xSOoqEHk"
   },
   "outputs": [],
   "source": [
    "def training(model, rounds, batch_size, lr, ds, data_dict, test_ds, C, K, E, q, sampling, plt_title, plt_color,\n",
    "             classes, eval_every=1, tb_logger=None):\n",
    "  \"\"\"\n",
    "  Function implements the Federated Averaging Algorithm from the FedAvg paper.\n",
    "  Specifically, this function is used for the server side training and weight update\n",
    "\n",
    "  Params:\n",
    "    - model:           PyTorch model to train\n",
    "    - rounds:          Number of communication rounds for the client update\n",
    "    - batch_size:      Batch size for client update training\n",
    "    - lr:              Learning rate used for client update training\n",
    "    - ds:              Dataset used for training\n",
    "    - data_dict:       Type of data partition used for training (IID or non-IID)\n",
    "    - test_data_dict:  Data used for testing the model\n",
    "    - C:               Fraction of clients randomly chosen to perform computation on each round\n",
    "    - K:               Total number of clients\n",
    "    - E:               Number of training passes each client makes over its local dataset per round\n",
    "    - mu:              proximal term constant\n",
    "    - percentage:      percentage of selected client to have fewer than E epochs\n",
    "  Returns:\n",
    "    - model:           Trained model on the server\n",
    "  \"\"\"\n",
    "\n",
    "  # global model weights\n",
    "  global_weights = model.state_dict()\n",
    "\n",
    "  # training loss\n",
    "  train_loss = []\n",
    "\n",
    "  # test accuracy\n",
    "  test_accuracy = []\n",
    "\n",
    "  # test loss\n",
    "  test_loss = []\n",
    "\n",
    "  # history\n",
    "  history=[]\n",
    "\n",
    "  # store last loss for convergence\n",
    "  last_loss = 0.0\n",
    "\n",
    "  # total time taken \n",
    "  total_time = 0\n",
    "  start = time.time()\n",
    "\n",
    "  # client weights by total samples\n",
    "  p_k = None\n",
    "  if sampling == 'weighted':\n",
    "    p_k = [len(data_dict[c]) for c in data_dict] if ds else [len(data_dict[c]['train_ds']) for c in data_dict]\n",
    "\n",
    "  # Time log\n",
    "  start_time = time.time()\n",
    "\n",
    "  users_id = list(data_dict.keys())\n",
    "\n",
    "  for curr_round in range(1, rounds+1):\n",
    "    deltas, hs, local_loss = [], [], []\n",
    "\n",
    "    m = max(int(C*K), 1)    \n",
    "    S_t = client_sampling(K, m, weights=p_k, with_replace=False)\n",
    "\n",
    "    print('Round: {} Picking {}/{} clients: {}'.format(curr_round, m, K, S_t))\n",
    "\n",
    "    global_weights = model.state_dict()\n",
    "\n",
    "    for k in tqdm(S_t):\n",
    "      key = users_id[k]\n",
    "      ds_ = ds if ds else data_dict[key]['train_ds']\n",
    "      idxs = data_dict[key] if ds else None\n",
    "      # print(f'Client {k}: {len(idxs) if idxs else len(ds_)} samples')\n",
    "      local_update = ClientUpdate(dataset=ds_, batch_size=batch_size, learning_rate=lr, epochs=E, idxs=idxs, q=q)\n",
    "      delta_k, h_k, loss = local_update.train(model=copy.deepcopy(model))\n",
    "\n",
    "      deltas.append(copy.deepcopy(delta_k))\n",
    "      hs.append(copy.deepcopy(h_k))\n",
    "      local_loss.append(copy.deepcopy(loss))\n",
    "\n",
    "      if tb_logger:\n",
    "        tb_logger.add_scalar(f'Round/S{k}', loss, curr_round)\n",
    "\n",
    "    # Perform qFedAvg\n",
    "    h_sum = copy.deepcopy(hs[0])\n",
    "    delta_sum = copy.deepcopy(deltas[0])\n",
    "    \n",
    "    for k in h_sum.keys():\n",
    "        for i in range(1, len(hs)):\n",
    "            h_sum[k] += hs[i][k]\n",
    "            delta_sum[k] += deltas[i][k]\n",
    "\n",
    "    new_weights = {}\n",
    "    for k in delta_sum.keys():\n",
    "        for i in range(len(deltas)):\n",
    "            new_weights[k] = delta_sum[k] / h_sum[k]\n",
    "\n",
    "    # Updating global model weights\n",
    "    for k in global_weights.keys():\n",
    "        global_weights[k] -= new_weights[k]\n",
    "\n",
    "    # move the updated weights to our model state dict\n",
    "    model.load_state_dict(global_weights)\n",
    "\n",
    "    # loss\n",
    "    loss_avg = sum(local_loss) / len(local_loss)\n",
    "    print('Round: {}... \\tAverage Loss: {}'.format(curr_round, round(loss_avg, 3)))\n",
    "    train_loss.append(loss_avg)\n",
    "    if tb_logger:\n",
    "        tb_logger.add_scalar(f'Train/Loss', loss_avg, curr_round)\n",
    "\n",
    "    # testing\n",
    "    # if curr_round % eval_every == 0:\n",
    "    test_scores = testing(model, test_ds, batch_size * 2, nn.CrossEntropyLoss(), len(classes), classes)\n",
    "    test_scores['train_loss'] = loss_avg\n",
    "    test_loss_current, test_accuracy_current = test_scores['loss'], test_scores['accuracy']\n",
    "\n",
    "    history.append(test_scores)\n",
    "    test_accuracy.append(test_accuracy_current)\n",
    "    test_loss.append(test_loss_current)\n",
    "    \n",
    "    # print('Round: {}... \\tAverage Loss: {} \\tTest Loss: {} \\tTest Acc: {}'.format(curr_round, round(loss_avg, 3), round(test_loss, 3), round(test_accuracy, 3)))\n",
    "\n",
    "    if tb_logger:\n",
    "        tb_logger.add_scalar(f'Test/Loss', test_scores['loss'], curr_round)\n",
    "        tb_logger.add_scalars(f'Test/Scores', {\n",
    "            'accuracy': test_scores['accuracy'], 'f1_macro': test_scores['f1_macro'], 'f1_weighted': test_scores['f1_weighted']\n",
    "        }, curr_round)\n",
    "    \n",
    "    # update the last loss\n",
    "    last_loss = loss_avg\n",
    "\n",
    "  end = time.time()\n",
    "  \n",
    "  # plot train loss\n",
    "  fig, ax = plt.subplots()\n",
    "  x_axis = np.arange(1, rounds+1)\n",
    "  y_axis = np.array(train_loss)\n",
    "  ax.plot(x_axis, y_axis)\n",
    "\n",
    "  ax.set(xlabel='Number of Rounds', ylabel='Train Loss', title=plt_title)\n",
    "  ax.grid()\n",
    "  fig.savefig(plt_title+'_Train_loss.jpg', format='jpg')\n",
    "  plt.show()\n",
    "  \n",
    "  fig, ax = plt.subplots()\n",
    "  x_axis = np.arange(1, rounds+1)\n",
    "  y_axis = np.array(test_loss)\n",
    "  ax.plot(x_axis, y_axis, 'tab:'+plt_color)\n",
    "\n",
    "  ax.set(xlabel='Number of Rounds', ylabel='Test Loss',\n",
    "       title=plt_title)\n",
    "  ax.grid()\n",
    "  fig.savefig(plt_title+'_Test_loss.jpg', format='jpg')\n",
    "  plt.show()\n",
    "\n",
    "  # plot test accuracy\n",
    "  fig, ax = plt.subplots()\n",
    "  x_axis = np.arange(1, rounds+1)\n",
    "  y_axis = np.array(test_accuracy)\n",
    "  ax.plot(x_axis, y_axis)\n",
    "\n",
    "  ax.set(xlabel='Number of Rounds', ylabel='Test Accuracy', title=plt_title)\n",
    "  ax.grid()\n",
    "  fig.savefig(plt_title+'_Test_Accuracy.jpg', format='jpg')\n",
    "  plt.show()\n",
    "\n",
    "  print(\"Training Done! Total time taken to Train: {}\".format(end-start))\n",
    "\n",
    "  return model, train_loss, test_accuracy, test_loss, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXtGLkoAqLIW"
   },
   "source": [
    "### Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "dQJIJno4qKvc"
   },
   "outputs": [],
   "source": [
    "def testing(model, dataset, bs, criterion, num_classes, classes, print_all=False):\n",
    "  #test loss \n",
    "  test_loss = 0.0\n",
    "  y_true, y_hat = None, None\n",
    "\n",
    "  correct_class = list(0 for i in range(num_classes))\n",
    "  total_class = list(0 for i in range(num_classes))\n",
    "\n",
    "  if hasattr(dataset, 'dataloader'):\n",
    "    test_loader = dataset.dataloader(batch_size=bs, shuffle=False)\n",
    "  else:\n",
    "    test_loader = DataLoader(dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "  l = len(test_loader)\n",
    "\n",
    "  model.eval()\n",
    "  for i, (data, labels) in enumerate(tqdm(test_loader)):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "      data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "    output = model(data)\n",
    "    loss = criterion(output, labels)\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "\n",
    "    _, pred = torch.max(output, dim=1)\n",
    "\n",
    "    # For F1Score\n",
    "    y_true = np.append(y_true, labels.data.view_as(pred).cpu().numpy()) if i != 0 else labels.data.view_as(pred).cpu().numpy()\n",
    "    y_hat = np.append(y_hat, pred.cpu().numpy()) if i != 0 else pred.cpu().numpy()\n",
    "\n",
    "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
    "\n",
    "    #test accuracy for each object class\n",
    "    # for i in range(num_classes):\n",
    "    #   label = labels.data[i]\n",
    "    #   correct_class[label] += correct[i].item()\n",
    "    #   total_class[label] += 1\n",
    "\n",
    "    for i, lbl in enumerate(labels.data):\n",
    "      try:\n",
    "        # print(type(lbl))\n",
    "        # correct_class[lbl.data[0]] += correct.data[i]\n",
    "        correct_class[lbl.item()] += correct[i]\n",
    "        total_class[lbl.item()] += 1\n",
    "      except:\n",
    "          print('Error', lbl, i)\n",
    "    \n",
    "  # avg test loss\n",
    "  test_loss = test_loss/len(test_loader.dataset)\n",
    "  print(\"Test Loss: {:.6f}\\n\".format(test_loss))\n",
    "\n",
    "  # Avg F1 Score\n",
    "  f1_macro = f1_score(y_true, y_hat, average='macro')\n",
    "  # F1-Score -> weigthed to consider class imbalance\n",
    "  f1_weighted =  f1_score(y_true, y_hat, average='weighted')\n",
    "  print(\"F1 Score: {:.6f} (macro) {:.6f} (weighted) %\\n\".format(f1_macro, f1_weighted))\n",
    "\n",
    "  # print test accuracy\n",
    "  if print_all:\n",
    "    for i in range(num_classes):\n",
    "        if total_class[i] > 0:\n",
    "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % \n",
    "                    (classes[i], 100 * correct_class[i] / total_class[i],\n",
    "                    np.sum(correct_class[i]), np.sum(total_class[i])))\n",
    "        else:\n",
    "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "  overall_accuracy = np.sum(correct_class) / np.sum(total_class)\n",
    "\n",
    "  print('\\nFinal Test  Accuracy: {:.3f} ({}/{})'.format(overall_accuracy, np.sum(correct_class), np.sum(total_class)))\n",
    "\n",
    "  return {'loss': test_loss, 'accuracy': overall_accuracy, 'f1_macro': f1_macro, 'f1_weighted': f1_weighted}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxqXLBd8qbC2"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "c8gl5P3SMq4a"
   },
   "outputs": [],
   "source": [
    "log_dict = {}\n",
    "NUM_REPEAT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "E2CfSkNVqKtL"
   },
   "outputs": [],
   "source": [
    "seq_length = 80  # mcmahan17a, fedprox, qFFL\n",
    "embedding_dim = 8  # mcmahan17a, fedprox, qFFL\n",
    "# hidden_dim = 100  # fedprox paper\n",
    "hidden_dim = 256  # mcmahan17a, fedprox impl\n",
    "num_classes = len(corpus)\n",
    "classes = list(range(num_classes))\n",
    "lstm_layers = 2  # mcmahan17a, fedprox, qFFL\n",
    "dropout = 0.1  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPIpStyNJ-63"
   },
   "source": [
    "## LSTM qFedAvg on IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "gpS1gyJ_H_MA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective 413629 / 413629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Total users:', 143)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, data_dict, test_ds = iid_partition(corpus, seq_length, val_split=True)\n",
    "\n",
    "total_clients = len(data_dict.keys())\n",
    "'Total users:', total_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYoGsy05H_RC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Run Number:  0\n",
      "Objective 413629 / 413629\n",
      "Round: 1 Picking 10/143 clients: [125  23 129  49  10  65  87  75  55 102]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55caa0f280e443c94d930f4c9f379bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
    "test_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "\n",
    "for exp_num in range(NUM_REPEAT):\n",
    "  print(\"Experiment Run Number: \", exp_num)\n",
    "\n",
    "  # partition data\n",
    "  train_ds, data_dict, test_ds = iid_partition(corpus, seq_length, val_split=True)  # Not using val_ds but makes train eval periods faster\n",
    "  total_clients = len(data_dict.keys())\n",
    "  # number of training rounds\n",
    "  rounds = 50\n",
    "  # client fraction\n",
    "  C = 0.07  # 10 clients\n",
    "  # number of clients\n",
    "  K = total_clients\n",
    "  # number of training passes on local dataset for each roung\n",
    "  E = 1\n",
    "  # batch size\n",
    "  batch_size = 10\n",
    "  # learning Rate\n",
    "  lr = 0.8\n",
    "  # fairness\n",
    "  q = 0.001  # qFFL\n",
    "  # sampling\n",
    "  # self.sampling = 'uniform'\n",
    "  sampling = 'weighted'\n",
    "\n",
    "  shakespeare_lstm = ShakespeareLSTM(input_dim=seq_length,  \n",
    "                                    embedding_dim=embedding_dim,  \n",
    "                                    hidden_dim=hidden_dim,\n",
    "                                    classes=num_classes,\n",
    "                                    lstm_layers=lstm_layers,\n",
    "                                    dropout=dropout,\n",
    "                                    batch_first=True\n",
    "                                    )\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "      shakespeare_lstm.cuda()\n",
    "\n",
    "  lstm_iid_trained, train_loss, test_accuracy, test_loss, history = training(shakespeare_lstm,\n",
    "                                          rounds, batch_size, lr,\n",
    "                                          train_ds,\n",
    "                                          data_dict,\n",
    "                                          test_ds,\n",
    "                                          C, K, E, q, sampling,\n",
    "                                          'Shakespeare LSTM on IID', \"green\",\n",
    "                                          corpus, # classes\n",
    "                                          )\n",
    "\n",
    "  train_loss_multiple_runs[exp_num] = train_loss\n",
    "  test_accuracy_multiple_runs[exp_num] = test_accuracy\n",
    "  test_loss_multiple_runs[exp_num] = test_loss\n",
    "\n",
    "  del lstm_iid_trained\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-irvQHuNNTtB"
   },
   "outputs": [],
   "source": [
    "hyperparams = {'rounds': rounds,   \n",
    "               'C': C,\n",
    "               'K': K,\n",
    "               'E': E,\n",
    "               'batch_size': batch_size,\n",
    "               'lr': lr,\n",
    "               'q': q,\n",
    "               'sampling': sampling,\n",
    "               }\n",
    "\n",
    "log_dict['Shakespeare LSTM on IID'] = {'train_loss': train_loss_multiple_runs, \n",
    "                                'test_loss': test_loss_multiple_runs, \n",
    "                                'test_accuracy': test_accuracy_multiple_runs,\n",
    "                                'history': history,\n",
    "                                'hyperparams': hyperparams,\n",
    "                                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QO0GkVyKEgu"
   },
   "source": [
    "## LSTM qFedAvg on Non IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wsHRfwmKGdS"
   },
   "outputs": [],
   "source": [
    "data_dict, test_ds = noniid_partition(corpus, seq_length=seq_length, val_split=False)\n",
    " \n",
    "total_clients = len(data_dict.keys())  \n",
    "'Total users:', total_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNJZG5tvKGgd"
   },
   "outputs": [],
   "source": [
    "train_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
    "test_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "\n",
    "for exp_num in range(NUM_REPEAT):\n",
    "  print(\"Experiment Run Number: \", exp_num)\n",
    "  # partition dataset\n",
    "  data_dict, test_ds = noniid_partition(corpus, seq_length=seq_length, val_split=False)\n",
    "  total_clients = len(data_dict.keys())  \n",
    "  # number of training rounds\n",
    "  rounds = 50\n",
    "  # client fraction\n",
    "  C = 0.07  # 10 clients\n",
    "  # number of clients\n",
    "  K = total_clients\n",
    "  # number of training passes on local dataset for each roung\n",
    "  E = 1\n",
    "  # batch size\n",
    "  batch_size = 10\n",
    "  # learning Rate\n",
    "  lr = 0.8\n",
    "  # fairness\n",
    "  q = 0.001  # qFFL\n",
    "  # sampling\n",
    "  # self.sampling = 'uniform'\n",
    "  sampling = 'weighted'\n",
    "\n",
    "  shakespeare_lstm = ShakespeareLSTM(input_dim=seq_length,\n",
    "                                        embedding_dim=embedding_dim,\n",
    "                                        hidden_dim=hidden_dim,\n",
    "                                        classes=num_classes,\n",
    "                                        lstm_layers=lstm_layers,\n",
    "                                        dropout=dropout,\n",
    "                                        batch_first=True\n",
    "                                        )\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "      shakespeare_lstm.cuda()\n",
    "\n",
    "  lstm_non_iid_trained, train_loss, test_accuracy, test_loss, history = training(shakespeare_lstm,\n",
    "                                                rounds, batch_size, lr,\n",
    "                                                None, #  ds empty as it is included in data_dict\n",
    "                                                data_dict,\n",
    "                                                test_ds,\n",
    "                                                C, K, E, q, sampling,\n",
    "                                                'Shakespeare LSTM on Non IID', \"green\",\n",
    "                                                corpus, # classes,\n",
    "                                                )\n",
    "\n",
    "  train_loss_multiple_runs[exp_num] = train_loss\n",
    "  test_accuracy_multiple_runs[exp_num] = test_accuracy\n",
    "  test_loss_multiple_runs[exp_num] = test_loss\n",
    "\n",
    "  del lstm_non_iid_trained\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uANkBpTRKGkv"
   },
   "outputs": [],
   "source": [
    "hyperparams = {'rounds': rounds,\n",
    "               'C': C,\n",
    "               'K': K, \n",
    "               'E': E,\n",
    "               'batch_size': batch_size,\n",
    "               'lr': lr,\n",
    "               'q': q,\n",
    "               'sampling': sampling,\n",
    "               }\n",
    "\n",
    "log_dict['Shakespeare LSTM on Non IID'] = {'train_loss': train_loss_multiple_runs, \n",
    "                                'test_loss': test_loss_multiple_runs, \n",
    "                                'test_accuracy': test_accuracy_multiple_runs,\n",
    "                                'history': history,\n",
    "                                'hyperparams': hyperparams,\n",
    "                                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShdScPNuQzUQ"
   },
   "source": [
    "## Pickle Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwHt7jviQ1AV"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(path + 'Local_Round_qFedAvg_1.pkl', 'wb') as file:\n",
    "  pickle.dump(log_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "qNkwXxO8Q3Ei"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Shakespeare LSTM on IID': {'history': [{'accuracy': 0.23448862757233482,\n",
       "    'f1_macro': 0.017150159261345,\n",
       "    'f1_weighted': 0.12950082122222362,\n",
       "    'loss': 2.912823109061988,\n",
       "    'train_loss': 3.2284536208107264},\n",
       "   {'accuracy': 0.2865542317809067,\n",
       "    'f1_macro': 0.03364640761775492,\n",
       "    'f1_weighted': 0.19364068653912545,\n",
       "    'loss': 2.6091076837371014,\n",
       "    'train_loss': 2.824486470090568},\n",
       "   {'accuracy': 0.30926040538449634,\n",
       "    'f1_macro': 0.03998380924752695,\n",
       "    'f1_weighted': 0.21432383249162226,\n",
       "    'loss': 2.4971092173176324,\n",
       "    'train_loss': 2.6664741330786526},\n",
       "   {'accuracy': 0.3279049976790964,\n",
       "    'f1_macro': 0.05923861355128075,\n",
       "    'f1_weighted': 0.25527267982483937,\n",
       "    'loss': 2.419030918465224,\n",
       "    'train_loss': 2.562092457524144},\n",
       "   {'accuracy': 0.3350997988550209,\n",
       "    'f1_macro': 0.0634669702495159,\n",
       "    'f1_weighted': 0.2639684161336742,\n",
       "    'loss': 2.3398654666244663,\n",
       "    'train_loss': 2.5096276162488493},\n",
       "   {'accuracy': 0.35883103821754603,\n",
       "    'f1_macro': 0.07657915703325893,\n",
       "    'f1_weighted': 0.2930443951994218,\n",
       "    'loss': 2.2843812693734113,\n",
       "    'train_loss': 2.451088166475956},\n",
       "   {'accuracy': 0.3642658208262417,\n",
       "    'f1_macro': 0.08023138724359954,\n",
       "    'f1_weighted': 0.29870918030167687,\n",
       "    'loss': 2.237351987142598,\n",
       "    'train_loss': 2.393052770079913},\n",
       "   {'accuracy': 0.3738202073340554,\n",
       "    'f1_macro': 0.08503115718136736,\n",
       "    'f1_weighted': 0.30871219493681673,\n",
       "    'loss': 2.2056334648784537,\n",
       "    'train_loss': 2.3581236547495488},\n",
       "   {'accuracy': 0.39031796379390377,\n",
       "    'f1_macro': 0.0963565032655903,\n",
       "    'f1_weighted': 0.3407478069522619,\n",
       "    'loss': 2.1547107573252204,\n",
       "    'train_loss': 2.304422500898564},\n",
       "   {'accuracy': 0.39087884883181184,\n",
       "    'f1_macro': 0.09830955130795929,\n",
       "    'f1_weighted': 0.3381160038933404,\n",
       "    'loss': 2.1233465182496247,\n",
       "    'train_loss': 2.277852781280813},\n",
       "   {'accuracy': 0.39764815101346124,\n",
       "    'f1_macro': 0.09834078015727889,\n",
       "    'f1_weighted': 0.3454400535316689,\n",
       "    'loss': 2.0963269060074996,\n",
       "    'train_loss': 2.2438074072779135},\n",
       "   {'accuracy': 0.40406931765434007,\n",
       "    'f1_macro': 0.10397860348654381,\n",
       "    'f1_weighted': 0.35364304525741286,\n",
       "    'loss': 2.0661403852941893,\n",
       "    'train_loss': 2.2135541278997724},\n",
       "   {'accuracy': 0.4124052297694569,\n",
       "    'f1_macro': 0.10943740269630357,\n",
       "    'f1_weighted': 0.36104106680032144,\n",
       "    'loss': 2.043885225987224,\n",
       "    'train_loss': 2.1934464683341455},\n",
       "   {'accuracy': 0.41859430604982206,\n",
       "    'f1_macro': 0.11283139642758368,\n",
       "    'f1_weighted': 0.3697143156888086,\n",
       "    'loss': 2.0172161090287304,\n",
       "    'train_loss': 2.164673021821148},\n",
       "   {'accuracy': 0.4215727990097478,\n",
       "    'f1_macro': 0.11527802777407037,\n",
       "    'f1_weighted': 0.3726881719382646,\n",
       "    'loss': 1.99693998865842,\n",
       "    'train_loss': 2.150038237251434},\n",
       "   {'accuracy': 0.42476404146681107,\n",
       "    'f1_macro': 0.11935905636980751,\n",
       "    'f1_weighted': 0.3717534381386689,\n",
       "    'loss': 1.979129001736853,\n",
       "    'train_loss': 2.11367012893222},\n",
       "   {'accuracy': 0.42841946464490177,\n",
       "    'f1_macro': 0.12678510039438648,\n",
       "    'f1_weighted': 0.3790566355288886,\n",
       "    'loss': 1.9629957049712712,\n",
       "    'train_loss': 2.1146289622841374},\n",
       "   {'accuracy': 0.4376257156119449,\n",
       "    'f1_macro': 0.13798127046258102,\n",
       "    'f1_weighted': 0.3914053443691212,\n",
       "    'loss': 1.944139197201307,\n",
       "    'train_loss': 2.086181281710026},\n",
       "   {'accuracy': 0.4395984836763113,\n",
       "    'f1_macro': 0.14020799132867992,\n",
       "    'f1_weighted': 0.39705613876025014,\n",
       "    'loss': 1.9310889892359646,\n",
       "    'train_loss': 2.080200016059569},\n",
       "   {'accuracy': 0.44348599721491566,\n",
       "    'f1_macro': 0.13278907173757185,\n",
       "    'f1_weighted': 0.4009275740417422,\n",
       "    'loss': 1.9134888538974608,\n",
       "    'train_loss': 2.059425624763982},\n",
       "   {'accuracy': 0.446541853628346,\n",
       "    'f1_macro': 0.140781557029177,\n",
       "    'f1_weighted': 0.40882009197459385,\n",
       "    'loss': 1.9005861927850427,\n",
       "    'train_loss': 2.0463765880296507},\n",
       "   {'accuracy': 0.4491335293207489,\n",
       "    'f1_macro': 0.14740965125742844,\n",
       "    'f1_weighted': 0.40749807403376803,\n",
       "    'loss': 1.8872371864887412,\n",
       "    'train_loss': 2.0322250107692277},\n",
       "   {'accuracy': 0.4526148847284543,\n",
       "    'f1_macro': 0.14631877454791276,\n",
       "    'f1_weighted': 0.40858902532323116,\n",
       "    'loss': 1.874209257616553,\n",
       "    'train_loss': 2.0075806943583787},\n",
       "   {'accuracy': 0.45833978028779204,\n",
       "    'f1_macro': 0.1568562701532558,\n",
       "    'f1_weighted': 0.42033996182728117,\n",
       "    'loss': 1.8578492624651448,\n",
       "    'train_loss': 2.0000856725280047},\n",
       "   {'accuracy': 0.4585331889215535,\n",
       "    'f1_macro': 0.15878640495754026,\n",
       "    'f1_weighted': 0.4228592058353141,\n",
       "    'loss': 1.8509967520593231,\n",
       "    'train_loss': 1.9846287604421378},\n",
       "   {'accuracy': 0.4612409097942132,\n",
       "    'f1_macro': 0.1621908633699634,\n",
       "    'f1_weighted': 0.4261350018477551,\n",
       "    'loss': 1.8373788936118693,\n",
       "    'train_loss': 1.9712682856914086},\n",
       "   {'accuracy': 0.46545721801021195,\n",
       "    'f1_macro': 0.16540092701182713,\n",
       "    'f1_weighted': 0.4318428864216413,\n",
       "    'loss': 1.8224823541227977,\n",
       "    'train_loss': 1.9480085273769219},\n",
       "   {'accuracy': 0.4661728299551292,\n",
       "    'f1_macro': 0.15899775645136527,\n",
       "    'f1_weighted': 0.4210055183057648,\n",
       "    'loss': 1.8207945176801565,\n",
       "    'train_loss': 1.9555542242869137},\n",
       "   {'accuracy': 0.47052452421476093,\n",
       "    'f1_macro': 0.1708555542079,\n",
       "    'f1_weighted': 0.4377771421001233,\n",
       "    'loss': 1.803748751933289,\n",
       "    'train_loss': 1.9369110732739934},\n",
       "   {'accuracy': 0.47212981587498065,\n",
       "    'f1_macro': 0.1681591386121756,\n",
       "    'f1_weighted': 0.4338772855697619,\n",
       "    'loss': 1.797151307584483,\n",
       "    'train_loss': 1.9245341588610354},\n",
       "   {'accuracy': 0.47476017329413583,\n",
       "    'f1_macro': 0.17277726630496554,\n",
       "    'f1_weighted': 0.4403043245004236,\n",
       "    'loss': 1.7904989820376613,\n",
       "    'train_loss': 1.896792905520066},\n",
       "   {'accuracy': 0.4748568776110166,\n",
       "    'f1_macro': 0.17028457845949882,\n",
       "    'f1_weighted': 0.43717860084026305,\n",
       "    'loss': 1.7851950289768301,\n",
       "    'train_loss': 1.9065059435915817},\n",
       "   {'accuracy': 0.47833823301872197,\n",
       "    'f1_macro': 0.17133445652860382,\n",
       "    'f1_weighted': 0.44577131727102953,\n",
       "    'loss': 1.7701811190972974,\n",
       "    'train_loss': 1.8956821967390358},\n",
       "   {'accuracy': 0.47978879777193256,\n",
       "    'f1_macro': 0.17651422894603733,\n",
       "    'f1_weighted': 0.4475967886476281,\n",
       "    'loss': 1.7671855645556578,\n",
       "    'train_loss': 1.901250132370127},\n",
       "   {'accuracy': 0.4824965186445923,\n",
       "    'f1_macro': 0.18042053562581045,\n",
       "    'f1_weighted': 0.4509075430069699,\n",
       "    'loss': 1.7591087810421957,\n",
       "    'train_loss': 1.8974176232271502},\n",
       "   {'accuracy': 0.48340553922327095,\n",
       "    'f1_macro': 0.1762969669040012,\n",
       "    'f1_weighted': 0.4487688549426532,\n",
       "    'loss': 1.7509211971381722,\n",
       "    'train_loss': 1.8751834811161003},\n",
       "   {'accuracy': 0.4859778740522977,\n",
       "    'f1_macro': 0.18552677567757866,\n",
       "    'f1_weighted': 0.45617330162812675,\n",
       "    'loss': 1.7379944112861412,\n",
       "    'train_loss': 1.890308898980027},\n",
       "   {'accuracy': 0.48992341018103047,\n",
       "    'f1_macro': 0.18952693964057807,\n",
       "    'f1_weighted': 0.459359706265733,\n",
       "    'loss': 1.7333975711552956,\n",
       "    'train_loss': 1.8585425696112885},\n",
       "   {'accuracy': 0.4885695497447006,\n",
       "    'f1_macro': 0.18402291359367332,\n",
       "    'f1_weighted': 0.4558894814125204,\n",
       "    'loss': 1.7302508568804773,\n",
       "    'train_loss': 1.852947832670779},\n",
       "   {'accuracy': 0.490542317809067,\n",
       "    'f1_macro': 0.18657081734662856,\n",
       "    'f1_weighted': 0.46200049182568753,\n",
       "    'loss': 1.7211268141821923,\n",
       "    'train_loss': 1.8532094644787072},\n",
       "   {'accuracy': 0.49207024601578214,\n",
       "    'f1_macro': 0.18927772361612796,\n",
       "    'f1_weighted': 0.46075004779180073,\n",
       "    'loss': 1.7180225733542533,\n",
       "    'train_loss': 1.8494131649018652},\n",
       "   {'accuracy': 0.4935981742224973,\n",
       "    'f1_macro': 0.1963984124271011,\n",
       "    'f1_weighted': 0.4635763009867305,\n",
       "    'loss': 1.7092358441361974,\n",
       "    'train_loss': 1.8289419079212315},\n",
       "   {'accuracy': 0.4944491722110475,\n",
       "    'f1_macro': 0.1934310842436928,\n",
       "    'f1_weighted': 0.4621338846415928,\n",
       "    'loss': 1.7047828343684932,\n",
       "    'train_loss': 1.818271799465927},\n",
       "   {'accuracy': 0.49644128113879005,\n",
       "    'f1_macro': 0.1979130057055249,\n",
       "    'f1_weighted': 0.4678579832852181,\n",
       "    'loss': 1.6994632555593485,\n",
       "    'train_loss': 1.8217995083740142},\n",
       "   {'accuracy': 0.49856877611016553,\n",
       "    'f1_macro': 0.19303178247714317,\n",
       "    'f1_weighted': 0.46764149137894967,\n",
       "    'loss': 1.6958354614962095,\n",
       "    'train_loss': 1.801736170467747},\n",
       "   {'accuracy': 0.49957450100572487,\n",
       "    'f1_macro': 0.20074776108674672,\n",
       "    'f1_weighted': 0.47294065143072483,\n",
       "    'loss': 1.6892066609697152,\n",
       "    'train_loss': 1.804138150279453},\n",
       "   {'accuracy': 0.49982593222961474,\n",
       "    'f1_macro': 0.20009655017514524,\n",
       "    'f1_weighted': 0.46719696366621827,\n",
       "    'loss': 1.6850210865762107,\n",
       "    'train_loss': 1.8145302125238612},\n",
       "   {'accuracy': 0.5003481355407705,\n",
       "    'f1_macro': 0.20386184665974846,\n",
       "    'f1_weighted': 0.47232861459990544,\n",
       "    'loss': 1.679965175509582,\n",
       "    'train_loss': 1.8163193573183374},\n",
       "   {'accuracy': 0.5036167414513384,\n",
       "    'f1_macro': 0.19807163622334714,\n",
       "    'f1_weighted': 0.47301305652823344,\n",
       "    'loss': 1.669740960207145,\n",
       "    'train_loss': 1.7961309826864575},\n",
       "   {'accuracy': 0.502881788643045,\n",
       "    'f1_macro': 0.20125408346797327,\n",
       "    'f1_weighted': 0.4756987211690307,\n",
       "    'loss': 1.6715501481391928,\n",
       "    'train_loss': 1.7928250978761362}],\n",
       "  'hyperparams': {'C': 0.07,\n",
       "   'E': 1,\n",
       "   'K': 143,\n",
       "   'batch_size': 10,\n",
       "   'lr': 0.8,\n",
       "   'q': 0.001,\n",
       "   'rounds': 50,\n",
       "   'sampling': 'weighted'},\n",
       "  'test_accuracy': [[0.22354169890143896,\n",
       "    0.2753752127494971,\n",
       "    0.3024717623394708,\n",
       "    0.33047733250812317,\n",
       "    0.34929599257310845,\n",
       "    0.36119062354943526,\n",
       "    0.3696425808448089,\n",
       "    0.3804154417453195,\n",
       "    0.3872234256537212,\n",
       "    0.3971452885656816,\n",
       "    0.3968745164784156,\n",
       "    0.4062354943524679,\n",
       "    0.4140492031564289,\n",
       "    0.41491954200835524,\n",
       "    0.4218048893702615,\n",
       "    0.42706560420857187,\n",
       "    0.4308564134302955,\n",
       "    0.43449249574501003,\n",
       "    0.4365232863995049,\n",
       "    0.44044948166486153,\n",
       "    0.4430604982206406,\n",
       "    0.44770230543091444,\n",
       "    0.4521313631440507,\n",
       "    0.455864149775646,\n",
       "    0.45872659755531486,\n",
       "    0.4594808912269844,\n",
       "    0.46261411109391926,\n",
       "    0.4663082159987622,\n",
       "    0.4661341482283769,\n",
       "    0.46812625715611944,\n",
       "    0.46886120996441283,\n",
       "    0.4710080457991645,\n",
       "    0.4731548816339161,\n",
       "    0.47470215070400745,\n",
       "    0.47640414668110787,\n",
       "    0.4798661612254371,\n",
       "    0.4805237505802259,\n",
       "    0.4821290422404456,\n",
       "    0.4838116973541699,\n",
       "    0.4857264428284079,\n",
       "    0.4873123936252514,\n",
       "    0.4897493424106452,\n",
       "    0.49075506730620455,\n",
       "    0.4934627881788643,\n",
       "    0.4938496054463871,\n",
       "    0.49630589509515705,\n",
       "    0.49847207179328484,\n",
       "    0.49965186445922943,\n",
       "    0.501373201299706,\n",
       "    0.5021081541079994],\n",
       "   [0.23448862757233482,\n",
       "    0.2865542317809067,\n",
       "    0.30926040538449634,\n",
       "    0.3279049976790964,\n",
       "    0.3350997988550209,\n",
       "    0.35883103821754603,\n",
       "    0.3642658208262417,\n",
       "    0.3738202073340554,\n",
       "    0.39031796379390377,\n",
       "    0.39087884883181184,\n",
       "    0.39764815101346124,\n",
       "    0.40406931765434007,\n",
       "    0.4124052297694569,\n",
       "    0.41859430604982206,\n",
       "    0.4215727990097478,\n",
       "    0.42476404146681107,\n",
       "    0.42841946464490177,\n",
       "    0.4376257156119449,\n",
       "    0.4395984836763113,\n",
       "    0.44348599721491566,\n",
       "    0.446541853628346,\n",
       "    0.4491335293207489,\n",
       "    0.4526148847284543,\n",
       "    0.45833978028779204,\n",
       "    0.4585331889215535,\n",
       "    0.4612409097942132,\n",
       "    0.46545721801021195,\n",
       "    0.4661728299551292,\n",
       "    0.47052452421476093,\n",
       "    0.47212981587498065,\n",
       "    0.47476017329413583,\n",
       "    0.4748568776110166,\n",
       "    0.47833823301872197,\n",
       "    0.47978879777193256,\n",
       "    0.4824965186445923,\n",
       "    0.48340553922327095,\n",
       "    0.4859778740522977,\n",
       "    0.48992341018103047,\n",
       "    0.4885695497447006,\n",
       "    0.490542317809067,\n",
       "    0.49207024601578214,\n",
       "    0.4935981742224973,\n",
       "    0.4944491722110475,\n",
       "    0.49644128113879005,\n",
       "    0.49856877611016553,\n",
       "    0.49957450100572487,\n",
       "    0.49982593222961474,\n",
       "    0.5003481355407705,\n",
       "    0.5036167414513384,\n",
       "    0.502881788643045]],\n",
       "  'test_loss': [[2.9419204479545407,\n",
       "    2.636301907766498,\n",
       "    2.4971947573977284,\n",
       "    2.4041615034909363,\n",
       "    2.3371581739913045,\n",
       "    2.2803248840599037,\n",
       "    2.2353907369126778,\n",
       "    2.181934679187886,\n",
       "    2.1480624583249637,\n",
       "    2.1142523757353686,\n",
       "    2.09234725933971,\n",
       "    2.0635637814023724,\n",
       "    2.0353781872008323,\n",
       "    2.023066542120992,\n",
       "    2.0076807907320076,\n",
       "    1.9821732308419888,\n",
       "    1.9749744579878437,\n",
       "    1.9453908765076686,\n",
       "    1.9334995658670797,\n",
       "    1.9226735211089172,\n",
       "    1.9064812609753234,\n",
       "    1.895098859655512,\n",
       "    1.882246903243973,\n",
       "    1.8681269508410252,\n",
       "    1.8565446719026928,\n",
       "    1.8488995237556953,\n",
       "    1.8321931384827779,\n",
       "    1.8266589034180993,\n",
       "    1.8216814556803642,\n",
       "    1.8110178347648749,\n",
       "    1.8007471615371644,\n",
       "    1.7929092145465981,\n",
       "    1.786063473714905,\n",
       "    1.7751810605447949,\n",
       "    1.7701144810998273,\n",
       "    1.7598755337803997,\n",
       "    1.7561266562037843,\n",
       "    1.750969667864366,\n",
       "    1.7388328033246234,\n",
       "    1.7363500736299893,\n",
       "    1.7238316836451462,\n",
       "    1.720372400775554,\n",
       "    1.7232314674407083,\n",
       "    1.7105423264296526,\n",
       "    1.7044633092762478,\n",
       "    1.6989321606702044,\n",
       "    1.6972755227902532,\n",
       "    1.6868498212822576,\n",
       "    1.6841438292509425,\n",
       "    1.6751884882484318],\n",
       "   [2.912823109061988,\n",
       "    2.6091076837371014,\n",
       "    2.4971092173176324,\n",
       "    2.419030918465224,\n",
       "    2.3398654666244663,\n",
       "    2.2843812693734113,\n",
       "    2.237351987142598,\n",
       "    2.2056334648784537,\n",
       "    2.1547107573252204,\n",
       "    2.1233465182496247,\n",
       "    2.0963269060074996,\n",
       "    2.0661403852941893,\n",
       "    2.043885225987224,\n",
       "    2.0172161090287304,\n",
       "    1.99693998865842,\n",
       "    1.979129001736853,\n",
       "    1.9629957049712712,\n",
       "    1.944139197201307,\n",
       "    1.9310889892359646,\n",
       "    1.9134888538974608,\n",
       "    1.9005861927850427,\n",
       "    1.8872371864887412,\n",
       "    1.874209257616553,\n",
       "    1.8578492624651448,\n",
       "    1.8509967520593231,\n",
       "    1.8373788936118693,\n",
       "    1.8224823541227977,\n",
       "    1.8207945176801565,\n",
       "    1.803748751933289,\n",
       "    1.797151307584483,\n",
       "    1.7904989820376613,\n",
       "    1.7851950289768301,\n",
       "    1.7701811190972974,\n",
       "    1.7671855645556578,\n",
       "    1.7591087810421957,\n",
       "    1.7509211971381722,\n",
       "    1.7379944112861412,\n",
       "    1.7333975711552956,\n",
       "    1.7302508568804773,\n",
       "    1.7211268141821923,\n",
       "    1.7180225733542533,\n",
       "    1.7092358441361974,\n",
       "    1.7047828343684932,\n",
       "    1.6994632555593485,\n",
       "    1.6958354614962095,\n",
       "    1.6892066609697152,\n",
       "    1.6850210865762107,\n",
       "    1.679965175509582,\n",
       "    1.669740960207145,\n",
       "    1.6715501481391928]],\n",
       "  'train_loss': [[3.193372477576611,\n",
       "    2.862557113393214,\n",
       "    2.695853003336021,\n",
       "    2.5675959342011194,\n",
       "    2.499652817651609,\n",
       "    2.4564314125979094,\n",
       "    2.411159196302274,\n",
       "    2.356273466187889,\n",
       "    2.3090502015411607,\n",
       "    2.281071037392069,\n",
       "    2.2564948797040456,\n",
       "    2.2153583508969015,\n",
       "    2.1967589686561593,\n",
       "    2.1651430528523354,\n",
       "    2.1322934121965207,\n",
       "    2.1353717076559944,\n",
       "    2.118075990146168,\n",
       "    2.0942656514431923,\n",
       "    2.082012872428815,\n",
       "    2.0602956049113352,\n",
       "    2.0496686600189897,\n",
       "    2.0250501513542973,\n",
       "    2.0125615105589394,\n",
       "    2.0301302323656296,\n",
       "    2.0046110685147025,\n",
       "    2.0087198432632833,\n",
       "    1.9616260993695025,\n",
       "    1.957141077398924,\n",
       "    1.946904167642155,\n",
       "    1.94199539050769,\n",
       "    1.9389316128950724,\n",
       "    1.9173364322132755,\n",
       "    1.9247651827182843,\n",
       "    1.903736454784293,\n",
       "    1.8848202890936765,\n",
       "    1.8861631432983554,\n",
       "    1.875795110720075,\n",
       "    1.8911444410722946,\n",
       "    1.8678833571334792,\n",
       "    1.887645394222123,\n",
       "    1.849843285221962,\n",
       "    1.8313774801656746,\n",
       "    1.8201389412188267,\n",
       "    1.843921677024203,\n",
       "    1.8435901674801383,\n",
       "    1.8232525162541156,\n",
       "    1.8381785085463425,\n",
       "    1.8080302352181028,\n",
       "    1.815514776553768,\n",
       "    1.8150578092778538],\n",
       "   [3.2284536208107264,\n",
       "    2.824486470090568,\n",
       "    2.6664741330786526,\n",
       "    2.562092457524144,\n",
       "    2.5096276162488493,\n",
       "    2.451088166475956,\n",
       "    2.393052770079913,\n",
       "    2.3581236547495488,\n",
       "    2.304422500898564,\n",
       "    2.277852781280813,\n",
       "    2.2438074072779135,\n",
       "    2.2135541278997724,\n",
       "    2.1934464683341455,\n",
       "    2.164673021821148,\n",
       "    2.150038237251434,\n",
       "    2.11367012893222,\n",
       "    2.1146289622841374,\n",
       "    2.086181281710026,\n",
       "    2.080200016059569,\n",
       "    2.059425624763982,\n",
       "    2.0463765880296507,\n",
       "    2.0322250107692277,\n",
       "    2.0075806943583787,\n",
       "    2.0000856725280047,\n",
       "    1.9846287604421378,\n",
       "    1.9712682856914086,\n",
       "    1.9480085273769219,\n",
       "    1.9555542242869137,\n",
       "    1.9369110732739934,\n",
       "    1.9245341588610354,\n",
       "    1.896792905520066,\n",
       "    1.9065059435915817,\n",
       "    1.8956821967390358,\n",
       "    1.901250132370127,\n",
       "    1.8974176232271502,\n",
       "    1.8751834811161003,\n",
       "    1.890308898980027,\n",
       "    1.8585425696112885,\n",
       "    1.852947832670779,\n",
       "    1.8532094644787072,\n",
       "    1.8494131649018652,\n",
       "    1.8289419079212315,\n",
       "    1.818271799465927,\n",
       "    1.8217995083740142,\n",
       "    1.801736170467747,\n",
       "    1.804138150279453,\n",
       "    1.8145302125238612,\n",
       "    1.8163193573183374,\n",
       "    1.7961309826864575,\n",
       "    1.7928250978761362]]},\n",
       " 'Shakespeare LSTM on Non IID': {'history': [{'accuracy': 0.23356881239309218,\n",
       "    'f1_macro': 0.016271294207603575,\n",
       "    'f1_weighted': 0.11776653400540035,\n",
       "    'loss': 2.8475171223135787,\n",
       "    'train_loss': 3.0229272611356843},\n",
       "   {'accuracy': 0.3579442774722885,\n",
       "    'f1_macro': 0.06517449852444901,\n",
       "    'f1_weighted': 0.28626700549873174,\n",
       "    'loss': 2.2794720017698142,\n",
       "    'train_loss': 2.493998846445161},\n",
       "   {'accuracy': 0.3905119011954347,\n",
       "    'f1_macro': 0.088873237477276,\n",
       "    'f1_weighted': 0.3304299397605979,\n",
       "    'loss': 2.1278864741814996,\n",
       "    'train_loss': 2.2635211467733622},\n",
       "   {'accuracy': 0.41253611913758614,\n",
       "    'f1_macro': 0.10292427175063776,\n",
       "    'f1_weighted': 0.35996560120585014,\n",
       "    'loss': 2.0264979446349143,\n",
       "    'train_loss': 2.201133020312656},\n",
       "   {'accuracy': 0.43033717637735924,\n",
       "    'f1_macro': 0.12465547385550088,\n",
       "    'f1_weighted': 0.3865291294684046,\n",
       "    'loss': 1.9651417773343436,\n",
       "    'train_loss': 2.1163020193495887},\n",
       "   {'accuracy': 0.44089991012495533,\n",
       "    'f1_macro': 0.1355861856741676,\n",
       "    'f1_weighted': 0.40161627321642407,\n",
       "    'loss': 1.9162426607572183,\n",
       "    'train_loss': 2.009485280383541},\n",
       "   {'accuracy': 0.4514433159059501,\n",
       "    'f1_macro': 0.13813920777491853,\n",
       "    'f1_weighted': 0.4094574238876562,\n",
       "    'loss': 1.8796987606308926,\n",
       "    'train_loss': 1.9685083964987793},\n",
       "   {'accuracy': 0.45879760719773477,\n",
       "    'f1_macro': 0.14581345399728876,\n",
       "    'f1_weighted': 0.41581030898155824,\n",
       "    'loss': 1.8471106054350233,\n",
       "    'train_loss': 1.941804758265878},\n",
       "   {'accuracy': 0.46679938537066207,\n",
       "    'f1_macro': 0.1512198025463069,\n",
       "    'f1_weighted': 0.43067829321374607,\n",
       "    'loss': 1.8106107938639515,\n",
       "    'train_loss': 1.8912598737747928},\n",
       "   {'accuracy': 0.47193096050330025,\n",
       "    'f1_macro': 0.1553868797276972,\n",
       "    'f1_weighted': 0.43211946973396914,\n",
       "    'loss': 1.791507285408315,\n",
       "    'train_loss': 1.8850696251698795},\n",
       "   {'accuracy': 0.4783671733815244,\n",
       "    'f1_macro': 0.16233698694026438,\n",
       "    'f1_weighted': 0.4414042013917285,\n",
       "    'loss': 1.7685192816278927,\n",
       "    'train_loss': 1.8412357190053559},\n",
       "   {'accuracy': 0.4824647023009944,\n",
       "    'f1_macro': 0.1657739188625328,\n",
       "    'f1_weighted': 0.44414482034428404,\n",
       "    'loss': 1.7498061709599977,\n",
       "    'train_loss': 1.8532745879163912},\n",
       "   {'accuracy': 0.48839838804758545,\n",
       "    'f1_macro': 0.17460204223378822,\n",
       "    'f1_weighted': 0.4561758028673021,\n",
       "    'loss': 1.7300418752548743,\n",
       "    'train_loss': 1.784127915074826},\n",
       "   {'accuracy': 0.4889878910289243,\n",
       "    'f1_macro': 0.1731513045607691,\n",
       "    'f1_weighted': 0.4540122074951646,\n",
       "    'loss': 1.7190872827677994,\n",
       "    'train_loss': 1.8000523874224452},\n",
       "   {'accuracy': 0.49450602549358796,\n",
       "    'f1_macro': 0.18156717930060137,\n",
       "    'f1_weighted': 0.4642002633716541,\n",
       "    'loss': 1.7038970134485383,\n",
       "    'train_loss': 1.8330779054810982},\n",
       "   {'accuracy': 0.4969316853020478,\n",
       "    'f1_macro': 0.18202963669814737,\n",
       "    'f1_weighted': 0.46626960258633887,\n",
       "    'loss': 1.688945885935057,\n",
       "    'train_loss': 1.7580150450492789},\n",
       "   {'accuracy': 0.500410719290277,\n",
       "    'f1_macro': 0.18494041252765328,\n",
       "    'f1_weighted': 0.4701169442749169,\n",
       "    'loss': 1.6772287296636188,\n",
       "    'train_loss': 1.756233756483666},\n",
       "   {'accuracy': 0.5039477371783102,\n",
       "    'f1_macro': 0.19229686644634536,\n",
       "    'f1_weighted': 0.47684072770578,\n",
       "    'loss': 1.66432765317092,\n",
       "    'train_loss': 1.7178759339635417},\n",
       "   {'accuracy': 0.5046821999091585,\n",
       "    'f1_macro': 0.18762126482696537,\n",
       "    'f1_weighted': 0.47544230934256176,\n",
       "    'loss': 1.6579952764904182,\n",
       "    'train_loss': 1.7376430153554057},\n",
       "   {'accuracy': 0.5089343525614388,\n",
       "    'f1_macro': 0.19731127858892636,\n",
       "    'f1_weighted': 0.4811258453745008,\n",
       "    'loss': 1.6476997495744772,\n",
       "    'train_loss': 1.7224761281924814},\n",
       "   {'accuracy': 0.5098524309749993,\n",
       "    'f1_macro': 0.19907234765819184,\n",
       "    'f1_weighted': 0.4842722822239171,\n",
       "    'loss': 1.6425897886089118,\n",
       "    'train_loss': 1.7006465038242844},\n",
       "   {'accuracy': 0.512867593764798,\n",
       "    'f1_macro': 0.20275480303128432,\n",
       "    'f1_weighted': 0.48620443278839026,\n",
       "    'loss': 1.6305530958983374,\n",
       "    'train_loss': 1.7149713822397714},\n",
       "   {'accuracy': 0.5131091933473139,\n",
       "    'f1_macro': 0.20251414655748315,\n",
       "    'f1_weighted': 0.4856991023278728,\n",
       "    'loss': 1.6286970330414585,\n",
       "    'train_loss': 1.6951030531832025},\n",
       "   {'accuracy': 0.516965122684268,\n",
       "    'f1_macro': 0.20395769362102825,\n",
       "    'f1_weighted': 0.4881293649730538,\n",
       "    'loss': 1.6162743089357516,\n",
       "    'train_loss': 1.633503169587799},\n",
       "   {'accuracy': 0.5191298549436106,\n",
       "    'f1_macro': 0.20859936387583933,\n",
       "    'f1_weighted': 0.493017515876293,\n",
       "    'loss': 1.6110250873030298,\n",
       "    'train_loss': 1.7498150341428587},\n",
       "   {'accuracy': 0.5195550702088387,\n",
       "    'f1_macro': 0.20787517580313028,\n",
       "    'f1_weighted': 0.493471054433471,\n",
       "    'loss': 1.6065913059915204,\n",
       "    'train_loss': 1.669883671161715},\n",
       "   {'accuracy': 0.5225605690153368,\n",
       "    'f1_macro': 0.2082956179433751,\n",
       "    'f1_weighted': 0.4961715074851074,\n",
       "    'loss': 1.5959330253134638,\n",
       "    'train_loss': 1.676783391128661},\n",
       "   {'accuracy': 0.5228504885143559,\n",
       "    'f1_macro': 0.2112020219764783,\n",
       "    'f1_weighted': 0.49716727987333204,\n",
       "    'loss': 1.5904769490907227,\n",
       "    'train_loss': 1.6145417298942832},\n",
       "   {'accuracy': 0.5247156372913787,\n",
       "    'f1_macro': 0.2112313959503124,\n",
       "    'f1_weighted': 0.5006971482219429,\n",
       "    'loss': 1.5861330620025536,\n",
       "    'train_loss': 1.641072180481959},\n",
       "   {'accuracy': 0.5253244682393189,\n",
       "    'f1_macro': 0.21119186734425607,\n",
       "    'f1_weighted': 0.49790944812752563,\n",
       "    'loss': 1.581698790123914,\n",
       "    'train_loss': 1.6351156581251196},\n",
       "   {'accuracy': 0.5267933937010156,\n",
       "    'f1_macro': 0.21422014037602866,\n",
       "    'f1_weighted': 0.5008873192783879,\n",
       "    'loss': 1.5764184988156629,\n",
       "    'train_loss': 1.6223056705611458},\n",
       "   {'accuracy': 0.5287358543444437,\n",
       "    'f1_macro': 0.21649061162468344,\n",
       "    'f1_weighted': 0.5026124354113877,\n",
       "    'loss': 1.5706082233061058,\n",
       "    'train_loss': 1.6134397439229058},\n",
       "   {'accuracy': 0.5295476289416972,\n",
       "    'f1_macro': 0.21939340251169634,\n",
       "    'f1_weighted': 0.5047259970024668,\n",
       "    'loss': 1.5667977651929532,\n",
       "    'train_loss': 1.5750611193689197},\n",
       "   {'accuracy': 0.5298858683572195,\n",
       "    'f1_macro': 0.21963405504933917,\n",
       "    'f1_weighted': 0.5037049365904002,\n",
       "    'loss': 1.564860292556882,\n",
       "    'train_loss': 1.6073043954778403},\n",
       "   {'accuracy': 0.5320216086666603,\n",
       "    'f1_macro': 0.22332122132437104,\n",
       "    'f1_weighted': 0.5082042817804553,\n",
       "    'loss': 1.5593764866350104,\n",
       "    'train_loss': 1.5656606193766973},\n",
       "   {'accuracy': 0.5309875624534921,\n",
       "    'f1_macro': 0.22023022901990663,\n",
       "    'f1_weighted': 0.5059594274671888,\n",
       "    'loss': 1.558853857151609,\n",
       "    'train_loss': 1.6639787953476368},\n",
       "   {'accuracy': 0.5320602645998628,\n",
       "    'f1_macro': 0.2266823912976481,\n",
       "    'f1_weighted': 0.5087271974169174,\n",
       "    'loss': 1.55571311718198,\n",
       "    'train_loss': 1.5972943314397874},\n",
       "   {'accuracy': 0.5332585985291417,\n",
       "    'f1_macro': 0.22850096670709663,\n",
       "    'f1_weighted': 0.510379076260205,\n",
       "    'loss': 1.553207228532053,\n",
       "    'train_loss': 1.6062459667298719},\n",
       "   {'accuracy': 0.5336741498110691,\n",
       "    'f1_macro': 0.2263982619695824,\n",
       "    'f1_weighted': 0.5098613310567716,\n",
       "    'loss': 1.5502998082172388,\n",
       "    'train_loss': 1.5952807216530138},\n",
       "   {'accuracy': 0.5344955883916233,\n",
       "    'f1_macro': 0.22764962821026127,\n",
       "    'f1_weighted': 0.5120692593990718,\n",
       "    'loss': 1.5466923223826607,\n",
       "    'train_loss': 1.5710656941032546},\n",
       "   {'accuracy': 0.534930467640152,\n",
       "    'f1_macro': 0.22827115785207538,\n",
       "    'f1_weighted': 0.5113399013260133,\n",
       "    'loss': 1.5467215852189886,\n",
       "    'train_loss': 1.6281479927798959},\n",
       "   {'accuracy': 0.5353363549387786,\n",
       "    'f1_macro': 0.2294624022243044,\n",
       "    'f1_weighted': 0.5096923717098041,\n",
       "    'loss': 1.5459146954630638,\n",
       "    'train_loss': 1.5988678019638942},\n",
       "   {'accuracy': 0.5358485460537125,\n",
       "    'f1_macro': 0.23003811942998198,\n",
       "    'f1_weighted': 0.5119262277338362,\n",
       "    'loss': 1.5427332955603381,\n",
       "    'train_loss': 1.6054383581401581},\n",
       "   {'accuracy': 0.5354813146882882,\n",
       "    'f1_macro': 0.23213645615046102,\n",
       "    'f1_weighted': 0.5134264142988052,\n",
       "    'loss': 1.5414720462681584,\n",
       "    'train_loss': 1.5922591195804696},\n",
       "   {'accuracy': 0.53641872106845,\n",
       "    'f1_macro': 0.23094452031730864,\n",
       "    'f1_weighted': 0.5142805666845266,\n",
       "    'loss': 1.5374806419610965,\n",
       "    'train_loss': 1.5894151637643483},\n",
       "   {'accuracy': 0.5370275520163901,\n",
       "    'f1_macro': 0.23488613855409746,\n",
       "    'f1_weighted': 0.5152186906861944,\n",
       "    'loss': 1.535264202696316,\n",
       "    'train_loss': 1.59015425998034},\n",
       "   {'accuracy': 0.5374141113484155,\n",
       "    'f1_macro': 0.23575497493836772,\n",
       "    'f1_weighted': 0.5159401153726255,\n",
       "    'loss': 1.5334494366114448,\n",
       "    'train_loss': 1.5745674502096108},\n",
       "   {'accuracy': 0.5380615982295582,\n",
       "    'f1_macro': 0.23559331940304726,\n",
       "    'f1_weighted': 0.5118828189001344,\n",
       "    'loss': 1.5325829232551345,\n",
       "    'train_loss': 1.6027718680014793},\n",
       "   {'accuracy': 0.5389796766431187,\n",
       "    'f1_macro': 0.23772395992005158,\n",
       "    'f1_weighted': 0.5172396211866443,\n",
       "    'loss': 1.528323637019524,\n",
       "    'train_loss': 1.5572001300359157},\n",
       "   {'accuracy': 0.5390086685930207,\n",
       "    'f1_macro': 0.2440888926296837,\n",
       "    'f1_weighted': 0.5151809743089254,\n",
       "    'loss': 1.5283290356346075,\n",
       "    'train_loss': 1.574802541603804}],\n",
       "  'hyperparams': {'C': 0.07,\n",
       "   'E': 1,\n",
       "   'K': 143,\n",
       "   'batch_size': 10,\n",
       "   'lr': 0.8,\n",
       "   'q': 0.001,\n",
       "   'rounds': 50,\n",
       "   'sampling': 'weighted'},\n",
       "  'test_accuracy': [[0.2712196913323734,\n",
       "    0.36329812422084135,\n",
       "    0.39083081264435576,\n",
       "    0.41505841877905236,\n",
       "    0.43134223064062543,\n",
       "    0.44424364834697566,\n",
       "    0.44647602848942275,\n",
       "    0.45545386897571444,\n",
       "    0.46249891280187866,\n",
       "    0.4676691438677194,\n",
       "    0.4718729766034964,\n",
       "    0.47719783140214733,\n",
       "    0.4830155493491307,\n",
       "    0.4883597321143829,\n",
       "    0.4908530398059472,\n",
       "    0.49455434541009113,\n",
       "    0.4989417938285803,\n",
       "    0.5005846709896885,\n",
       "    0.5030006668148478,\n",
       "    0.5051847270407916,\n",
       "    0.5077553465987611,\n",
       "    0.5103356301400311,\n",
       "    0.5124520424828706,\n",
       "    0.5145008069426056,\n",
       "    0.5162403239367203,\n",
       "    0.5177382413483189,\n",
       "    0.518453376112566,\n",
       "    0.5198643176744591,\n",
       "    0.5214492109357635,\n",
       "    0.5249185809406921,\n",
       "    0.5254404360389265,\n",
       "    0.5264744822520947,\n",
       "    0.5278564318640857,\n",
       "    0.5274215526155571,\n",
       "    0.5289871179102602,\n",
       "    0.5278274399141838,\n",
       "    0.5309972264367927,\n",
       "    0.5323501840988819,\n",
       "    0.5312774819525112,\n",
       "    0.5316833692511379,\n",
       "    0.5338674294770819,\n",
       "    0.535007779506557,\n",
       "    0.535539298588092,\n",
       "    0.5370662079495927,\n",
       "    0.5369309121833837,\n",
       "    0.5372594876156054,\n",
       "    0.5377910066971404,\n",
       "    0.5391922842757327,\n",
       "    0.5377426867806372,\n",
       "    0.5395885075910589],\n",
       "   [0.23356881239309218,\n",
       "    0.3579442774722885,\n",
       "    0.3905119011954347,\n",
       "    0.41253611913758614,\n",
       "    0.43033717637735924,\n",
       "    0.44089991012495533,\n",
       "    0.4514433159059501,\n",
       "    0.45879760719773477,\n",
       "    0.46679938537066207,\n",
       "    0.47193096050330025,\n",
       "    0.4783671733815244,\n",
       "    0.4824647023009944,\n",
       "    0.48839838804758545,\n",
       "    0.4889878910289243,\n",
       "    0.49450602549358796,\n",
       "    0.4969316853020478,\n",
       "    0.500410719290277,\n",
       "    0.5039477371783102,\n",
       "    0.5046821999091585,\n",
       "    0.5089343525614388,\n",
       "    0.5098524309749993,\n",
       "    0.512867593764798,\n",
       "    0.5131091933473139,\n",
       "    0.516965122684268,\n",
       "    0.5191298549436106,\n",
       "    0.5195550702088387,\n",
       "    0.5225605690153368,\n",
       "    0.5228504885143559,\n",
       "    0.5247156372913787,\n",
       "    0.5253244682393189,\n",
       "    0.5267933937010156,\n",
       "    0.5287358543444437,\n",
       "    0.5295476289416972,\n",
       "    0.5298858683572195,\n",
       "    0.5320216086666603,\n",
       "    0.5309875624534921,\n",
       "    0.5320602645998628,\n",
       "    0.5332585985291417,\n",
       "    0.5336741498110691,\n",
       "    0.5344955883916233,\n",
       "    0.534930467640152,\n",
       "    0.5353363549387786,\n",
       "    0.5358485460537125,\n",
       "    0.5354813146882882,\n",
       "    0.53641872106845,\n",
       "    0.5370275520163901,\n",
       "    0.5374141113484155,\n",
       "    0.5380615982295582,\n",
       "    0.5389796766431187,\n",
       "    0.5390086685930207]],\n",
       "  'test_loss': [[2.7035938599039215,\n",
       "    2.2579377224147725,\n",
       "    2.1358407852571974,\n",
       "    2.027087160219991,\n",
       "    1.9612273486142082,\n",
       "    1.9158540784627767,\n",
       "    1.888561520517078,\n",
       "    1.8634819240946185,\n",
       "    1.8324344365550713,\n",
       "    1.807886907217213,\n",
       "    1.784854603122236,\n",
       "    1.762212996107193,\n",
       "    1.7488854706517216,\n",
       "    1.7245316781840265,\n",
       "    1.7091671097364651,\n",
       "    1.6976799458169973,\n",
       "    1.687124206500537,\n",
       "    1.6799479909030592,\n",
       "    1.66476725580285,\n",
       "    1.6581602854194477,\n",
       "    1.6491556029254466,\n",
       "    1.6398009432504166,\n",
       "    1.6322506010409688,\n",
       "    1.623842622753465,\n",
       "    1.6171771362477898,\n",
       "    1.6085729771585624,\n",
       "    1.6052244731731709,\n",
       "    1.6018328058734266,\n",
       "    1.5932872010296666,\n",
       "    1.5870065873320605,\n",
       "    1.5824181888267006,\n",
       "    1.5772844880310408,\n",
       "    1.5738760908318383,\n",
       "    1.5713741904058394,\n",
       "    1.5710308439165235,\n",
       "    1.5700176410945916,\n",
       "    1.5613168733971574,\n",
       "    1.5597231924123143,\n",
       "    1.5552921574085592,\n",
       "    1.5555822461440565,\n",
       "    1.5490486450157157,\n",
       "    1.542934803437978,\n",
       "    1.5404002737738371,\n",
       "    1.5394361877793468,\n",
       "    1.5375478388529835,\n",
       "    1.5359747995723323,\n",
       "    1.5342088628794708,\n",
       "    1.5305147943645363,\n",
       "    1.5320727575023092,\n",
       "    1.5321176476492262],\n",
       "   [2.8475171223135787,\n",
       "    2.2794720017698142,\n",
       "    2.1278864741814996,\n",
       "    2.0264979446349143,\n",
       "    1.9651417773343436,\n",
       "    1.9162426607572183,\n",
       "    1.8796987606308926,\n",
       "    1.8471106054350233,\n",
       "    1.8106107938639515,\n",
       "    1.791507285408315,\n",
       "    1.7685192816278927,\n",
       "    1.7498061709599977,\n",
       "    1.7300418752548743,\n",
       "    1.7190872827677994,\n",
       "    1.7038970134485383,\n",
       "    1.688945885935057,\n",
       "    1.6772287296636188,\n",
       "    1.66432765317092,\n",
       "    1.6579952764904182,\n",
       "    1.6476997495744772,\n",
       "    1.6425897886089118,\n",
       "    1.6305530958983374,\n",
       "    1.6286970330414585,\n",
       "    1.6162743089357516,\n",
       "    1.6110250873030298,\n",
       "    1.6065913059915204,\n",
       "    1.5959330253134638,\n",
       "    1.5904769490907227,\n",
       "    1.5861330620025536,\n",
       "    1.581698790123914,\n",
       "    1.5764184988156629,\n",
       "    1.5706082233061058,\n",
       "    1.5667977651929532,\n",
       "    1.564860292556882,\n",
       "    1.5593764866350104,\n",
       "    1.558853857151609,\n",
       "    1.55571311718198,\n",
       "    1.553207228532053,\n",
       "    1.5502998082172388,\n",
       "    1.5466923223826607,\n",
       "    1.5467215852189886,\n",
       "    1.5459146954630638,\n",
       "    1.5427332955603381,\n",
       "    1.5414720462681584,\n",
       "    1.5374806419610965,\n",
       "    1.535264202696316,\n",
       "    1.5334494366114448,\n",
       "    1.5325829232551345,\n",
       "    1.528323637019524,\n",
       "    1.5283290356346075]],\n",
       "  'train_loss': [[2.8236461781605695,\n",
       "    2.4464992156235903,\n",
       "    2.2577788213010344,\n",
       "    2.1526429616258103,\n",
       "    2.0677854371190674,\n",
       "    1.99857221473088,\n",
       "    1.957125765121848,\n",
       "    1.9570556822916125,\n",
       "    1.9210626357272098,\n",
       "    1.8918481087975025,\n",
       "    1.8746457436565513,\n",
       "    1.839249468107204,\n",
       "    1.8513254775125205,\n",
       "    1.7771730036306497,\n",
       "    1.745241817693503,\n",
       "    1.7470599260077608,\n",
       "    1.7439942954862873,\n",
       "    1.7384987970091477,\n",
       "    1.737111645227111,\n",
       "    1.6872100789161284,\n",
       "    1.710802725348414,\n",
       "    1.7153997802567162,\n",
       "    1.711948915001305,\n",
       "    1.7020789509767202,\n",
       "    1.6610586425078353,\n",
       "    1.6859925634477317,\n",
       "    1.6938773204410218,\n",
       "    1.6730817008911223,\n",
       "    1.646248196497932,\n",
       "    1.648033157514302,\n",
       "    1.6494462986347742,\n",
       "    1.6346543887397753,\n",
       "    1.6178274117194877,\n",
       "    1.6242168298096513,\n",
       "    1.6455842852484772,\n",
       "    1.6274446186843334,\n",
       "    1.6055450628766998,\n",
       "    1.6244739741696208,\n",
       "    1.614045991938622,\n",
       "    1.6178137336820562,\n",
       "    1.6090781238734102,\n",
       "    1.5636890417431661,\n",
       "    1.5679168903724063,\n",
       "    1.542404838708119,\n",
       "    1.6035869369397122,\n",
       "    1.5767936703335417,\n",
       "    1.6085968775837158,\n",
       "    1.5345253134580856,\n",
       "    1.5977859300334427,\n",
       "    1.5714764791201488],\n",
       "   [3.0229272611356843,\n",
       "    2.493998846445161,\n",
       "    2.2635211467733622,\n",
       "    2.201133020312656,\n",
       "    2.1163020193495887,\n",
       "    2.009485280383541,\n",
       "    1.9685083964987793,\n",
       "    1.941804758265878,\n",
       "    1.8912598737747928,\n",
       "    1.8850696251698795,\n",
       "    1.8412357190053559,\n",
       "    1.8532745879163912,\n",
       "    1.784127915074826,\n",
       "    1.8000523874224452,\n",
       "    1.8330779054810982,\n",
       "    1.7580150450492789,\n",
       "    1.756233756483666,\n",
       "    1.7178759339635417,\n",
       "    1.7376430153554057,\n",
       "    1.7224761281924814,\n",
       "    1.7006465038242844,\n",
       "    1.7149713822397714,\n",
       "    1.6951030531832025,\n",
       "    1.633503169587799,\n",
       "    1.7498150341428587,\n",
       "    1.669883671161715,\n",
       "    1.676783391128661,\n",
       "    1.6145417298942832,\n",
       "    1.641072180481959,\n",
       "    1.6351156581251196,\n",
       "    1.6223056705611458,\n",
       "    1.6134397439229058,\n",
       "    1.5750611193689197,\n",
       "    1.6073043954778403,\n",
       "    1.5656606193766973,\n",
       "    1.6639787953476368,\n",
       "    1.5972943314397874,\n",
       "    1.6062459667298719,\n",
       "    1.5952807216530138,\n",
       "    1.5710656941032546,\n",
       "    1.6281479927798959,\n",
       "    1.5988678019638942,\n",
       "    1.6054383581401581,\n",
       "    1.5922591195804696,\n",
       "    1.5894151637643483,\n",
       "    1.59015425998034,\n",
       "    1.5745674502096108,\n",
       "    1.6027718680014793,\n",
       "    1.5572001300359157,\n",
       "    1.574802541603804]]}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "aSs9xXfpQ3ML"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(path + 'Local_Round_qFedAvg_1.pkl', 'rb') as file:\n",
    "  log_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "N8ep-MalQ3PO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5021081541079994, 0.502881788643045]\n"
     ]
    }
   ],
   "source": [
    "print([test_acc[-1] for test_acc in log_dict['Shakespeare LSTM on IID']['test_accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "_-2I2g0HQ3WA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5395885075910589, 0.5390086685930207]\n"
     ]
    }
   ],
   "source": [
    "print([test_acc[-1] for test_acc in log_dict['Shakespeare LSTM on Non IID']['test_accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfR3lDa7Q-8s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "FederatedAveraging",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "QSGD",
   "language": "python",
   "name": "qsgd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
