{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tushar-semwal/fedperf/blob/main/Vineeth/Local_Rounds/Shakesphere/qFedAvg/qFederatedAveraging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WQ6Rq0UiG6ev"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummaryX in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (1.3.0)\n",
      "Requirement already satisfied: unidecode in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (from torchsummaryX) (1.19.1)\n",
      "Requirement already satisfied: torch in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (from torchsummaryX) (1.7.1+cu101)\n",
      "Requirement already satisfied: pandas in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (from torchsummaryX) (0.24.2)\n",
      "Requirement already satisfied: typing-extensions in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (from torch->torchsummaryX) (3.7.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (from pandas->torchsummaryX) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2011k in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (from pandas->torchsummaryX) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages (from python-dateutil>=2.5.0->pandas->torchsummaryX) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummaryX unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yKcpjZLrQQJV"
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    import os\n",
    "    path = '/content/drive/MyDrive/Colab Notebooks/OpenMined/'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "except:\n",
    "    path = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0_nKpfq2h1R"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DLLNM9X2JbQ8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 26 12:10:24 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 31%   54C    P2   120W / 250W |   3106MiB / 11178MiB |     64%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "| 30%   54C    P2   116W / 250W |   2347MiB / 11178MiB |     60%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     71868      C   ...3/envs/QSGD-PT/bin/python      781MiB |\n",
      "|    0   N/A  N/A     71906      C   ...3/envs/QSGD-PT/bin/python      781MiB |\n",
      "|    0   N/A  N/A     74743      C   ...3/envs/QSGD-PT/bin/python      771MiB |\n",
      "|    0   N/A  N/A     75531      C   ...3/envs/QSGD-PT/bin/python      771MiB |\n",
      "|    1   N/A  N/A     73633      C   ...3/envs/QSGD-PT/bin/python      791MiB |\n",
      "|    1   N/A  N/A     73673      C   ...3/envs/QSGD-PT/bin/python      791MiB |\n",
      "|    1   N/A  N/A     76462      C   ...3/envs/QSGD-PT/bin/python      763MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import copy\n",
    "from functools import reduce\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from torchsummaryX import summary as summaryx\n",
    "from torchvision import transforms, utils, datasets\n",
    "from tqdm.notebook import tqdm\n",
    "from unidecode import unidecode\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Check assigned GPU\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "\n",
    "# set manual seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# general reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# gpu training specific\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY4eWzGiL6Mj"
   },
   "source": [
    "## Load the Shakespeare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hf03LRxof7Zj"
   },
   "outputs": [],
   "source": [
    "!rm -Rf data\n",
    "!mkdir -p data scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ngygA4-Fgobx"
   },
   "outputs": [],
   "source": [
    "GENERATE_DATASET = False  # If False, download the dataset provided by the q-FFL paper\n",
    "DATA_DIR = 'data/'\n",
    "# Dataset generation params\n",
    "SAMPLES_FRACTION = 1.  # If using an already generated dataset\n",
    "# SAMPLES_FRACTION = 0.2  # Fraction of total samples in the dataset - FedProx default script\n",
    "# SAMPLES_FRACTION = 0.05  # Fraction of total samples in the dataset - qFFL\n",
    "TRAIN_FRACTION = 0.8  # Train set size\n",
    "MIN_SAMPLES = 0  # Min samples per client (for filtering purposes) - FedProx\n",
    "# MIN_SAMPLES = 64  # Min samples per client (for filtering purposes) - qFFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nUmwJgJygoYD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-26 12:10:24--  http://www.gutenberg.org/files/100/100-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5757108 (5.5M) [text/plain]\n",
      "Saving to: ‘data/shakespeare.txt’\n",
      "\n",
      "data/shakespeare.tx 100%[===================>]   5.49M  1.02MB/s    in 6.2s    \n",
      "\n",
      "2021-04-26 12:10:31 (902 KB/s) - ‘data/shakespeare.txt’ saved [5757108/5757108]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download raw dataset\n",
    "# !wget https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt -O data/shakespeare.txt\n",
    "!wget --adjust-extension http://www.gutenberg.org/files/100/100-0.txt -O data/shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4dCvx80BgoVr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1n46Mftp3_ahRi1Z6jYhEriyLtdRDS1tD\n",
      "To: /home/vineeth/code/OM/Vineeth/Local_Rounds/Shakespeare/qFedAvg/05/shakespeare.zip\n",
      "2.96MB [00:00, 4.97MB/s]\n",
      "Archive:  shakespeare.zip\n",
      "   creating: shakespeare_paper/\n",
      "   creating: shakespeare_paper/test/\n",
      "  inflating: shakespeare_paper/test/all_data_niid_2_keep_0_test_8.json  \n",
      "   creating: shakespeare_paper/train/\n",
      "  inflating: shakespeare_paper/train/all_data_niid_2_keep_0_train_8.json  \n"
     ]
    }
   ],
   "source": [
    "if not GENERATE_DATASET:\n",
    "    !rm -Rf data/train data/test\n",
    "    !gdown --id 1n46Mftp3_ahRi1Z6jYhEriyLtdRDS1tD  # Download Shakespeare dataset used by the FedProx paper\n",
    "    !unzip shakespeare.zip\n",
    "    !mv -f shakespeare_paper/train data/\n",
    "    !mv -f shakespeare_paper/test data/\n",
    "    !rm -R shakespeare_paper/ shakespeare.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "a4pzFvPvhQhq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Length: 90\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    data = list(unidecode(f.read()))\n",
    "    corpus = list(set(list(data)))\n",
    "print('Corpus Length:', len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cce_-qnxhD4n"
   },
   "source": [
    "#### Dataset Preprocessing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Rt13M4IcgoTV"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "if GENERATE_DATASET:\n",
    "    # Download dataset generation scripts\n",
    "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/data/shakespeare/preprocess/preprocess_shakespeare.py -O scripts/preprocess_shakespeare.py\n",
    "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/data/shakespeare/preprocess/shake_utils.py -O scripts/shake_utils.py\n",
    "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/data/shakespeare/preprocess/gen_all_data.py -O scripts/gen_all_data.py\n",
    "\n",
    "    # Download data preprocessing scripts\n",
    "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/utils/sample.py -O scripts/sample.py\n",
    "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/utils/remove_users.py -O scripts/remove_users.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EIEyRW27goPo"
   },
   "outputs": [],
   "source": [
    "# Running scripts\n",
    "if GENERATE_DATASET:\n",
    "    !mkdir -p data/raw_data data/all_data data/train data/test\n",
    "    !python scripts/preprocess_shakespeare.py data/shakespeare.txt data/raw_data\n",
    "    !python scripts/gen_all_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mq8V6v_4hhhD"
   },
   "source": [
    "#### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "H2SjEBKoWDxv"
   },
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, x, y, corpus, seq_length):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.corpus = corpus\n",
    "        self.corpus_size = len(self.corpus)\n",
    "        super(ShakespeareDataset, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__} - (length: {self.__len__()})'\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        input_seq = self.x[i]\n",
    "        next_char = self.y[i]\n",
    "        # print('\\tgetitem', i, input_seq, next_char)\n",
    "        input_value = self.text2charindxs(input_seq).long()\n",
    "        target_value = self.get_label_from_char(next_char)\n",
    "        return input_value, target_value\n",
    "\n",
    "    def text2charindxs(self, text):\n",
    "        tensor = torch.zeros(len(text), dtype=torch.int32)\n",
    "        for i, c in enumerate(text):\n",
    "            tensor[i] = self.get_label_from_char(c)\n",
    "        return tensor\n",
    "\n",
    "    def get_label_from_char(self, c):\n",
    "        return self.corpus.index(c)\n",
    "\n",
    "    def get_char_from_label(self, l):\n",
    "        return self.corpus[l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fgJtS62lYAN"
   },
   "source": [
    "##### Federated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5DqL5pTmgn5X"
   },
   "outputs": [],
   "source": [
    "class ShakespeareFedDataset(ShakespeareDataset):\n",
    "    def __init__(self, x, y, corpus, seq_length):\n",
    "        super(ShakespeareFedDataset, self).__init__(x, y, corpus, seq_length)\n",
    "\n",
    "    def dataloader(self, batch_size, shuffle=True):\n",
    "        return DataLoader(self,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XelbyPsDlfgb"
   },
   "source": [
    "## Partitioning & Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOBblyFGlwlU"
   },
   "source": [
    "### IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "cSZFWKmsgn1p"
   },
   "outputs": [],
   "source": [
    "def iid_partition_(dataset, clients):\n",
    "  \"\"\"\n",
    "  I.I.D paritioning of data over clients\n",
    "  Shuffle the data\n",
    "  Split it between clients\n",
    "  \n",
    "  params:\n",
    "    - dataset (torch.utils.Dataset): Dataset\n",
    "    - clients (int): Number of Clients to split the data between\n",
    "\n",
    "  returns:\n",
    "    - Dictionary of image indexes for each client\n",
    "  \"\"\"\n",
    "\n",
    "  num_items_per_client = int(len(dataset)/clients)\n",
    "  client_dict = {}\n",
    "  image_idxs = [i for i in range(len(dataset))]\n",
    "\n",
    "  for i in range(clients):\n",
    "    client_dict[i] = set(np.random.choice(image_idxs, num_items_per_client, replace=False))\n",
    "    image_idxs = list(set(image_idxs) - client_dict[i])\n",
    "\n",
    "  return client_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-lGwDyhSll9h"
   },
   "outputs": [],
   "source": [
    "def iid_partition(corpus, seq_length=80, val_split=False):\n",
    "\n",
    "    train_file = [os.path.join(DATA_DIR, 'train', f) for f in os.listdir(f'{DATA_DIR}/train') if f.endswith('.json')][0]\n",
    "    test_file = [os.path.join(DATA_DIR, 'test', f) for f in os.listdir(f'{DATA_DIR}/test') if f.endswith('.json')][0]\n",
    "\n",
    "    with open(train_file, 'r') as file:\n",
    "        data_train = json.loads(unidecode(file.read()))\n",
    "\n",
    "    with open(test_file, 'r') as file:\n",
    "        data_test = json.loads(unidecode(file.read()))\n",
    "\n",
    "    \n",
    "    total_samples_train = sum(data_train['num_samples'])\n",
    "\n",
    "    data_dict = {}\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    x_test, y_test = [], []\n",
    "    # x_val, y_val = [], []\n",
    "\n",
    "    users = list(zip(data_train['users'], data_train['num_samples']))\n",
    "    # random.shuffle(users)\n",
    "\n",
    "\n",
    "\n",
    "    total_samples = int(sum(data_train['num_samples']) * SAMPLES_FRACTION)\n",
    "    print('Objective', total_samples, '/', sum(data_train['num_samples']))\n",
    "    sample_count = 0\n",
    "    \n",
    "    for i, (author_id, samples) in enumerate(users):\n",
    "\n",
    "        if sample_count >= total_samples:\n",
    "            print('Max samples reached', sample_count, '/', total_samples)\n",
    "            break\n",
    "\n",
    "        if samples < MIN_SAMPLES: # or data_train['num_samples'][i] > 10000:\n",
    "            print('SKIP', author_id, samples)\n",
    "            continue\n",
    "        else:\n",
    "            udata_train = data_train['user_data'][author_id]\n",
    "            max_samples = samples if (sample_count + samples) <= total_samples else (sample_count + samples - total_samples) \n",
    "            \n",
    "            sample_count += max_samples\n",
    "            # print('sample_count', sample_count)\n",
    "\n",
    "            x_train.extend(data_train['user_data'][author_id]['x'][:max_samples])\n",
    "            y_train.extend(data_train['user_data'][author_id]['y'][:max_samples])\n",
    "\n",
    "            author_data = data_test['user_data'][author_id]\n",
    "            test_size = int(len(author_data['x']) * SAMPLES_FRACTION)\n",
    "\n",
    "            if val_split:\n",
    "                x_test.extend(author_data['x'][:int(test_size / 2)])\n",
    "                y_test.extend(author_data['y'][:int(test_size / 2)])\n",
    "                # x_val.extend(author_data['x'][int(test_size / 2):])\n",
    "                # y_val.extend(author_data['y'][int(test_size / 2):int(test_size)])\n",
    "\n",
    "            else:\n",
    "                x_test.extend(author_data['x'][:int(test_size)])\n",
    "                y_test.extend(author_data['y'][:int(test_size)])\n",
    "\n",
    "    train_ds = ShakespeareDataset(x_train, y_train, corpus, seq_length)\n",
    "    test_ds = ShakespeareDataset(x_test, y_test, corpus, seq_length)\n",
    "    # val_ds = ShakespeareDataset(x_val, y_val, corpus, seq_length)\n",
    "\n",
    "    data_dict = iid_partition_(train_ds, clients=len(users))\n",
    "\n",
    "    return train_ds, data_dict, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFvc8mLoouKa"
   },
   "source": [
    "### Non-IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GZ76WsCZot9s"
   },
   "outputs": [],
   "source": [
    "def noniid_partition(corpus, seq_length=80, val_split=False):\n",
    "\n",
    "    train_file = [os.path.join(DATA_DIR, 'train', f) for f in os.listdir(f'{DATA_DIR}/train') if f.endswith('.json')][0]\n",
    "    test_file = [os.path.join(DATA_DIR, 'test', f) for f in os.listdir(f'{DATA_DIR}/test') if f.endswith('.json')][0]\n",
    "\n",
    "    with open(train_file, 'r') as file:\n",
    "        data_train = json.loads(unidecode(file.read()))\n",
    "\n",
    "    with open(test_file, 'r') as file:\n",
    "        data_test = json.loads(unidecode(file.read()))\n",
    "\n",
    "    \n",
    "    total_samples_train = sum(data_train['num_samples'])\n",
    "\n",
    "    data_dict = {}\n",
    "\n",
    "    x_test, y_test = [], []\n",
    "\n",
    "    users = list(zip(data_train['users'], data_train['num_samples']))\n",
    "    # random.shuffle(users)\n",
    "\n",
    "    total_samples = int(sum(data_train['num_samples']) * SAMPLES_FRACTION)\n",
    "    print('Objective', total_samples, '/', sum(data_train['num_samples']))\n",
    "    sample_count = 0\n",
    "    \n",
    "    for i, (author_id, samples) in enumerate(users):\n",
    "\n",
    "        if sample_count >= total_samples:\n",
    "            print('Max samples reached', sample_count, '/', total_samples)\n",
    "            break\n",
    "\n",
    "        if samples < MIN_SAMPLES: # or data_train['num_samples'][i] > 10000:\n",
    "            print('SKIP', author_id, samples)\n",
    "            continue\n",
    "        else:\n",
    "            udata_train = data_train['user_data'][author_id]\n",
    "            max_samples = samples if (sample_count + samples) <= total_samples else (sample_count + samples - total_samples) \n",
    "            \n",
    "            sample_count += max_samples\n",
    "            # print('sample_count', sample_count)\n",
    "\n",
    "            x_train = data_train['user_data'][author_id]['x'][:max_samples]\n",
    "            y_train = data_train['user_data'][author_id]['y'][:max_samples]\n",
    "\n",
    "            train_ds = ShakespeareFedDataset(x_train, y_train, corpus, seq_length)\n",
    "\n",
    "            x_val, y_val = None, None\n",
    "            val_ds = None\n",
    "            author_data = data_test['user_data'][author_id]\n",
    "            test_size = int(len(author_data['x']) * SAMPLES_FRACTION)\n",
    "            if val_split:\n",
    "                x_test += author_data['x'][:int(test_size / 2)]\n",
    "                y_test += author_data['y'][:int(test_size / 2)]\n",
    "                x_val = author_data['x'][int(test_size / 2):]\n",
    "                y_val = author_data['y'][int(test_size / 2):int(test_size)]\n",
    "\n",
    "                val_ds = ShakespeareFedDataset(x_val, y_val, corpus, seq_length)\n",
    "\n",
    "            else:\n",
    "                x_test += author_data['x'][:int(test_size)]\n",
    "                y_test += author_data['y'][:int(test_size)]\n",
    "\n",
    "            data_dict[author_id] = {\n",
    "                'train_ds': train_ds,\n",
    "                'val_ds': val_ds\n",
    "            }\n",
    "\n",
    "    test_ds = ShakespeareFedDataset(x_test, y_test, corpus, seq_length)\n",
    "\n",
    "    return data_dict, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWVOxcAao2_t"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQQQ2mLeo6EA"
   },
   "source": [
    "### Shakespeare LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "2mGXTrXRot7R"
   },
   "outputs": [],
   "source": [
    "class ShakespeareLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, classes, lstm_layers=2, dropout=0.1, batch_first=True):\n",
    "        super(ShakespeareLSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.classes = classes\n",
    "        self.no_layers = lstm_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=self.classes,\n",
    "                                      embedding_dim=self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, \n",
    "                            hidden_size=self.hidden_dim,\n",
    "                            num_layers=self.no_layers,\n",
    "                            batch_first=batch_first, \n",
    "                            dropout=dropout if self.no_layers > 1 else 0.)\n",
    "        self.fc = nn.Linear(hidden_dim, self.classes)\n",
    "\n",
    "    def forward(self, x, hc=None):\n",
    "        batch_size = x.size(0)\n",
    "        x_emb = self.embedding(x)\n",
    "        self.lstm.flatten_parameters()\n",
    "        out, (ht, ct) = self.lstm(x_emb.view(batch_size, -1, self.embedding_dim), hc)\n",
    "        dense = self.fc(ht[-1])\n",
    "        return dense\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(self.no_layers, batch_size, self.hidden_dim)),\n",
    "                Variable(torch.zeros(self.no_layers, batch_size, self.hidden_dim)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QsuJlVipMc8"
   },
   "source": [
    "#### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "n_Vb0BYpot5I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shakespeare LSTM SUMMARY\n",
      "==========================================================\n",
      "            Kernel Shape   Output Shape  Params  Mult-Adds\n",
      "Layer                                                     \n",
      "0_embedding      [8, 90]    [10, 80, 8]     720        720\n",
      "1_lstm                 -  [10, 80, 256]  798720     794624\n",
      "2_fc           [256, 90]       [10, 90]   23130      23040\n",
      "----------------------------------------------------------\n",
      "                      Totals\n",
      "Total params          822570\n",
      "Trainable params      822570\n",
      "Non-trainable params       0\n",
      "Mult-Adds             818384\n",
      "==========================================================\n",
      "            Kernel Shape   Output Shape  Params  Mult-Adds\n",
      "Layer                                                     \n",
      "0_embedding      [8, 90]    [10, 80, 8]     720        720\n",
      "1_lstm                 -  [10, 80, 256]  798720     794624\n",
      "2_fc           [256, 90]       [10, 90]   23130      23040\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "seq_length = 80 # mcmahan17a, fedprox, qFFL\n",
    "\n",
    "shakespeare_lstm = ShakespeareLSTM(input_dim=seq_length,  \n",
    "                                   embedding_dim=8,  # mcmahan17a, fedprox, qFFL\n",
    "                                   hidden_dim=256,  # mcmahan17a, fedprox impl\n",
    "                                   # hidden_dim=100,  # fedprox paper\n",
    "                                   classes=len(corpus),\n",
    "                                   lstm_layers=2,\n",
    "                                   dropout=0.1,\n",
    "                                   batch_first=True\n",
    "                                   )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  shakespeare_lstm.cuda()\n",
    "\n",
    "hc = shakespeare_lstm.init_hidden(batch_size)\n",
    "\n",
    "x_sample = torch.zeros((batch_size, seq_length),\n",
    "                       dtype=torch.long,\n",
    "                       device=(torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')))\n",
    "\n",
    "x_sample[0][0] = 1\n",
    "x_sample\n",
    "\n",
    "print(\"\\nShakespeare LSTM SUMMARY\")\n",
    "print(summaryx(shakespeare_lstm, x_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn7egnzTpeks"
   },
   "source": [
    "## FedAvg Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFFAfTOwpk4j"
   },
   "source": [
    "### Plot Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oyYjWa6IpnTY"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "367THsiTpo-C"
   },
   "outputs": [],
   "source": [
    "def plot_scores(history, exp_id, title, suffix):\n",
    "    accuracies = [x['accuracy'] for x in history]\n",
    "    f1_macro = [x['f1_macro'] for x in history]\n",
    "    f1_weighted = [x['f1_weighted'] for x in history]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(accuracies, 'tab:orange')\n",
    "    ax.set(xlabel='Rounds', ylabel='Test Accuracy', title=title)\n",
    "    ax.grid()\n",
    "    fig.savefig(f'{BASE_DIR}/{exp_id}/Test_Accuracy_{suffix}.jpg', format='jpg', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(f1_macro, 'tab:orange')\n",
    "    ax.set(xlabel='Rounds', ylabel='Test F1 (macro)', title=title)\n",
    "    ax.grid()\n",
    "    fig.savefig(f'{BASE_DIR}/{exp_id}/Test_F1_Macro_{suffix}.jpg', format='jpg')\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(f1_weighted, 'tab:orange')\n",
    "    ax.set(xlabel='Rounds', ylabel='Test F1 (weighted)', title=title)\n",
    "    ax.grid()\n",
    "    fig.savefig(f'{BASE_DIR}/{exp_id}/Test_F1_Weighted_{suffix}.jpg', format='jpg')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_losses(history, exp_id, title, suffix):\n",
    "    val_losses = [x['loss'] for x in history]\n",
    "    train_losses = [x['train_loss'] for x in history]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(train_losses, 'tab:orange')\n",
    "    ax.set(xlabel='Rounds', ylabel='Train Loss', title=title)\n",
    "    ax.grid()\n",
    "    fig.savefig(f'{BASE_DIR}/{exp_id}/Train_Loss_{suffix}.jpg', format='jpg')\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(val_losses, 'tab:orange')\n",
    "    ax.set(xlabel='Rounds', ylabel='Test Loss', title=title)\n",
    "    ax.grid()\n",
    "    fig.savefig(f'{BASE_DIR}/{exp_id}/Test_Loss_{suffix}.jpg', format='jpg')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ9PZM0Gp9ve"
   },
   "source": [
    "### Local Training (Client Update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "EDJFltwdotzZ"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, dataset, idxs):\n",
    "      self.dataset = dataset\n",
    "      self.idxs = list(idxs)\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.idxs)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "      data, label = self.dataset[self.idxs[item]]\n",
    "      return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HtRzU5Yepddq"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ClientUpdate(object):\n",
    "  def __init__(self, dataset, batch_size, learning_rate, epochs, idxs, q=None):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    if hasattr(dataset, 'dataloader'):\n",
    "        self.train_loader = dataset.dataloader(batch_size=batch_size, shuffle=True)\n",
    "    else:\n",
    "        self.train_loader = DataLoader(CustomDataset(dataset, idxs), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    self.learning_rate = learning_rate\n",
    "    self.epochs = epochs\n",
    "    self.q = q\n",
    "    if not self.q:\n",
    "        # TODO: Client itself adjust fairness \n",
    "        pass\n",
    "    self.mu = 1e-10\n",
    "\n",
    "  def train(self, model):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.5)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    e_loss = []\n",
    "    model_weights = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(1, self.epochs+1):\n",
    "\n",
    "      train_loss = 0.0\n",
    "\n",
    "      model.train()\n",
    "      # for data, labels in tqdm(self.train_loader):\n",
    "      for data, labels in self.train_loader:\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "          data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # make a forward pass\n",
    "        # print('input', data.size())\n",
    "        output = model(data)\n",
    "        # print('output', output.size())\n",
    "        # print('labels', labels.size())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, labels)\n",
    "        # do a backwards pass\n",
    "        loss.backward()\n",
    "        # perform a single optimization step\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "      # average losses\n",
    "      train_loss = train_loss/len(self.train_loader.dataset)\n",
    "      e_loss.append(train_loss)\n",
    "\n",
    "\n",
    "    total_loss = sum(e_loss)/len(e_loss)\n",
    "\n",
    "    # delta weights\n",
    "    model_weights_new = copy.deepcopy(model.state_dict())\n",
    "    L = 1.0 / self.learning_rate\n",
    "\n",
    "    delta_weights, delta, h = {}, {}, {}\n",
    "    loss_q = np.float_power(total_loss + self.mu, self.q)\n",
    "    # updating the global weights\n",
    "    for k in model_weights_new.keys():\n",
    "      delta_weights[k] = (model_weights[k] - model_weights_new[k]) * L\n",
    "      delta[k] =  loss_q * delta_weights[k]\n",
    "      # Estimation of the local Lipchitz constant\n",
    "      h[k] = (self.q * np.float_power(total_loss + self.mu, self.q - 1) * torch.pow(torch.norm(delta_weights[k]), 2)) + (L * loss_q)\n",
    "\n",
    "    return delta, h, total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3crFDN0xqGu6"
   },
   "source": [
    "### Server Side Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IZghjoL5_Hbv"
   },
   "outputs": [],
   "source": [
    "def client_sampling(n, m, weights=None, with_replace=False):\n",
    "    pk = None\n",
    "    if weights:\n",
    "        total_weights = np.sum(np.asarray(weights))\n",
    "        pk = [w * 1.0 / total_weights for w in weights]\n",
    "\n",
    "    return np.random.choice(range(n), m, replace=with_replace, p=pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "c085xSOoqEHk"
   },
   "outputs": [],
   "source": [
    "def training(model, rounds, batch_size, lr, ds, data_dict, test_ds, C, K, E, q, sampling, plt_title, plt_color,\n",
    "             classes, eval_every=1, tb_logger=None):\n",
    "  \"\"\"\n",
    "  Function implements the Federated Averaging Algorithm from the FedAvg paper.\n",
    "  Specifically, this function is used for the server side training and weight update\n",
    "\n",
    "  Params:\n",
    "    - model:           PyTorch model to train\n",
    "    - rounds:          Number of communication rounds for the client update\n",
    "    - batch_size:      Batch size for client update training\n",
    "    - lr:              Learning rate used for client update training\n",
    "    - ds:              Dataset used for training\n",
    "    - data_dict:       Type of data partition used for training (IID or non-IID)\n",
    "    - test_data_dict:  Data used for testing the model\n",
    "    - C:               Fraction of clients randomly chosen to perform computation on each round\n",
    "    - K:               Total number of clients\n",
    "    - E:               Number of training passes each client makes over its local dataset per round\n",
    "    - mu:              proximal term constant\n",
    "    - percentage:      percentage of selected client to have fewer than E epochs\n",
    "  Returns:\n",
    "    - model:           Trained model on the server\n",
    "  \"\"\"\n",
    "\n",
    "  # global model weights\n",
    "  global_weights = model.state_dict()\n",
    "\n",
    "  # training loss\n",
    "  train_loss = []\n",
    "\n",
    "  # test accuracy\n",
    "  test_accuracy = []\n",
    "\n",
    "  # test loss\n",
    "  test_loss = []\n",
    "\n",
    "  # history\n",
    "  history=[]\n",
    "\n",
    "  # store last loss for convergence\n",
    "  last_loss = 0.0\n",
    "\n",
    "  # total time taken \n",
    "  total_time = 0\n",
    "  start = time.time()\n",
    "\n",
    "  # client weights by total samples\n",
    "  p_k = None\n",
    "  if sampling == 'weighted':\n",
    "    p_k = [len(data_dict[c]) for c in data_dict] if ds else [len(data_dict[c]['train_ds']) for c in data_dict]\n",
    "\n",
    "  # Time log\n",
    "  start_time = time.time()\n",
    "\n",
    "  users_id = list(data_dict.keys())\n",
    "\n",
    "  for curr_round in range(1, rounds+1):\n",
    "    deltas, hs, local_loss = [], [], []\n",
    "\n",
    "    m = max(int(C*K), 1)    \n",
    "    S_t = client_sampling(K, m, weights=p_k, with_replace=False)\n",
    "\n",
    "    print('Round: {} Picking {}/{} clients: {}'.format(curr_round, m, K, S_t))\n",
    "\n",
    "    global_weights = model.state_dict()\n",
    "\n",
    "    for k in tqdm(S_t):\n",
    "      key = users_id[k]\n",
    "      ds_ = ds if ds else data_dict[key]['train_ds']\n",
    "      idxs = data_dict[key] if ds else None\n",
    "      # print(f'Client {k}: {len(idxs) if idxs else len(ds_)} samples')\n",
    "      local_update = ClientUpdate(dataset=ds_, batch_size=batch_size, learning_rate=lr, epochs=E, idxs=idxs, q=q)\n",
    "      delta_k, h_k, loss = local_update.train(model=copy.deepcopy(model))\n",
    "\n",
    "      deltas.append(copy.deepcopy(delta_k))\n",
    "      hs.append(copy.deepcopy(h_k))\n",
    "      local_loss.append(copy.deepcopy(loss))\n",
    "\n",
    "      if tb_logger:\n",
    "        tb_logger.add_scalar(f'Round/S{k}', loss, curr_round)\n",
    "\n",
    "    # Perform qFedAvg\n",
    "    h_sum = copy.deepcopy(hs[0])\n",
    "    delta_sum = copy.deepcopy(deltas[0])\n",
    "    \n",
    "    for k in h_sum.keys():\n",
    "        for i in range(1, len(hs)):\n",
    "            h_sum[k] += hs[i][k]\n",
    "            delta_sum[k] += deltas[i][k]\n",
    "\n",
    "    new_weights = {}\n",
    "    for k in delta_sum.keys():\n",
    "        for i in range(len(deltas)):\n",
    "            new_weights[k] = delta_sum[k] / h_sum[k]\n",
    "\n",
    "    # Updating global model weights\n",
    "    for k in global_weights.keys():\n",
    "        global_weights[k] -= new_weights[k]\n",
    "\n",
    "    # move the updated weights to our model state dict\n",
    "    model.load_state_dict(global_weights)\n",
    "\n",
    "    # loss\n",
    "    loss_avg = sum(local_loss) / len(local_loss)\n",
    "    print('Round: {}... \\tAverage Loss: {}'.format(curr_round, round(loss_avg, 3)))\n",
    "    train_loss.append(loss_avg)\n",
    "    if tb_logger:\n",
    "        tb_logger.add_scalar(f'Train/Loss', loss_avg, curr_round)\n",
    "\n",
    "    # testing\n",
    "    # if curr_round % eval_every == 0:\n",
    "    test_scores = testing(model, test_ds, batch_size * 2, nn.CrossEntropyLoss(), len(classes), classes)\n",
    "    test_scores['train_loss'] = loss_avg\n",
    "    test_loss_current, test_accuracy_current = test_scores['loss'], test_scores['accuracy']\n",
    "\n",
    "    history.append(test_scores)\n",
    "    test_accuracy.append(test_accuracy_current)\n",
    "    test_loss.append(test_loss_current)\n",
    "    \n",
    "    # print('Round: {}... \\tAverage Loss: {} \\tTest Loss: {} \\tTest Acc: {}'.format(curr_round, round(loss_avg, 3), round(test_loss, 3), round(test_accuracy, 3)))\n",
    "\n",
    "    if tb_logger:\n",
    "        tb_logger.add_scalar(f'Test/Loss', test_scores['loss'], curr_round)\n",
    "        tb_logger.add_scalars(f'Test/Scores', {\n",
    "            'accuracy': test_scores['accuracy'], 'f1_macro': test_scores['f1_macro'], 'f1_weighted': test_scores['f1_weighted']\n",
    "        }, curr_round)\n",
    "    \n",
    "    # update the last loss\n",
    "    last_loss = loss_avg\n",
    "\n",
    "  end = time.time()\n",
    "  \n",
    "  # plot train loss\n",
    "  fig, ax = plt.subplots()\n",
    "  x_axis = np.arange(1, rounds+1)\n",
    "  y_axis = np.array(train_loss)\n",
    "  ax.plot(x_axis, y_axis)\n",
    "\n",
    "  ax.set(xlabel='Number of Rounds', ylabel='Train Loss', title=plt_title)\n",
    "  ax.grid()\n",
    "  fig.savefig(plt_title+'_Train_loss.jpg', format='jpg')\n",
    "  plt.show()\n",
    "  \n",
    "  fig, ax = plt.subplots()\n",
    "  x_axis = np.arange(1, rounds+1)\n",
    "  y_axis = np.array(test_loss)\n",
    "  ax.plot(x_axis, y_axis, 'tab:'+plt_color)\n",
    "\n",
    "  ax.set(xlabel='Number of Rounds', ylabel='Test Loss',\n",
    "       title=plt_title)\n",
    "  ax.grid()\n",
    "  fig.savefig(plt_title+'_Test_loss.jpg', format='jpg')\n",
    "  plt.show()\n",
    "\n",
    "  # plot test accuracy\n",
    "  fig, ax = plt.subplots()\n",
    "  x_axis = np.arange(1, rounds+1)\n",
    "  y_axis = np.array(test_accuracy)\n",
    "  ax.plot(x_axis, y_axis)\n",
    "\n",
    "  ax.set(xlabel='Number of Rounds', ylabel='Test Accuracy', title=plt_title)\n",
    "  ax.grid()\n",
    "  fig.savefig(plt_title+'_Test_Accuracy.jpg', format='jpg')\n",
    "  plt.show()\n",
    "\n",
    "  print(\"Training Done! Total time taken to Train: {}\".format(end-start))\n",
    "\n",
    "  return model, train_loss, test_accuracy, test_loss, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXtGLkoAqLIW"
   },
   "source": [
    "### Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "dQJIJno4qKvc"
   },
   "outputs": [],
   "source": [
    "def testing(model, dataset, bs, criterion, num_classes, classes, print_all=False):\n",
    "  #test loss \n",
    "  test_loss = 0.0\n",
    "  y_true, y_hat = None, None\n",
    "\n",
    "  correct_class = list(0 for i in range(num_classes))\n",
    "  total_class = list(0 for i in range(num_classes))\n",
    "\n",
    "  if hasattr(dataset, 'dataloader'):\n",
    "    test_loader = dataset.dataloader(batch_size=bs, shuffle=False)\n",
    "  else:\n",
    "    test_loader = DataLoader(dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "  l = len(test_loader)\n",
    "\n",
    "  model.eval()\n",
    "  for i, (data, labels) in enumerate(tqdm(test_loader)):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "      data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "    output = model(data)\n",
    "    loss = criterion(output, labels)\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "\n",
    "    _, pred = torch.max(output, dim=1)\n",
    "\n",
    "    # For F1Score\n",
    "    y_true = np.append(y_true, labels.data.view_as(pred).cpu().numpy()) if i != 0 else labels.data.view_as(pred).cpu().numpy()\n",
    "    y_hat = np.append(y_hat, pred.cpu().numpy()) if i != 0 else pred.cpu().numpy()\n",
    "\n",
    "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
    "\n",
    "    #test accuracy for each object class\n",
    "    # for i in range(num_classes):\n",
    "    #   label = labels.data[i]\n",
    "    #   correct_class[label] += correct[i].item()\n",
    "    #   total_class[label] += 1\n",
    "\n",
    "    for i, lbl in enumerate(labels.data):\n",
    "      try:\n",
    "        # print(type(lbl))\n",
    "        # correct_class[lbl.data[0]] += correct.data[i]\n",
    "        correct_class[lbl.item()] += correct[i]\n",
    "        total_class[lbl.item()] += 1\n",
    "      except:\n",
    "          print('Error', lbl, i)\n",
    "    \n",
    "  # avg test loss\n",
    "  test_loss = test_loss/len(test_loader.dataset)\n",
    "  print(\"Test Loss: {:.6f}\\n\".format(test_loss))\n",
    "\n",
    "  # Avg F1 Score\n",
    "  f1_macro = f1_score(y_true, y_hat, average='macro')\n",
    "  # F1-Score -> weigthed to consider class imbalance\n",
    "  f1_weighted =  f1_score(y_true, y_hat, average='weighted')\n",
    "  print(\"F1 Score: {:.6f} (macro) {:.6f} (weighted) %\\n\".format(f1_macro, f1_weighted))\n",
    "\n",
    "  # print test accuracy\n",
    "  if print_all:\n",
    "    for i in range(num_classes):\n",
    "        if total_class[i] > 0:\n",
    "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % \n",
    "                    (classes[i], 100 * correct_class[i] / total_class[i],\n",
    "                    np.sum(correct_class[i]), np.sum(total_class[i])))\n",
    "        else:\n",
    "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "  overall_accuracy = np.sum(correct_class) / np.sum(total_class)\n",
    "\n",
    "  print('\\nFinal Test  Accuracy: {:.3f} ({}/{})'.format(overall_accuracy, np.sum(correct_class), np.sum(total_class)))\n",
    "\n",
    "  return {'loss': test_loss, 'accuracy': overall_accuracy, 'f1_macro': f1_macro, 'f1_weighted': f1_weighted}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxqXLBd8qbC2"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "c8gl5P3SMq4a"
   },
   "outputs": [],
   "source": [
    "log_dict = {}\n",
    "NUM_REPEAT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "E2CfSkNVqKtL"
   },
   "outputs": [],
   "source": [
    "seq_length = 80  # mcmahan17a, fedprox, qFFL\n",
    "embedding_dim = 8  # mcmahan17a, fedprox, qFFL\n",
    "# hidden_dim = 100  # fedprox paper\n",
    "hidden_dim = 256  # mcmahan17a, fedprox impl\n",
    "num_classes = len(corpus)\n",
    "classes = list(range(num_classes))\n",
    "lstm_layers = 2  # mcmahan17a, fedprox, qFFL\n",
    "dropout = 0.1  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPIpStyNJ-63"
   },
   "source": [
    "## LSTM qFedAvg on IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "gpS1gyJ_H_MA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective 413629 / 413629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Total users:', 143)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, data_dict, test_ds = iid_partition(corpus, seq_length, val_split=True)\n",
    "\n",
    "total_clients = len(data_dict.keys())\n",
    "'Total users:', total_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYoGsy05H_RC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Run Number:  0\n",
      "Objective 413629 / 413629\n",
      "Round: 1 Picking 10/143 clients: [125  23 129  49  10  65  87  75  55 102]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5964e3303943dba48c744b19f74308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
    "test_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "\n",
    "for exp_num in range(NUM_REPEAT):\n",
    "  print(\"Experiment Run Number: \", exp_num)\n",
    "\n",
    "  # partition data\n",
    "  train_ds, data_dict, test_ds = iid_partition(corpus, seq_length, val_split=True)  # Not using val_ds but makes train eval periods faster\n",
    "  total_clients = len(data_dict.keys())\n",
    "  # number of training rounds\n",
    "  rounds = 50\n",
    "  # client fraction\n",
    "  C = 0.07  # 10 clients\n",
    "  # number of clients\n",
    "  K = total_clients\n",
    "  # number of training passes on local dataset for each roung\n",
    "  E = 5\n",
    "  # batch size\n",
    "  batch_size = 10\n",
    "  # learning Rate\n",
    "  lr = 0.8\n",
    "  # fairness\n",
    "  q = 0.001  # qFFL\n",
    "  # sampling\n",
    "  # self.sampling = 'uniform'\n",
    "  sampling = 'weighted'\n",
    "\n",
    "  shakespeare_lstm = ShakespeareLSTM(input_dim=seq_length,  \n",
    "                                    embedding_dim=embedding_dim,  \n",
    "                                    hidden_dim=hidden_dim,\n",
    "                                    classes=num_classes,\n",
    "                                    lstm_layers=lstm_layers,\n",
    "                                    dropout=dropout,\n",
    "                                    batch_first=True\n",
    "                                    )\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "      shakespeare_lstm.cuda()\n",
    "\n",
    "  lstm_iid_trained, train_loss, test_accuracy, test_loss, history = training(shakespeare_lstm,\n",
    "                                          rounds, batch_size, lr,\n",
    "                                          train_ds,\n",
    "                                          data_dict,\n",
    "                                          test_ds,\n",
    "                                          C, K, E, q, sampling,\n",
    "                                          'Shakespeare LSTM on IID', \"green\",\n",
    "                                          corpus, # classes\n",
    "                                          )\n",
    "\n",
    "  train_loss_multiple_runs[exp_num] = train_loss\n",
    "  test_accuracy_multiple_runs[exp_num] = test_accuracy\n",
    "  test_loss_multiple_runs[exp_num] = test_loss\n",
    "\n",
    "  del lstm_iid_trained\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-irvQHuNNTtB"
   },
   "outputs": [],
   "source": [
    "hyperparams = {'rounds': rounds,   \n",
    "               'C': C,\n",
    "               'K': K,\n",
    "               'E': E,\n",
    "               'batch_size': batch_size,\n",
    "               'lr': lr,\n",
    "               'q': q,\n",
    "               'sampling': sampling,\n",
    "               }\n",
    "\n",
    "log_dict['Shakespeare LSTM on IID'] = {'train_loss': train_loss_multiple_runs, \n",
    "                                'test_loss': test_loss_multiple_runs, \n",
    "                                'test_accuracy': test_accuracy_multiple_runs,\n",
    "                                'history': history,\n",
    "                                'hyperparams': hyperparams,\n",
    "                                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QO0GkVyKEgu"
   },
   "source": [
    "## LSTM qFedAvg on Non IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wsHRfwmKGdS"
   },
   "outputs": [],
   "source": [
    "data_dict, test_ds = noniid_partition(corpus, seq_length=seq_length, val_split=False)\n",
    " \n",
    "total_clients = len(data_dict.keys())  \n",
    "'Total users:', total_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNJZG5tvKGgd"
   },
   "outputs": [],
   "source": [
    "train_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
    "test_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "\n",
    "for exp_num in range(NUM_REPEAT):\n",
    "  print(\"Experiment Run Number: \", exp_num)\n",
    "  # partition dataset\n",
    "  data_dict, test_ds = noniid_partition(corpus, seq_length=seq_length, val_split=False)\n",
    "  total_clients = len(data_dict.keys())  \n",
    "  # number of training rounds\n",
    "  rounds = 50\n",
    "  # client fraction\n",
    "  C = 0.07  # 10 clients\n",
    "  # number of clients\n",
    "  K = total_clients\n",
    "  # number of training passes on local dataset for each roung\n",
    "  E = 5\n",
    "  # batch size\n",
    "  batch_size = 10\n",
    "  # learning Rate\n",
    "  lr = 0.8\n",
    "  # fairness\n",
    "  q = 0.001  # qFFL\n",
    "  # sampling\n",
    "  # self.sampling = 'uniform'\n",
    "  sampling = 'weighted'\n",
    "\n",
    "  shakespeare_lstm = ShakespeareLSTM(input_dim=seq_length,\n",
    "                                        embedding_dim=embedding_dim,\n",
    "                                        hidden_dim=hidden_dim,\n",
    "                                        classes=num_classes,\n",
    "                                        lstm_layers=lstm_layers,\n",
    "                                        dropout=dropout,\n",
    "                                        batch_first=True\n",
    "                                        )\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "      shakespeare_lstm.cuda()\n",
    "\n",
    "  lstm_non_iid_trained, train_loss, test_accuracy, test_loss, history = training(shakespeare_lstm,\n",
    "                                                rounds, batch_size, lr,\n",
    "                                                None, #  ds empty as it is included in data_dict\n",
    "                                                data_dict,\n",
    "                                                test_ds,\n",
    "                                                C, K, E, q, sampling,\n",
    "                                                'Shakespeare LSTM on Non IID', \"green\",\n",
    "                                                corpus, # classes,\n",
    "                                                )\n",
    "\n",
    "  train_loss_multiple_runs[exp_num] = train_loss\n",
    "  test_accuracy_multiple_runs[exp_num] = test_accuracy\n",
    "  test_loss_multiple_runs[exp_num] = test_loss\n",
    "\n",
    "  del lstm_non_iid_trained\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uANkBpTRKGkv"
   },
   "outputs": [],
   "source": [
    "hyperparams = {'rounds': rounds,\n",
    "               'C': C,\n",
    "               'K': K, \n",
    "               'E': E,\n",
    "               'batch_size': batch_size,\n",
    "               'lr': lr,\n",
    "               'q': q,\n",
    "               'sampling': sampling,\n",
    "               }\n",
    "\n",
    "log_dict['Shakespeare LSTM on Non IID'] = {'train_loss': train_loss_multiple_runs, \n",
    "                                'test_loss': test_loss_multiple_runs, \n",
    "                                'test_accuracy': test_accuracy_multiple_runs,\n",
    "                                'history': history,\n",
    "                                'hyperparams': hyperparams,\n",
    "                                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShdScPNuQzUQ"
   },
   "source": [
    "## Pickle Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwHt7jviQ1AV"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(path + 'Local_Round_qFedAvg_5.pkl', 'wb') as file:\n",
    "  pickle.dump(log_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "qNkwXxO8Q3Ei"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Shakespeare LSTM on IID': {'history': [{'accuracy': 0.2923178090669967,\n",
       "    'f1_macro': 0.03620456025277612,\n",
       "    'f1_weighted': 0.2067964785918393,\n",
       "    'loss': 2.652704846212485,\n",
       "    'train_loss': 2.7272956011350553},\n",
       "   {'accuracy': 0.37169271236267987,\n",
       "    'f1_macro': 0.08239088495958753,\n",
       "    'f1_weighted': 0.3093051778947028,\n",
       "    'loss': 2.2233643219319914,\n",
       "    'train_loss': 2.3028240809854466},\n",
       "   {'accuracy': 0.4005879622466347,\n",
       "    'f1_macro': 0.10941497333558184,\n",
       "    'f1_weighted': 0.3493957678614938,\n",
       "    'loss': 2.1176259331318703,\n",
       "    'train_loss': 2.045692445731736},\n",
       "   {'accuracy': 0.41958069008200527,\n",
       "    'f1_macro': 0.12614032595539745,\n",
       "    'f1_weighted': 0.3762842007138118,\n",
       "    'loss': 2.0671037282554354,\n",
       "    'train_loss': 1.836873912240223},\n",
       "   {'accuracy': 0.42936716695033267,\n",
       "    'f1_macro': 0.1395592920702932,\n",
       "    'f1_weighted': 0.3901513356658677,\n",
       "    'loss': 2.03810870114892,\n",
       "    'train_loss': 1.6999859041227015},\n",
       "   {'accuracy': 0.43872814482438494,\n",
       "    'f1_macro': 0.1508611162444862,\n",
       "    'f1_weighted': 0.40322423583328265,\n",
       "    'loss': 2.024138481455201,\n",
       "    'train_loss': 1.6067341897176657},\n",
       "   {'accuracy': 0.44431765434008974,\n",
       "    'f1_macro': 0.15826616939088653,\n",
       "    'f1_weighted': 0.4114671033997695,\n",
       "    'loss': 2.003778269855384,\n",
       "    'train_loss': 1.514981447570335},\n",
       "   {'accuracy': 0.44634844499458454,\n",
       "    'f1_macro': 0.15994666863174353,\n",
       "    'f1_weighted': 0.41447692698136335,\n",
       "    'loss': 2.0106139550016944,\n",
       "    'train_loss': 1.4513321400982073},\n",
       "   {'accuracy': 0.4503133219866935,\n",
       "    'f1_macro': 0.16367319530524488,\n",
       "    'f1_weighted': 0.41921244620583353,\n",
       "    'loss': 2.0016165157846624,\n",
       "    'train_loss': 1.3845416207666437},\n",
       "   {'accuracy': 0.4504487080303265,\n",
       "    'f1_macro': 0.16944829821511392,\n",
       "    'f1_weighted': 0.41856833784408704,\n",
       "    'loss': 2.0066052218845813,\n",
       "    'train_loss': 1.3457389915995508},\n",
       "   {'accuracy': 0.45400742689153645,\n",
       "    'f1_macro': 0.16973940668567858,\n",
       "    'f1_weighted': 0.42239999432726477,\n",
       "    'loss': 1.9923861056857899,\n",
       "    'train_loss': 1.320808141418265},\n",
       "   {'accuracy': 0.4550131517870958,\n",
       "    'f1_macro': 0.1698977033781655,\n",
       "    'f1_weighted': 0.4224801220278515,\n",
       "    'loss': 1.9933959577897191,\n",
       "    'train_loss': 1.2970313886045781},\n",
       "   {'accuracy': 0.4553612873278663,\n",
       "    'f1_macro': 0.17211115043819958,\n",
       "    'f1_weighted': 0.4250854098086304,\n",
       "    'loss': 1.9946121757712578,\n",
       "    'train_loss': 1.2574155471782311},\n",
       "   {'accuracy': 0.45617360358966424,\n",
       "    'f1_macro': 0.1691309555780945,\n",
       "    'f1_weighted': 0.4217546438976611,\n",
       "    'loss': 1.9893686290090848,\n",
       "    'train_loss': 1.257828726876188},\n",
       "   {'accuracy': 0.4583204394244159,\n",
       "    'f1_macro': 0.17143367102015086,\n",
       "    'f1_weighted': 0.42665580181447205,\n",
       "    'loss': 1.9982383346544474,\n",
       "    'train_loss': 1.2061427966008196},\n",
       "   {'accuracy': 0.4578949404301408,\n",
       "    'f1_macro': 0.178967452407747,\n",
       "    'f1_weighted': 0.4287921034022442,\n",
       "    'loss': 1.9977361304299572,\n",
       "    'train_loss': 1.1771547338080286},\n",
       "   {'accuracy': 0.4593068234565991,\n",
       "    'f1_macro': 0.18222736978740353,\n",
       "    'f1_weighted': 0.43026511848018717,\n",
       "    'loss': 1.9954206705740107,\n",
       "    'train_loss': 1.193709564097921},\n",
       "   {'accuracy': 0.4590360513693331,\n",
       "    'f1_macro': 0.18160329846934048,\n",
       "    'f1_weighted': 0.4315762763860958,\n",
       "    'loss': 2.005250474300887,\n",
       "    'train_loss': 1.1423883964991348},\n",
       "   {'accuracy': 0.4605833204394244,\n",
       "    'f1_macro': 0.1815228869379422,\n",
       "    'f1_weighted': 0.4309289960440311,\n",
       "    'loss': 2.026330827902377,\n",
       "    'train_loss': 1.134605618361358},\n",
       "   {'accuracy': 0.4606993656196813,\n",
       "    'f1_macro': 0.1820919977650958,\n",
       "    'f1_weighted': 0.43165955600227046,\n",
       "    'loss': 2.0222239721999893,\n",
       "    'train_loss': 1.1255085661418138},\n",
       "   {'accuracy': 0.4619178400123782,\n",
       "    'f1_macro': 0.18178778392224826,\n",
       "    'f1_weighted': 0.4315189637010336,\n",
       "    'loss': 2.0248546780525265,\n",
       "    'train_loss': 1.1130728841054007},\n",
       "   {'accuracy': 0.46412269843725823,\n",
       "    'f1_macro': 0.18161715035457532,\n",
       "    'f1_weighted': 0.4354258940452129,\n",
       "    'loss': 2.030406770280688,\n",
       "    'train_loss': 1.0896799799346433},\n",
       "   {'accuracy': 0.4659213987312394,\n",
       "    'f1_macro': 0.18370205259542247,\n",
       "    'f1_weighted': 0.4358330067925183,\n",
       "    'loss': 2.032103030493219,\n",
       "    'train_loss': 1.0744198371437934},\n",
       "   {'accuracy': 0.4645868791582856,\n",
       "    'f1_macro': 0.18298990761691014,\n",
       "    'f1_weighted': 0.43514279883388984,\n",
       "    'loss': 2.0309916064131652,\n",
       "    'train_loss': 1.0482667198766509},\n",
       "   {'accuracy': 0.46644360204239516,\n",
       "    'f1_macro': 0.18447916817692242,\n",
       "    'f1_weighted': 0.437454058843667,\n",
       "    'loss': 2.0331273469986035,\n",
       "    'train_loss': 1.0568415483075129},\n",
       "   {'accuracy': 0.46729460003094536,\n",
       "    'f1_macro': 0.18552137156677292,\n",
       "    'f1_weighted': 0.4395172405961853,\n",
       "    'loss': 2.0503096213489633,\n",
       "    'train_loss': 1.0150999037502204},\n",
       "   {'accuracy': 0.46690778276342254,\n",
       "    'f1_macro': 0.1827507853973305,\n",
       "    'f1_weighted': 0.4382652899910129,\n",
       "    'loss': 2.0382954226730385,\n",
       "    'train_loss': 1.0224658815520102},\n",
       "   {'accuracy': 0.4692867089586879,\n",
       "    'f1_macro': 0.18464811711208523,\n",
       "    'f1_weighted': 0.43945701200128856,\n",
       "    'loss': 2.0570394863080135,\n",
       "    'train_loss': 1.0016865217199302},\n",
       "   {'accuracy': 0.47033111558099955,\n",
       "    'f1_macro': 0.1864689949034963,\n",
       "    'f1_weighted': 0.44381626832404264,\n",
       "    'loss': 2.0530471755880484,\n",
       "    'train_loss': 0.98069137147859},\n",
       "   {'accuracy': 0.4720524524214761,\n",
       "    'f1_macro': 0.18947446594622908,\n",
       "    'f1_weighted': 0.4450652352706424,\n",
       "    'loss': 2.0478419349536776,\n",
       "    'train_loss': 0.9985947975942959},\n",
       "   {'accuracy': 0.4721491567383568,\n",
       "    'f1_macro': 0.18933423600834115,\n",
       "    'f1_weighted': 0.44770878611724224,\n",
       "    'loss': 2.0543902124830584,\n",
       "    'train_loss': 0.943887535915333},\n",
       "   {'accuracy': 0.4719750889679715,\n",
       "    'f1_macro': 0.1900056399098191,\n",
       "    'f1_weighted': 0.44383362855539926,\n",
       "    'loss': 2.063784583423586,\n",
       "    'train_loss': 0.9917505341479529},\n",
       "   {'accuracy': 0.47476017329413583,\n",
       "    'f1_macro': 0.1897469475922506,\n",
       "    'f1_weighted': 0.4484687050006096,\n",
       "    'loss': 2.068937101615378,\n",
       "    'train_loss': 0.9447496513177592},\n",
       "   {'accuracy': 0.4725359740058796,\n",
       "    'f1_macro': 0.19271450990454164,\n",
       "    'f1_weighted': 0.4474974267883803,\n",
       "    'loss': 2.042702988166787,\n",
       "    'train_loss': 0.986078454303281},\n",
       "   {'accuracy': 0.4746247872505029,\n",
       "    'f1_macro': 0.19457816583755735,\n",
       "    'f1_weighted': 0.44819734177361914,\n",
       "    'loss': 2.049206137165782,\n",
       "    'train_loss': 0.9639825433962622},\n",
       "   {'accuracy': 0.4761720563205942,\n",
       "    'f1_macro': 0.19626885606440003,\n",
       "    'f1_weighted': 0.45004629540692015,\n",
       "    'loss': 2.052913122543693,\n",
       "    'train_loss': 0.9425309443438069},\n",
       "   {'accuracy': 0.47603667027696117,\n",
       "    'f1_macro': 0.19634997889540803,\n",
       "    'f1_weighted': 0.4496392080231657,\n",
       "    'loss': 2.0595491724153634,\n",
       "    'train_loss': 0.9328499982853333},\n",
       "   {'accuracy': 0.4765782144514931,\n",
       "    'f1_macro': 0.19717125730738214,\n",
       "    'f1_weighted': 0.4504094976233363,\n",
       "    'loss': 2.0577544146591347,\n",
       "    'train_loss': 0.9011798955645721},\n",
       "   {'accuracy': 0.47684898653875907,\n",
       "    'f1_macro': 0.19782717473293962,\n",
       "    'f1_weighted': 0.45156079404308314,\n",
       "    'loss': 2.0545264010185194,\n",
       "    'train_loss': 0.9306444063062648},\n",
       "   {'accuracy': 0.4776999845273093,\n",
       "    'f1_macro': 0.1962573422066413,\n",
       "    'f1_weighted': 0.45159811582730297,\n",
       "    'loss': 2.038803823891714,\n",
       "    'train_loss': 0.9529736528959528},\n",
       "   {'accuracy': 0.47963407086492343,\n",
       "    'f1_macro': 0.2026233021774114,\n",
       "    'f1_weighted': 0.45529167166997203,\n",
       "    'loss': 2.0561740883663076,\n",
       "    'train_loss': 0.8897828809240057},\n",
       "   {'accuracy': 0.47829955129196966,\n",
       "    'f1_macro': 0.20390148635522182,\n",
       "    'f1_weighted': 0.45261299284562345,\n",
       "    'loss': 2.0541610816960807,\n",
       "    'train_loss': 0.9091063706000415},\n",
       "   {'accuracy': 0.4801369333127031,\n",
       "    'f1_macro': 0.20468802951652634,\n",
       "    'f1_weighted': 0.45551675889401905,\n",
       "    'loss': 2.0580422693875615,\n",
       "    'train_loss': 0.8666354176287513},\n",
       "   {'accuracy': 0.4798274794986848,\n",
       "    'f1_macro': 0.20302592908357497,\n",
       "    'f1_weighted': 0.4534648782257164,\n",
       "    'loss': 2.054166200084855,\n",
       "    'train_loss': 0.9164528134889546},\n",
       "   {'accuracy': 0.4815101346124091,\n",
       "    'f1_macro': 0.19969638192878575,\n",
       "    'f1_weighted': 0.45593218354862297,\n",
       "    'loss': 2.068959765144888,\n",
       "    'train_loss': 0.8743351312476001},\n",
       "   {'accuracy': 0.48181958842642736,\n",
       "    'f1_macro': 0.2033688938906018,\n",
       "    'f1_weighted': 0.45519343608910456,\n",
       "    'loss': 2.0627066966233554,\n",
       "    'train_loss': 0.8874371030515918},\n",
       "   {'accuracy': 0.4817422249729228,\n",
       "    'f1_macro': 0.20421322781875983,\n",
       "    'f1_weighted': 0.4544961127633075,\n",
       "    'loss': 2.049703128309351,\n",
       "    'train_loss': 0.915873171730829},\n",
       "   {'accuracy': 0.4833088349063902,\n",
       "    'f1_macro': 0.20785451126379526,\n",
       "    'f1_weighted': 0.4583294932356829,\n",
       "    'loss': 2.0418880312987,\n",
       "    'train_loss': 0.9000608530582902},\n",
       "   {'accuracy': 0.4833281757697664,\n",
       "    'f1_macro': 0.21061416246113782,\n",
       "    'f1_weighted': 0.46057159099360384,\n",
       "    'loss': 2.051416554578737,\n",
       "    'train_loss': 0.8909077486979232},\n",
       "   {'accuracy': 0.4829606993656197,\n",
       "    'f1_macro': 0.21046087769579483,\n",
       "    'f1_weighted': 0.45991243858905967,\n",
       "    'loss': 2.05271058351017,\n",
       "    'train_loss': 0.8667137795055735}],\n",
       "  'hyperparams': {'C': 0.07,\n",
       "   'E': 5,\n",
       "   'K': 143,\n",
       "   'batch_size': 10,\n",
       "   'lr': 0.8,\n",
       "   'q': 0.001,\n",
       "   'rounds': 50,\n",
       "   'sampling': 'weighted'},\n",
       "  'test_accuracy': [[0.292898034968281,\n",
       "    0.36830806127185517,\n",
       "    0.3950371344576822,\n",
       "    0.4103164165248337,\n",
       "    0.42310072721646297,\n",
       "    0.4332353396255609,\n",
       "    0.4385153953272474,\n",
       "    0.4435440198050441,\n",
       "    0.44592294600030946,\n",
       "    0.4488627572334829,\n",
       "    0.44979111867553767,\n",
       "    0.45147377378926196,\n",
       "    0.45545799164474704,\n",
       "    0.45772087265975553,\n",
       "    0.45481974315333434,\n",
       "    0.45692789726133376,\n",
       "    0.45692789726133376,\n",
       "    0.46050595698591984,\n",
       "    0.46404533498375367,\n",
       "    0.46294290577131364,\n",
       "    0.465051059879313,\n",
       "    0.4631556552684512,\n",
       "    0.4639679715302491,\n",
       "    0.46632755686213834,\n",
       "    0.4656506266439734,\n",
       "    0.46694646449017485,\n",
       "    0.46855175615039457,\n",
       "    0.46959616277270616,\n",
       "    0.47019572953736655,\n",
       "    0.47160761256382483,\n",
       "    0.4725746557326319,\n",
       "    0.47071793284852237,\n",
       "    0.4702730929908711,\n",
       "    0.47321290422404455,\n",
       "    0.4733289494043014,\n",
       "    0.4733289494043014,\n",
       "    0.4723425653721182,\n",
       "    0.4751083088349064,\n",
       "    0.4778547114343184,\n",
       "    0.4781061426582083,\n",
       "    0.4771584403527773,\n",
       "    0.47706173603589663,\n",
       "    0.47578523905307135,\n",
       "    0.47684898653875907,\n",
       "    0.47880241373974936,\n",
       "    0.4780868017948321,\n",
       "    0.47978879777193256,\n",
       "    0.480678477487235,\n",
       "    0.48108463561813397,\n",
       "    0.4817615658362989],\n",
       "   [0.2923178090669967,\n",
       "    0.37169271236267987,\n",
       "    0.4005879622466347,\n",
       "    0.41958069008200527,\n",
       "    0.42936716695033267,\n",
       "    0.43872814482438494,\n",
       "    0.44431765434008974,\n",
       "    0.44634844499458454,\n",
       "    0.4503133219866935,\n",
       "    0.4504487080303265,\n",
       "    0.45400742689153645,\n",
       "    0.4550131517870958,\n",
       "    0.4553612873278663,\n",
       "    0.45617360358966424,\n",
       "    0.4583204394244159,\n",
       "    0.4578949404301408,\n",
       "    0.4593068234565991,\n",
       "    0.4590360513693331,\n",
       "    0.4605833204394244,\n",
       "    0.4606993656196813,\n",
       "    0.4619178400123782,\n",
       "    0.46412269843725823,\n",
       "    0.4659213987312394,\n",
       "    0.4645868791582856,\n",
       "    0.46644360204239516,\n",
       "    0.46729460003094536,\n",
       "    0.46690778276342254,\n",
       "    0.4692867089586879,\n",
       "    0.47033111558099955,\n",
       "    0.4720524524214761,\n",
       "    0.4721491567383568,\n",
       "    0.4719750889679715,\n",
       "    0.47476017329413583,\n",
       "    0.4725359740058796,\n",
       "    0.4746247872505029,\n",
       "    0.4761720563205942,\n",
       "    0.47603667027696117,\n",
       "    0.4765782144514931,\n",
       "    0.47684898653875907,\n",
       "    0.4776999845273093,\n",
       "    0.47963407086492343,\n",
       "    0.47829955129196966,\n",
       "    0.4801369333127031,\n",
       "    0.4798274794986848,\n",
       "    0.4815101346124091,\n",
       "    0.48181958842642736,\n",
       "    0.4817422249729228,\n",
       "    0.4833088349063902,\n",
       "    0.4833281757697664,\n",
       "    0.4829606993656197]],\n",
       "  'test_loss': [[2.5581289257046684,\n",
       "    2.218436333127625,\n",
       "    2.1158172151887404,\n",
       "    2.0784430108398415,\n",
       "    2.061665152546993,\n",
       "    2.0540060047440676,\n",
       "    2.0452000739341956,\n",
       "    2.025749674418939,\n",
       "    2.0264591153745086,\n",
       "    2.0218635041880693,\n",
       "    1.9998738174141548,\n",
       "    2.0125170246948407,\n",
       "    2.0147720201900823,\n",
       "    2.0251577232280433,\n",
       "    2.022848348061432,\n",
       "    2.0207681652827234,\n",
       "    2.0089272870522588,\n",
       "    2.014824871934004,\n",
       "    2.0304467742456045,\n",
       "    2.0351871681677776,\n",
       "    2.03332470583648,\n",
       "    2.0302479388879475,\n",
       "    2.0270872704391527,\n",
       "    2.036471764617712,\n",
       "    2.038291541599892,\n",
       "    2.0373060801661595,\n",
       "    2.040015296947812,\n",
       "    2.060084943463609,\n",
       "    2.0711203245903245,\n",
       "    2.0459274614834877,\n",
       "    2.027106110527711,\n",
       "    2.0288039445521147,\n",
       "    2.047516053423828,\n",
       "    2.0457025629325565,\n",
       "    2.057361962107209,\n",
       "    2.0655207055129536,\n",
       "    2.053477464741151,\n",
       "    2.056795124654304,\n",
       "    2.060399648482756,\n",
       "    2.0537710090639534,\n",
       "    2.0514414620116654,\n",
       "    2.0721149106815693,\n",
       "    2.0646883833631913,\n",
       "    2.051091642652703,\n",
       "    2.076462191652634,\n",
       "    2.0659302856017185,\n",
       "    2.0656829666646472,\n",
       "    2.05508699122597,\n",
       "    2.0680684652749757,\n",
       "    2.0597170653694117],\n",
       "   [2.652704846212485,\n",
       "    2.2233643219319914,\n",
       "    2.1176259331318703,\n",
       "    2.0671037282554354,\n",
       "    2.03810870114892,\n",
       "    2.024138481455201,\n",
       "    2.003778269855384,\n",
       "    2.0106139550016944,\n",
       "    2.0016165157846624,\n",
       "    2.0066052218845813,\n",
       "    1.9923861056857899,\n",
       "    1.9933959577897191,\n",
       "    1.9946121757712578,\n",
       "    1.9893686290090848,\n",
       "    1.9982383346544474,\n",
       "    1.9977361304299572,\n",
       "    1.9954206705740107,\n",
       "    2.005250474300887,\n",
       "    2.026330827902377,\n",
       "    2.0222239721999893,\n",
       "    2.0248546780525265,\n",
       "    2.030406770280688,\n",
       "    2.032103030493219,\n",
       "    2.0309916064131652,\n",
       "    2.0331273469986035,\n",
       "    2.0503096213489633,\n",
       "    2.0382954226730385,\n",
       "    2.0570394863080135,\n",
       "    2.0530471755880484,\n",
       "    2.0478419349536776,\n",
       "    2.0543902124830584,\n",
       "    2.063784583423586,\n",
       "    2.068937101615378,\n",
       "    2.042702988166787,\n",
       "    2.049206137165782,\n",
       "    2.052913122543693,\n",
       "    2.0595491724153634,\n",
       "    2.0577544146591347,\n",
       "    2.0545264010185194,\n",
       "    2.038803823891714,\n",
       "    2.0561740883663076,\n",
       "    2.0541610816960807,\n",
       "    2.0580422693875615,\n",
       "    2.054166200084855,\n",
       "    2.068959765144888,\n",
       "    2.0627066966233554,\n",
       "    2.049703128309351,\n",
       "    2.0418880312987,\n",
       "    2.051416554578737,\n",
       "    2.05271058351017]],\n",
       "  'train_loss': [[2.699278122255499,\n",
       "    2.305539687183347,\n",
       "    2.0556716528588472,\n",
       "    1.836080676275299,\n",
       "    1.7066959865109925,\n",
       "    1.6085174911077371,\n",
       "    1.5368751986635485,\n",
       "    1.4835905670391636,\n",
       "    1.4110441532901605,\n",
       "    1.3661474153779019,\n",
       "    1.3629961555627372,\n",
       "    1.2978580726299274,\n",
       "    1.258816942323655,\n",
       "    1.2115225147623776,\n",
       "    1.232719370109782,\n",
       "    1.2118411955386323,\n",
       "    1.201797117747226,\n",
       "    1.151738797380822,\n",
       "    1.1245534179475059,\n",
       "    1.1088995633534735,\n",
       "    1.1299371335178967,\n",
       "    1.080827967802267,\n",
       "    1.0709726753488424,\n",
       "    1.0768747998686106,\n",
       "    1.0750583447934545,\n",
       "    1.0667200142972884,\n",
       "    1.0099510159209077,\n",
       "    1.0145082815270918,\n",
       "    0.9884072208348378,\n",
       "    1.009362868215624,\n",
       "    1.0298660734896972,\n",
       "    0.9751431722384826,\n",
       "    0.9608616843161139,\n",
       "    0.9806603684182825,\n",
       "    0.9439412155977601,\n",
       "    0.9219187544765353,\n",
       "    0.9469983087545183,\n",
       "    0.9679912499015602,\n",
       "    0.933732788902469,\n",
       "    0.9568019603015753,\n",
       "    0.9305979289940236,\n",
       "    0.903573073921519,\n",
       "    0.9184320148712904,\n",
       "    0.907468166867672,\n",
       "    0.8977235820680507,\n",
       "    0.9077632588001784,\n",
       "    0.9200848911234741,\n",
       "    0.898993781311064,\n",
       "    0.8935516474068933,\n",
       "    0.9052526393851823],\n",
       "   [2.7272956011350553,\n",
       "    2.3028240809854466,\n",
       "    2.045692445731736,\n",
       "    1.836873912240223,\n",
       "    1.6999859041227015,\n",
       "    1.6067341897176657,\n",
       "    1.514981447570335,\n",
       "    1.4513321400982073,\n",
       "    1.3845416207666437,\n",
       "    1.3457389915995508,\n",
       "    1.320808141418265,\n",
       "    1.2970313886045781,\n",
       "    1.2574155471782311,\n",
       "    1.257828726876188,\n",
       "    1.2061427966008196,\n",
       "    1.1771547338080286,\n",
       "    1.193709564097921,\n",
       "    1.1423883964991348,\n",
       "    1.134605618361358,\n",
       "    1.1255085661418138,\n",
       "    1.1130728841054007,\n",
       "    1.0896799799346433,\n",
       "    1.0744198371437934,\n",
       "    1.0482667198766509,\n",
       "    1.0568415483075129,\n",
       "    1.0150999037502204,\n",
       "    1.0224658815520102,\n",
       "    1.0016865217199302,\n",
       "    0.98069137147859,\n",
       "    0.9985947975942959,\n",
       "    0.943887535915333,\n",
       "    0.9917505341479529,\n",
       "    0.9447496513177592,\n",
       "    0.986078454303281,\n",
       "    0.9639825433962622,\n",
       "    0.9425309443438069,\n",
       "    0.9328499982853333,\n",
       "    0.9011798955645721,\n",
       "    0.9306444063062648,\n",
       "    0.9529736528959528,\n",
       "    0.8897828809240057,\n",
       "    0.9091063706000415,\n",
       "    0.8666354176287513,\n",
       "    0.9164528134889546,\n",
       "    0.8743351312476001,\n",
       "    0.8874371030515918,\n",
       "    0.915873171730829,\n",
       "    0.9000608530582902,\n",
       "    0.8909077486979232,\n",
       "    0.8667137795055735]]},\n",
       " 'Shakespeare LSTM on Non IID': {'history': [{'accuracy': 0.25678170028122194,\n",
       "    'f1_macro': 0.015760177220794758,\n",
       "    'f1_weighted': 0.13388183999019063,\n",
       "    'loss': 2.835100895392369,\n",
       "    'train_loss': 2.4720839457969075},\n",
       "   {'accuracy': 0.3060100312146661,\n",
       "    'f1_macro': 0.030911415338816917,\n",
       "    'f1_weighted': 0.20043523859257492,\n",
       "    'loss': 2.5595373699309483,\n",
       "    'train_loss': 2.0694941413428234},\n",
       "   {'accuracy': 0.3988905747170869,\n",
       "    'f1_macro': 0.08599806206898258,\n",
       "    'f1_weighted': 0.3377245088189919,\n",
       "    'loss': 2.151853678319256,\n",
       "    'train_loss': 1.8046531148387424},\n",
       "   {'accuracy': 0.4070179846729225,\n",
       "    'f1_macro': 0.09082322830063834,\n",
       "    'f1_weighted': 0.34749278997413513,\n",
       "    'loss': 2.1579347416913053,\n",
       "    'train_loss': 1.7666795499923615},\n",
       "   {'accuracy': 0.4263169593242943,\n",
       "    'f1_macro': 0.10796686687943387,\n",
       "    'f1_weighted': 0.37001780665329026,\n",
       "    'loss': 2.020480688699534,\n",
       "    'train_loss': 1.6705646896203727},\n",
       "   {'accuracy': 0.44964581501203166,\n",
       "    'f1_macro': 0.13395495502199645,\n",
       "    'f1_weighted': 0.4043259560716532,\n",
       "    'loss': 1.9309818027309285,\n",
       "    'train_loss': 1.5348616719637473},\n",
       "   {'accuracy': 0.4481382336171323,\n",
       "    'f1_macro': 0.12896366329209047,\n",
       "    'f1_weighted': 0.40026393029921037,\n",
       "    'loss': 1.9240031803781072,\n",
       "    'train_loss': 1.479665440467944},\n",
       "   {'accuracy': 0.44880504846487623,\n",
       "    'f1_macro': 0.13512910268589112,\n",
       "    'f1_weighted': 0.3965901219988532,\n",
       "    'loss': 1.906328129956384,\n",
       "    'train_loss': 1.4902997136900795},\n",
       "   {'accuracy': 0.4568454825710061,\n",
       "    'f1_macro': 0.1411645908052693,\n",
       "    'f1_weighted': 0.408463357709528,\n",
       "    'loss': 1.8840588016993036,\n",
       "    'train_loss': 1.4609142578872107},\n",
       "   {'accuracy': 0.4638132145307653,\n",
       "    'f1_macro': 0.14750570622684167,\n",
       "    'f1_weighted': 0.42240802579728887,\n",
       "    'loss': 1.841513384779834,\n",
       "    'train_loss': 1.4365884860160758},\n",
       "   {'accuracy': 0.4755066343245359,\n",
       "    'f1_macro': 0.16594110164019965,\n",
       "    'f1_weighted': 0.4369739322944612,\n",
       "    'loss': 1.7892491360807063,\n",
       "    'train_loss': 1.3396467853739733},\n",
       "   {'accuracy': 0.47592218560646327,\n",
       "    'f1_macro': 0.1699248846033881,\n",
       "    'f1_weighted': 0.4379377986118908,\n",
       "    'loss': 1.7817795452246783,\n",
       "    'train_loss': 1.4015387400121508},\n",
       "   {'accuracy': 0.4844264909110237,\n",
       "    'f1_macro': 0.1767355167935505,\n",
       "    'f1_weighted': 0.45134567132954023,\n",
       "    'loss': 1.7596594240121743,\n",
       "    'train_loss': 1.2440602999475876},\n",
       "   {'accuracy': 0.4863399596045498,\n",
       "    'f1_macro': 0.1833404492390102,\n",
       "    'f1_weighted': 0.4515405345605937,\n",
       "    'loss': 1.7531939272671944,\n",
       "    'train_loss': 1.2691409179747077},\n",
       "   {'accuracy': 0.48634962358785044,\n",
       "    'f1_macro': 0.17903474104125938,\n",
       "    'f1_weighted': 0.45266233653827376,\n",
       "    'loss': 1.7511285689600653,\n",
       "    'train_loss': 1.33504066663199},\n",
       "   {'accuracy': 0.48692946258588865,\n",
       "    'f1_macro': 0.18192614504733015,\n",
       "    'f1_weighted': 0.45311989987453977,\n",
       "    'loss': 1.748277168008174,\n",
       "    'train_loss': 1.3303116533607142},\n",
       "   {'accuracy': 0.4916068305033969,\n",
       "    'f1_macro': 0.1869827397072604,\n",
       "    'f1_weighted': 0.4584187167802229,\n",
       "    'loss': 1.7333067947846557,\n",
       "    'train_loss': 1.3389881122590226},\n",
       "   {'accuracy': 0.4986615383128618,\n",
       "    'f1_macro': 0.1998671774413297,\n",
       "    'f1_weighted': 0.47260327899675575,\n",
       "    'loss': 1.7471943255885618,\n",
       "    'train_loss': 1.118338143944324},\n",
       "   {'accuracy': 0.4946993051596007,\n",
       "    'f1_macro': 0.1935360855152219,\n",
       "    'f1_weighted': 0.4623565390592523,\n",
       "    'loss': 1.7223140378254889,\n",
       "    'train_loss': 1.4223757158201213},\n",
       "   {'accuracy': 0.49714429293466184,\n",
       "    'f1_macro': 0.1937968280791713,\n",
       "    'f1_weighted': 0.46592345690516784,\n",
       "    'loss': 1.7215323925423827,\n",
       "    'train_loss': 1.2746711492319676},\n",
       "   {'accuracy': 0.49710563700145927,\n",
       "    'f1_macro': 0.19478522040023072,\n",
       "    'f1_weighted': 0.4645234526654254,\n",
       "    'loss': 1.7158357487510818,\n",
       "    'train_loss': 1.327280683444489},\n",
       "   {'accuracy': 0.499028769678286,\n",
       "    'f1_macro': 0.19695568011828224,\n",
       "    'f1_weighted': 0.4696269406494433,\n",
       "    'loss': 1.7166542162661524,\n",
       "    'train_loss': 1.2576751026198985},\n",
       "   {'accuracy': 0.4960039429051867,\n",
       "    'f1_macro': 0.19294417741933723,\n",
       "    'f1_weighted': 0.4637588800513779,\n",
       "    'loss': 1.7125592266576555,\n",
       "    'train_loss': 1.4387497037013468},\n",
       "   {'accuracy': 0.5024304918001101,\n",
       "    'f1_macro': 0.20351345085079728,\n",
       "    'f1_weighted': 0.4747729603128336,\n",
       "    'loss': 1.706575712024787,\n",
       "    'train_loss': 1.1792162442617933},\n",
       "   {'accuracy': 0.5032132744474618,\n",
       "    'f1_macro': 0.20366910572900368,\n",
       "    'f1_weighted': 0.47536853741347646,\n",
       "    'loss': 1.7020504322075687,\n",
       "    'train_loss': 1.1872249243604966},\n",
       "   {'accuracy': 0.5059288537549407,\n",
       "    'f1_macro': 0.20870464784723852,\n",
       "    'f1_weighted': 0.48088844900252037,\n",
       "    'loss': 1.7101729214856738,\n",
       "    'train_loss': 1.197572072716658},\n",
       "   {'accuracy': 0.5093692318099674,\n",
       "    'f1_macro': 0.20860935564283042,\n",
       "    'f1_weighted': 0.484872944726887,\n",
       "    'loss': 1.7214090039417478,\n",
       "    'train_loss': 1.1664243481065737},\n",
       "   {'accuracy': 0.5100553746243126,\n",
       "    'f1_macro': 0.2120913403323476,\n",
       "    'f1_weighted': 0.4887322117179695,\n",
       "    'loss': 1.7500856998240328,\n",
       "    'train_loss': 1.0743371020739416},\n",
       "   {'accuracy': 0.5096011674091827,\n",
       "    'f1_macro': 0.20967193366799508,\n",
       "    'f1_weighted': 0.48720496447740974,\n",
       "    'loss': 1.728980741978535,\n",
       "    'train_loss': 1.1163843024312925},\n",
       "   {'accuracy': 0.5101036945408158,\n",
       "    'f1_macro': 0.21830981648775027,\n",
       "    'f1_weighted': 0.48816241388888215,\n",
       "    'loss': 1.7297078282306595,\n",
       "    'train_loss': 1.0678612359820008},\n",
       "   {'accuracy': 0.5096784792755878,\n",
       "    'f1_macro': 0.21062415848368046,\n",
       "    'f1_weighted': 0.4831878821212187,\n",
       "    'loss': 1.692271125017436,\n",
       "    'train_loss': 1.3106648938152916},\n",
       "   {'accuracy': 0.5111570687205853,\n",
       "    'f1_macro': 0.21356396186272694,\n",
       "    'f1_weighted': 0.4898393252767404,\n",
       "    'loss': 1.733603745508555,\n",
       "    'train_loss': 1.019785893488449},\n",
       "   {'accuracy': 0.5099587347913063,\n",
       "    'f1_macro': 0.21997334619032288,\n",
       "    'f1_weighted': 0.48972629803372014,\n",
       "    'loss': 1.736409960429061,\n",
       "    'train_loss': 1.0597298995927562},\n",
       "   {'accuracy': 0.5113213564366961,\n",
       "    'f1_macro': 0.22067885406762788,\n",
       "    'f1_weighted': 0.48835087533560934,\n",
       "    'loss': 1.7200815295327962,\n",
       "    'train_loss': 1.1611195324784935},\n",
       "   {'accuracy': 0.5131091933473139,\n",
       "    'f1_macro': 0.2222760913734056,\n",
       "    'f1_weighted': 0.489709546265276,\n",
       "    'loss': 1.721634705247758,\n",
       "    'train_loss': 1.0755624870298814},\n",
       "   {'accuracy': 0.5128192738482948,\n",
       "    'f1_macro': 0.21961169387959842,\n",
       "    'f1_weighted': 0.48985994001904853,\n",
       "    'loss': 1.695401035056339,\n",
       "    'train_loss': 1.1627158508784454},\n",
       "   {'accuracy': 0.5128289378315954,\n",
       "    'f1_macro': 0.2194820340997234,\n",
       "    'f1_weighted': 0.4871063737411946,\n",
       "    'loss': 1.686290134429904,\n",
       "    'train_loss': 1.1673176948990822},\n",
       "   {'accuracy': 0.5140852556606782,\n",
       "    'f1_macro': 0.22061838003045278,\n",
       "    'f1_weighted': 0.49085661029032024,\n",
       "    'loss': 1.6928928323093748,\n",
       "    'train_loss': 1.1843918943075396},\n",
       "   {'accuracy': 0.5138050001449598,\n",
       "    'f1_macro': 0.22506103553315435,\n",
       "    'f1_weighted': 0.4912643105622097,\n",
       "    'loss': 1.7034677176282582,\n",
       "    'train_loss': 1.1578789612808167},\n",
       "   {'accuracy': 0.5140272717608744,\n",
       "    'f1_macro': 0.22371892908950516,\n",
       "    'f1_weighted': 0.49225110800321786,\n",
       "    'loss': 1.720480468335442,\n",
       "    'train_loss': 1.0769512864746198},\n",
       "   {'accuracy': 0.5133314649632286,\n",
       "    'f1_macro': 0.22431863415228204,\n",
       "    'f1_weighted': 0.4890686414093986,\n",
       "    'loss': 1.6842077019590724,\n",
       "    'train_loss': 1.2534472685072657},\n",
       "   {'accuracy': 0.5152835895899572,\n",
       "    'f1_macro': 0.22473296779679167,\n",
       "    'f1_weighted': 0.49071199724852177,\n",
       "    'loss': 1.6833842554755243,\n",
       "    'train_loss': 1.2229227222556758},\n",
       "   {'accuracy': 0.5158054446881916,\n",
       "    'f1_macro': 0.22257316685679562,\n",
       "    'f1_weighted': 0.49126542344625185,\n",
       "    'loss': 1.6817360966921036,\n",
       "    'train_loss': 1.2031809796559516},\n",
       "   {'accuracy': 0.5162983078365241,\n",
       "    'f1_macro': 0.22577418028338747,\n",
       "    'f1_weighted': 0.4918100078899864,\n",
       "    'loss': 1.6898513225039695,\n",
       "    'train_loss': 1.2065992048500906},\n",
       "   {'accuracy': 0.5168104989514578,\n",
       "    'f1_macro': 0.2279719906519521,\n",
       "    'f1_weighted': 0.4943871170865493,\n",
       "    'loss': 1.69817352506551,\n",
       "    'train_loss': 1.165711988856288},\n",
       "   {'accuracy': 0.5171873943001827,\n",
       "    'f1_macro': 0.22870229109241513,\n",
       "    'f1_weighted': 0.4945076079948006,\n",
       "    'loss': 1.7055647197318038,\n",
       "    'train_loss': 1.1363358983460987},\n",
       "   {'accuracy': 0.5178638731312273,\n",
       "    'f1_macro': 0.22968431821749824,\n",
       "    'f1_weighted': 0.49529122580971446,\n",
       "    'loss': 1.7030195353305966,\n",
       "    'train_loss': 1.1214295715381384},\n",
       "   {'accuracy': 0.5173516820162934,\n",
       "    'f1_macro': 0.2269938449927393,\n",
       "    'f1_weighted': 0.4970226481825146,\n",
       "    'loss': 1.689093002994033,\n",
       "    'train_loss': 1.2491395954669007},\n",
       "   {'accuracy': 0.5190718710438068,\n",
       "    'f1_macro': 0.2333787754821906,\n",
       "    'f1_weighted': 0.49907697058334427,\n",
       "    'loss': 1.691829698106775,\n",
       "    'train_loss': 1.1147915476814814},\n",
       "   {'accuracy': 0.5195260782589368,\n",
       "    'f1_macro': 0.23232320514227864,\n",
       "    'f1_weighted': 0.4990656918322006,\n",
       "    'loss': 1.6949409374101876,\n",
       "    'train_loss': 1.1777974859293656}],\n",
       "  'hyperparams': {'C': 0.07,\n",
       "   'E': 5,\n",
       "   'K': 143,\n",
       "   'batch_size': 10,\n",
       "   'lr': 0.8,\n",
       "   'q': 0.001,\n",
       "   'rounds': 50,\n",
       "   'sampling': 'weighted'},\n",
       "  'test_accuracy': [[0.24838369879296848,\n",
       "    0.33031494921576776,\n",
       "    0.40683436899021036,\n",
       "    0.41301931830261795,\n",
       "    0.4338741942653923,\n",
       "    0.45620765967316407,\n",
       "    0.4631947195995245,\n",
       "    0.4606627559747577,\n",
       "    0.46629685823902894,\n",
       "    0.47277172705045567,\n",
       "    0.46451868531171175,\n",
       "    0.4722885278854238,\n",
       "    0.47411502072924416,\n",
       "    0.4809281289561932,\n",
       "    0.4809667848893957,\n",
       "    0.49041816055741855,\n",
       "    0.489741681726374,\n",
       "    0.4915875025367956,\n",
       "    0.49506653652502486,\n",
       "    0.4961102467214937,\n",
       "    0.495994278921886,\n",
       "    0.4963518463040096,\n",
       "    0.5006523188727929,\n",
       "    0.5028653710486388,\n",
       "    0.5033775621635725,\n",
       "    0.5028750350319394,\n",
       "    0.5019472926350783,\n",
       "    0.5051943910240924,\n",
       "    0.5050880872077853,\n",
       "    0.5074750910830426,\n",
       "    0.5066053325859853,\n",
       "    0.5085671211960145,\n",
       "    0.5079389622814732,\n",
       "    0.5115339640693101,\n",
       "    0.5127226340152884,\n",
       "    0.5126066662156807,\n",
       "    0.5126356581655827,\n",
       "    0.5142592073600897,\n",
       "    0.5147230785585203,\n",
       "    0.5142688713433903,\n",
       "    0.5157571247716884,\n",
       "    0.51456845482571,\n",
       "    0.5168104989514578,\n",
       "    0.5176222735487113,\n",
       "    0.5175159697324043,\n",
       "    0.5181054727137432,\n",
       "    0.5192361587599177,\n",
       "    0.5202991969229878,\n",
       "    0.5195743981754399,\n",
       "    0.5199899494573673],\n",
       "   [0.25678170028122194,\n",
       "    0.3060100312146661,\n",
       "    0.3988905747170869,\n",
       "    0.4070179846729225,\n",
       "    0.4263169593242943,\n",
       "    0.44964581501203166,\n",
       "    0.4481382336171323,\n",
       "    0.44880504846487623,\n",
       "    0.4568454825710061,\n",
       "    0.4638132145307653,\n",
       "    0.4755066343245359,\n",
       "    0.47592218560646327,\n",
       "    0.4844264909110237,\n",
       "    0.4863399596045498,\n",
       "    0.48634962358785044,\n",
       "    0.48692946258588865,\n",
       "    0.4916068305033969,\n",
       "    0.4986615383128618,\n",
       "    0.4946993051596007,\n",
       "    0.49714429293466184,\n",
       "    0.49710563700145927,\n",
       "    0.499028769678286,\n",
       "    0.4960039429051867,\n",
       "    0.5024304918001101,\n",
       "    0.5032132744474618,\n",
       "    0.5059288537549407,\n",
       "    0.5093692318099674,\n",
       "    0.5100553746243126,\n",
       "    0.5096011674091827,\n",
       "    0.5101036945408158,\n",
       "    0.5096784792755878,\n",
       "    0.5111570687205853,\n",
       "    0.5099587347913063,\n",
       "    0.5113213564366961,\n",
       "    0.5131091933473139,\n",
       "    0.5128192738482948,\n",
       "    0.5128289378315954,\n",
       "    0.5140852556606782,\n",
       "    0.5138050001449598,\n",
       "    0.5140272717608744,\n",
       "    0.5133314649632286,\n",
       "    0.5152835895899572,\n",
       "    0.5158054446881916,\n",
       "    0.5162983078365241,\n",
       "    0.5168104989514578,\n",
       "    0.5171873943001827,\n",
       "    0.5178638731312273,\n",
       "    0.5173516820162934,\n",
       "    0.5190718710438068,\n",
       "    0.5195260782589368]],\n",
       "  'test_loss': [[2.9523252660674717,\n",
       "    2.5775654960858567,\n",
       "    2.0902606468502887,\n",
       "    2.1655671083698014,\n",
       "    2.0102656860371932,\n",
       "    1.8835006896273747,\n",
       "    1.8429608783519902,\n",
       "    1.86169810680899,\n",
       "    1.8206003537603326,\n",
       "    1.8004427940741783,\n",
       "    1.8551308041798489,\n",
       "    1.8176962086693966,\n",
       "    1.819323848725075,\n",
       "    1.7759854831611872,\n",
       "    1.7676884576195717,\n",
       "    1.7404475746169807,\n",
       "    1.736837695315805,\n",
       "    1.732982775826471,\n",
       "    1.7271518244283408,\n",
       "    1.7287475393545086,\n",
       "    1.7236693213641894,\n",
       "    1.7143787070951133,\n",
       "    1.7290639093462123,\n",
       "    1.7185276519303294,\n",
       "    1.7102869570964359,\n",
       "    1.6998170439132994,\n",
       "    1.7032510867970374,\n",
       "    1.7052043110161883,\n",
       "    1.6972136091220338,\n",
       "    1.6918845750307312,\n",
       "    1.6961959208191673,\n",
       "    1.6971319757728618,\n",
       "    1.6905753229235738,\n",
       "    1.6966975744017803,\n",
       "    1.6825193889935273,\n",
       "    1.68331149635747,\n",
       "    1.7029241544731215,\n",
       "    1.6815251258396582,\n",
       "    1.7020008262357622,\n",
       "    1.6825715480166472,\n",
       "    1.7058173287277372,\n",
       "    1.7535836667224782,\n",
       "    1.6932173392898409,\n",
       "    1.6938012882763138,\n",
       "    1.696776861251734,\n",
       "    1.6945206963082926,\n",
       "    1.6933795063035997,\n",
       "    1.6877599632184623,\n",
       "    1.6880560178998274,\n",
       "    1.6859019910544883],\n",
       "   [2.835100895392369,\n",
       "    2.5595373699309483,\n",
       "    2.151853678319256,\n",
       "    2.1579347416913053,\n",
       "    2.020480688699534,\n",
       "    1.9309818027309285,\n",
       "    1.9240031803781072,\n",
       "    1.906328129956384,\n",
       "    1.8840588016993036,\n",
       "    1.841513384779834,\n",
       "    1.7892491360807063,\n",
       "    1.7817795452246783,\n",
       "    1.7596594240121743,\n",
       "    1.7531939272671944,\n",
       "    1.7511285689600653,\n",
       "    1.748277168008174,\n",
       "    1.7333067947846557,\n",
       "    1.7471943255885618,\n",
       "    1.7223140378254889,\n",
       "    1.7215323925423827,\n",
       "    1.7158357487510818,\n",
       "    1.7166542162661524,\n",
       "    1.7125592266576555,\n",
       "    1.706575712024787,\n",
       "    1.7020504322075687,\n",
       "    1.7101729214856738,\n",
       "    1.7214090039417478,\n",
       "    1.7500856998240328,\n",
       "    1.728980741978535,\n",
       "    1.7297078282306595,\n",
       "    1.692271125017436,\n",
       "    1.733603745508555,\n",
       "    1.736409960429061,\n",
       "    1.7200815295327962,\n",
       "    1.721634705247758,\n",
       "    1.695401035056339,\n",
       "    1.686290134429904,\n",
       "    1.6928928323093748,\n",
       "    1.7034677176282582,\n",
       "    1.720480468335442,\n",
       "    1.6842077019590724,\n",
       "    1.6833842554755243,\n",
       "    1.6817360966921036,\n",
       "    1.6898513225039695,\n",
       "    1.69817352506551,\n",
       "    1.7055647197318038,\n",
       "    1.7030195353305966,\n",
       "    1.689093002994033,\n",
       "    1.691829698106775,\n",
       "    1.6949409374101876]],\n",
       "  'train_loss': [[2.2992499400109976,\n",
       "    2.060909304827173,\n",
       "    1.8244189061451432,\n",
       "    1.7389752270919399,\n",
       "    1.6356242687628324,\n",
       "    1.4682112582065363,\n",
       "    1.3149260531007412,\n",
       "    1.4376581595032418,\n",
       "    1.3416200576367145,\n",
       "    1.3045256202453883,\n",
       "    1.5176514173271116,\n",
       "    1.4526384760260278,\n",
       "    1.4579405429998455,\n",
       "    1.37656311489391,\n",
       "    1.3387497932621488,\n",
       "    1.1991992351164806,\n",
       "    1.344178533618423,\n",
       "    1.378095712974837,\n",
       "    1.224246043454919,\n",
       "    1.1776298771674973,\n",
       "    1.2636638270289997,\n",
       "    1.3511181526111886,\n",
       "    1.1465210467475555,\n",
       "    1.229132834978,\n",
       "    1.2526677372737376,\n",
       "    1.2719844711450943,\n",
       "    1.285359835373764,\n",
       "    1.2009418011827475,\n",
       "    1.2678708381745016,\n",
       "    1.284665537700494,\n",
       "    1.2980085924111717,\n",
       "    1.235410509370989,\n",
       "    1.2243099470205991,\n",
       "    1.1560887215909166,\n",
       "    1.2403495906034596,\n",
       "    1.2303547094198222,\n",
       "    1.152906202749644,\n",
       "    1.269305273401476,\n",
       "    1.1706439977578236,\n",
       "    1.2655930238204847,\n",
       "    1.1444824819377613,\n",
       "    1.0322235211778579,\n",
       "    1.146483454011643,\n",
       "    1.1832636136592256,\n",
       "    1.1883721289326554,\n",
       "    1.1650776770078526,\n",
       "    1.1331158342482637,\n",
       "    1.1372271710136679,\n",
       "    1.187172030863246,\n",
       "    1.2027654163684818],\n",
       "   [2.4720839457969075,\n",
       "    2.0694941413428234,\n",
       "    1.8046531148387424,\n",
       "    1.7666795499923615,\n",
       "    1.6705646896203727,\n",
       "    1.5348616719637473,\n",
       "    1.479665440467944,\n",
       "    1.4902997136900795,\n",
       "    1.4609142578872107,\n",
       "    1.4365884860160758,\n",
       "    1.3396467853739733,\n",
       "    1.4015387400121508,\n",
       "    1.2440602999475876,\n",
       "    1.2691409179747077,\n",
       "    1.33504066663199,\n",
       "    1.3303116533607142,\n",
       "    1.3389881122590226,\n",
       "    1.118338143944324,\n",
       "    1.4223757158201213,\n",
       "    1.2746711492319676,\n",
       "    1.327280683444489,\n",
       "    1.2576751026198985,\n",
       "    1.4387497037013468,\n",
       "    1.1792162442617933,\n",
       "    1.1872249243604966,\n",
       "    1.197572072716658,\n",
       "    1.1664243481065737,\n",
       "    1.0743371020739416,\n",
       "    1.1163843024312925,\n",
       "    1.0678612359820008,\n",
       "    1.3106648938152916,\n",
       "    1.019785893488449,\n",
       "    1.0597298995927562,\n",
       "    1.1611195324784935,\n",
       "    1.0755624870298814,\n",
       "    1.1627158508784454,\n",
       "    1.1673176948990822,\n",
       "    1.1843918943075396,\n",
       "    1.1578789612808167,\n",
       "    1.0769512864746198,\n",
       "    1.2534472685072657,\n",
       "    1.2229227222556758,\n",
       "    1.2031809796559516,\n",
       "    1.2065992048500906,\n",
       "    1.165711988856288,\n",
       "    1.1363358983460987,\n",
       "    1.1214295715381384,\n",
       "    1.2491395954669007,\n",
       "    1.1147915476814814,\n",
       "    1.1777974859293656]]}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "aSs9xXfpQ3ML"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(path + 'Local_Round_qFedAvg_5.pkl', 'rb') as file:\n",
    "  log_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "N8ep-MalQ3PO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4817615658362989, 0.4829606993656197]\n"
     ]
    }
   ],
   "source": [
    "print([test_acc[-1] for test_acc in log_dict['Shakespeare LSTM on IID']['test_accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "_-2I2g0HQ3WA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5199899494573673, 0.5195260782589368]\n"
     ]
    }
   ],
   "source": [
    "print([test_acc[-1] for test_acc in log_dict['Shakespeare LSTM on Non IID']['test_accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfR3lDa7Q-8s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "FederatedAveraging",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "QSGD",
   "language": "python",
   "name": "qsgd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
