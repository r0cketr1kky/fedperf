{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tushar-semwal/fedperf/blob/main/Vineeth/Local_Rounds/Shakesphere/FedAvg/FederatedAveraging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WQ6Rq0UiG6ev"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torchsummaryX unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yKcpjZLrQQJV"
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    import os\n",
    "    path = '/content/drive/MyDrive/Colab Notebooks/OpenMined/'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "except:\n",
    "    path = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0_nKpfq2h1R"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DLLNM9X2JbQ8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 26 11:52:23 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 17%   33C    P0    54W / 250W |      0MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "| 18%   32C    P0    57W / 250W |      0MiB / 11178MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "import copy\n",
    "from functools import reduce\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "from torchsummaryX import summary as summaryx\n",
    "from torchvision import transforms, utils, datasets\n",
    "from tqdm.notebook import tqdm\n",
    "from unidecode import unidecode\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Check assigned GPU\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "\n",
    "# set manual seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# general reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# gpu training specific\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY4eWzGiL6Mj"
   },
   "source": [
    "## Load the Shakespeare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "hf03LRxof7Zj"
   },
   "outputs": [],
   "source": [
    "!rm -Rf data\n",
    "!mkdir -p data scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ngygA4-Fgobx"
   },
   "outputs": [],
   "source": [
    "GENERATE_DATASET = False  # If False, download the dataset provided by the q-FFL paper\n",
    "DATA_DIR = 'data'\n",
    "# Dataset generation params\n",
    "SAMPLES_FRACTION = 1.  # If using an already generated dataset\n",
    "# SAMPLES_FRACTION = 0.2  # Fraction of total samples in the dataset - FedProx default script\n",
    "# SAMPLES_FRACTION = 0.05  # Fraction of total samples in the dataset - qFFL\n",
    "TRAIN_FRACTION = 0.8  # Train set size\n",
    "MIN_SAMPLES = 0  # Min samples per client (for filtering purposes) - FedProx\n",
    "# MIN_SAMPLES = 64  # Min samples per client (for filtering purposes) - qFFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nUmwJgJygoYD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-26 11:52:24--  http://www.gutenberg.org/files/100/100-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5757108 (5.5M) [text/plain]\n",
      "Saving to: ‘data/shakespeare.txt’\n",
      "\n",
      "data/shakespeare.tx 100%[===================>]   5.49M  1.49MB/s    in 3.7s    \n",
      "\n",
      "2021-04-26 11:52:28 (1.49 MB/s) - ‘data/shakespeare.txt’ saved [5757108/5757108]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download raw dataset\n",
    "# !wget https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt -O data/shakespeare.txt\n",
    "!wget --adjust-extension http://www.gutenberg.org/files/100/100-0.txt -O data/shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4dCvx80BgoVr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1n46Mftp3_ahRi1Z6jYhEriyLtdRDS1tD\n",
      "To: /home/vineeth/code/OM/Vineeth/Local_Rounds/Shakespeare/FedAvg/01/shakespeare.zip\n",
      "2.96MB [00:00, 5.24MB/s]\n",
      "Archive:  shakespeare.zip\n",
      "   creating: shakespeare_paper/\n",
      "   creating: shakespeare_paper/test/\n",
      "  inflating: shakespeare_paper/test/all_data_niid_2_keep_0_test_8.json  \n",
      "   creating: shakespeare_paper/train/\n",
      "  inflating: shakespeare_paper/train/all_data_niid_2_keep_0_train_8.json  \n"
     ]
    }
   ],
   "source": [
    "if not GENERATE_DATASET:\n",
    "    !rm -Rf data/train data/test\n",
    "    !gdown --id 1n46Mftp3_ahRi1Z6jYhEriyLtdRDS1tD  # Download Shakespeare dataset used by the FedProx paper\n",
    "    !unzip shakespeare.zip\n",
    "    !mv -f shakespeare_paper/train data/\n",
    "    !mv -f shakespeare_paper/test data/\n",
    "    !rm -R shakespeare_paper/ shakespeare.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "a4pzFvPvhQhq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Length: 90\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    data = list(unidecode(f.read()))\n",
    "    corpus = list(set(list(data)))\n",
    "print('Corpus Length:', len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cce_-qnxhD4n"
   },
   "source": [
    "#### Dataset Preprocessing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Rt13M4IcgoTV"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "if GENERATE_DATASET:\n",
    "    # Download dataset generation scripts\n",
    "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/data/shakespeare/preprocess/preprocess_shakespeare.py -O scripts/preprocess_shakespeare.py\n",
    "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/data/shakespeare/preprocess/shake_utils.py -O scripts/shake_utils.py\n",
    "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/data/shakespeare/preprocess/gen_all_data.py -O scripts/gen_all_data.py\n",
    "\n",
    "    # Download data preprocessing scripts\n",
    "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/utils/sample.py -O scripts/sample.py\n",
    "    !wget https://raw.githubusercontent.com/ml-lab/FedProx/master/utils/remove_users.py -O scripts/remove_users.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EIEyRW27goPo"
   },
   "outputs": [],
   "source": [
    "# Running scripts\n",
    "if GENERATE_DATASET:\n",
    "    !mkdir -p data/raw_data data/all_data data/train data/test\n",
    "    !python scripts/preprocess_shakespeare.py data/shakespeare.txt data/raw_data\n",
    "    !python scripts/gen_all_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mq8V6v_4hhhD"
   },
   "source": [
    "#### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "H2SjEBKoWDxv"
   },
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, x, y, corpus, seq_length):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.corpus = corpus\n",
    "        self.corpus_size = len(self.corpus)\n",
    "        super(ShakespeareDataset, self).__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__} - (length: {self.__len__()})'\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        input_seq = self.x[i]\n",
    "        next_char = self.y[i]\n",
    "        # print('\\tgetitem', i, input_seq, next_char)\n",
    "        input_value = self.text2charindxs(input_seq).long()\n",
    "        target_value = self.get_label_from_char(next_char)\n",
    "        # print(input_value, target_value)\n",
    "        return input_value, target_value\n",
    "\n",
    "    def text2charindxs(self, text):\n",
    "        tensor = torch.zeros(len(text), dtype=torch.int32)\n",
    "        for i, c in enumerate(text):\n",
    "            tensor[i] = self.get_label_from_char(c)\n",
    "        return tensor\n",
    "\n",
    "    def get_label_from_char(self, c):\n",
    "        return self.corpus.index(c)\n",
    "\n",
    "    def get_char_from_label(self, l):\n",
    "        return self.corpus[l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fgJtS62lYAN"
   },
   "source": [
    "##### Federated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5DqL5pTmgn5X"
   },
   "outputs": [],
   "source": [
    "class ShakespeareFedDataset(ShakespeareDataset):\n",
    "    def __init__(self, x, y, corpus, seq_length):\n",
    "        super(ShakespeareFedDataset, self).__init__(x, y, corpus, seq_length)\n",
    "\n",
    "    def dataloader(self, batch_size, shuffle=True):\n",
    "        return DataLoader(self,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=shuffle,\n",
    "                          num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XelbyPsDlfgb"
   },
   "source": [
    "## Partitioning & Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOBblyFGlwlU"
   },
   "source": [
    "### IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "cSZFWKmsgn1p"
   },
   "outputs": [],
   "source": [
    "def iid_partition_(dataset, clients):\n",
    "  \"\"\"\n",
    "  I.I.D paritioning of data over clients\n",
    "  Shuffle the data\n",
    "  Split it between clients\n",
    "  \n",
    "  params:\n",
    "    - dataset (torch.utils.Dataset): Dataset\n",
    "    - clients (int): Number of Clients to split the data between\n",
    "\n",
    "  returns:\n",
    "    - Dictionary of image indexes for each client\n",
    "  \"\"\"\n",
    "\n",
    "  num_items_per_client = int(len(dataset)/clients)\n",
    "  client_dict = {}\n",
    "  image_idxs = [i for i in range(len(dataset))]\n",
    "\n",
    "  for i in range(clients):\n",
    "    client_dict[i] = set(np.random.choice(image_idxs, num_items_per_client, replace=False))\n",
    "    image_idxs = list(set(image_idxs) - client_dict[i])\n",
    "\n",
    "  return client_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "-lGwDyhSll9h"
   },
   "outputs": [],
   "source": [
    "def iid_partition(corpus, seq_length=80, val_split=False):\n",
    "\n",
    "    train_file = [os.path.join(DATA_DIR, 'train', f) for f in os.listdir(f'{DATA_DIR}/train') if f.endswith('.json')][0]\n",
    "    test_file = [os.path.join(DATA_DIR, 'test', f) for f in os.listdir(f'{DATA_DIR}/test') if f.endswith('.json')][0]\n",
    "\n",
    "    with open(train_file, 'r') as file:\n",
    "        data_train = json.loads(unidecode(file.read()))\n",
    "\n",
    "    with open(test_file, 'r') as file:\n",
    "        data_test = json.loads(unidecode(file.read()))\n",
    "\n",
    "    \n",
    "    total_samples_train = sum(data_train['num_samples'])\n",
    "\n",
    "    data_dict = {}\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    x_test, y_test = [], []\n",
    "    # x_val, y_val = [], []\n",
    "\n",
    "    users = list(zip(data_train['users'], data_train['num_samples']))\n",
    "    # random.shuffle(users)\n",
    "\n",
    "\n",
    "\n",
    "    total_samples = int(sum(data_train['num_samples']) * SAMPLES_FRACTION)\n",
    "    print('Objective', total_samples, '/', sum(data_train['num_samples']))\n",
    "    sample_count = 0\n",
    "    \n",
    "    for i, (author_id, samples) in enumerate(users):\n",
    "\n",
    "        if sample_count >= total_samples:\n",
    "            print('Max samples reached', sample_count, '/', total_samples)\n",
    "            break\n",
    "\n",
    "        if samples < MIN_SAMPLES: # or data_train['num_samples'][i] > 10000:\n",
    "            print('SKIP', author_id, samples)\n",
    "            continue\n",
    "        else:\n",
    "            udata_train = data_train['user_data'][author_id]\n",
    "            max_samples = samples if (sample_count + samples) <= total_samples else (sample_count + samples - total_samples) \n",
    "            \n",
    "            sample_count += max_samples\n",
    "            # print('sample_count', sample_count)\n",
    "\n",
    "            x_train.extend(data_train['user_data'][author_id]['x'][:max_samples])\n",
    "            y_train.extend(data_train['user_data'][author_id]['y'][:max_samples])\n",
    "\n",
    "            author_data = data_test['user_data'][author_id]\n",
    "            test_size = int(len(author_data['x']) * SAMPLES_FRACTION)\n",
    "\n",
    "            if val_split:\n",
    "                x_test.extend(author_data['x'][:int(test_size / 2)])\n",
    "                y_test.extend(author_data['y'][:int(test_size / 2)])\n",
    "                # x_val.extend(author_data['x'][int(test_size / 2):])\n",
    "                # y_val.extend(author_data['y'][int(test_size / 2):int(test_size)])\n",
    "\n",
    "            else:\n",
    "                x_test.extend(author_data['x'][:int(test_size)])\n",
    "                y_test.extend(author_data['y'][:int(test_size)])\n",
    "\n",
    "    train_ds = ShakespeareDataset(x_train, y_train, corpus, seq_length)\n",
    "    test_ds = ShakespeareDataset(x_test, y_test, corpus, seq_length)\n",
    "    # val_ds = ShakespeareDataset(x_val, y_val, corpus, seq_length)\n",
    "\n",
    "    data_dict = iid_partition_(train_ds, clients=len(users))\n",
    "\n",
    "    return train_ds, data_dict, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFvc8mLoouKa"
   },
   "source": [
    "### Non-IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GZ76WsCZot9s"
   },
   "outputs": [],
   "source": [
    "def noniid_partition(corpus, seq_length=80, val_split=False):\n",
    "\n",
    "    train_file = [os.path.join(DATA_DIR, 'train', f) for f in os.listdir(f'{DATA_DIR}/train') if f.endswith('.json')][0]\n",
    "    test_file = [os.path.join(DATA_DIR, 'test', f) for f in os.listdir(f'{DATA_DIR}/test') if f.endswith('.json')][0]\n",
    "\n",
    "    with open(train_file, 'r') as file:\n",
    "        data_train = json.loads(unidecode(file.read()))\n",
    "\n",
    "    with open(test_file, 'r') as file:\n",
    "        data_test = json.loads(unidecode(file.read()))\n",
    "\n",
    "    \n",
    "    total_samples_train = sum(data_train['num_samples'])\n",
    "\n",
    "    data_dict = {}\n",
    "\n",
    "    x_test, y_test = [], []\n",
    "\n",
    "    users = list(zip(data_train['users'], data_train['num_samples']))\n",
    "    # random.shuffle(users)\n",
    "\n",
    "    total_samples = int(sum(data_train['num_samples']) * SAMPLES_FRACTION)\n",
    "    print('Objective', total_samples, '/', sum(data_train['num_samples']))\n",
    "    sample_count = 0\n",
    "    \n",
    "    for i, (author_id, samples) in enumerate(users):\n",
    "\n",
    "        if sample_count >= total_samples:\n",
    "            print('Max samples reached', sample_count, '/', total_samples)\n",
    "            break\n",
    "\n",
    "        if samples < MIN_SAMPLES: # or data_train['num_samples'][i] > 10000:\n",
    "            print('SKIP', author_id, samples)\n",
    "            continue\n",
    "        else:\n",
    "            udata_train = data_train['user_data'][author_id]\n",
    "            max_samples = samples if (sample_count + samples) <= total_samples else (sample_count + samples - total_samples) \n",
    "            \n",
    "            sample_count += max_samples\n",
    "            # print('sample_count', sample_count)\n",
    "\n",
    "            x_train = data_train['user_data'][author_id]['x'][:max_samples]\n",
    "            y_train = data_train['user_data'][author_id]['y'][:max_samples]\n",
    "\n",
    "            train_ds = ShakespeareFedDataset(x_train, y_train, corpus, seq_length)\n",
    "\n",
    "            x_val, y_val = None, None\n",
    "            val_ds = None\n",
    "            author_data = data_test['user_data'][author_id]\n",
    "            test_size = int(len(author_data['x']) * SAMPLES_FRACTION)\n",
    "            if val_split:\n",
    "                x_test += author_data['x'][:int(test_size / 2)]\n",
    "                y_test += author_data['y'][:int(test_size / 2)]\n",
    "                x_val = author_data['x'][int(test_size / 2):]\n",
    "                y_val = author_data['y'][int(test_size / 2):int(test_size)]\n",
    "\n",
    "                val_ds = ShakespeareFedDataset(x_val, y_val, corpus, seq_length)\n",
    "\n",
    "            else:\n",
    "                x_test += author_data['x'][:int(test_size)]\n",
    "                y_test += author_data['y'][:int(test_size)]\n",
    "\n",
    "            data_dict[author_id] = {\n",
    "                'train_ds': train_ds,\n",
    "                'val_ds': val_ds\n",
    "            }\n",
    "\n",
    "    test_ds = ShakespeareFedDataset(x_test, y_test, corpus, seq_length)\n",
    "\n",
    "    return data_dict, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWVOxcAao2_t"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQQQ2mLeo6EA"
   },
   "source": [
    "### Shakespeare LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2mGXTrXRot7R"
   },
   "outputs": [],
   "source": [
    "class ShakespeareLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, classes, lstm_layers=2, dropout=0.1, batch_first=True):\n",
    "        super(ShakespeareLSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.classes = classes\n",
    "        self.no_layers = lstm_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=self.classes,\n",
    "                                      embedding_dim=self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_dim, \n",
    "                            hidden_size=self.hidden_dim,\n",
    "                            num_layers=self.no_layers,\n",
    "                            batch_first=batch_first, \n",
    "                            dropout=dropout if self.no_layers > 1 else 0.)\n",
    "        self.fc = nn.Linear(hidden_dim, self.classes)\n",
    "\n",
    "    def forward(self, x, hc=None):\n",
    "        batch_size = x.size(0)\n",
    "        x_emb = self.embedding(x)\n",
    "        self.lstm.flatten_parameters()\n",
    "        out, (ht, ct) = self.lstm(x_emb.view(batch_size, -1, self.embedding_dim), hc)\n",
    "        dense = self.fc(ht[-1])\n",
    "        return dense\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(self.no_layers, batch_size, self.hidden_dim)),\n",
    "                Variable(torch.zeros(self.no_layers, batch_size, self.hidden_dim)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QsuJlVipMc8"
   },
   "source": [
    "#### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "n_Vb0BYpot5I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shakespeare LSTM SUMMARY\n",
      "==========================================================\n",
      "            Kernel Shape   Output Shape  Params  Mult-Adds\n",
      "Layer                                                     \n",
      "0_embedding      [8, 90]    [10, 80, 8]     720        720\n",
      "1_lstm                 -  [10, 80, 256]  798720     794624\n",
      "2_fc           [256, 90]       [10, 90]   23130      23040\n",
      "----------------------------------------------------------\n",
      "                      Totals\n",
      "Total params          822570\n",
      "Trainable params      822570\n",
      "Non-trainable params       0\n",
      "Mult-Adds             818384\n",
      "==========================================================\n",
      "            Kernel Shape   Output Shape  Params  Mult-Adds\n",
      "Layer                                                     \n",
      "0_embedding      [8, 90]    [10, 80, 8]     720        720\n",
      "1_lstm                 -  [10, 80, 256]  798720     794624\n",
      "2_fc           [256, 90]       [10, 90]   23130      23040\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "seq_length = 80 # mcmahan17a, fedprox, qFFL\n",
    "\n",
    "shakespeare_lstm = ShakespeareLSTM(input_dim=seq_length,  \n",
    "                                   embedding_dim=8,  # mcmahan17a, fedprox, qFFL\n",
    "                                   hidden_dim=256,  # mcmahan17a, fedprox impl\n",
    "                                   # hidden_dim=100,  # fedprox paper\n",
    "                                   classes=len(corpus),\n",
    "                                   lstm_layers=2,\n",
    "                                   dropout=0.1,\n",
    "                                   batch_first=True\n",
    "                                   )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  shakespeare_lstm.cuda()\n",
    "\n",
    "hc = shakespeare_lstm.init_hidden(batch_size)\n",
    "\n",
    "x_sample = torch.zeros((batch_size, seq_length),\n",
    "                       dtype=torch.long,\n",
    "                       device=(torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')))\n",
    "\n",
    "x_sample[0][0] = 1\n",
    "x_sample\n",
    "\n",
    "print(\"\\nShakespeare LSTM SUMMARY\")\n",
    "print(summaryx(shakespeare_lstm, x_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn7egnzTpeks"
   },
   "source": [
    "## FedAvg Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFFAfTOwpk4j"
   },
   "source": [
    "### Plot Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oyYjWa6IpnTY"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "367THsiTpo-C"
   },
   "outputs": [],
   "source": [
    "def plot_scores(history, exp_id, title, suffix):\n",
    "    accuracies = [x['accuracy'] for x in history]\n",
    "    f1_macro = [x['f1_macro'] for x in history]\n",
    "    f1_weighted = [x['f1_weighted'] for x in history]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(accuracies, 'tab:orange')\n",
    "    ax.set(xlabel='Rounds', ylabel='Test Accuracy', title=title)\n",
    "    ax.grid()\n",
    "    fig.savefig(f'{BASE_DIR}/{exp_id}/Test_Accuracy_{suffix}.jpg', format='jpg', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(f1_macro, 'tab:orange')\n",
    "    ax.set(xlabel='Rounds', ylabel='Test F1 (macro)', title=title)\n",
    "    ax.grid()\n",
    "    fig.savefig(f'{BASE_DIR}/{exp_id}/Test_F1_Macro_{suffix}.jpg', format='jpg')\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(f1_weighted, 'tab:orange')\n",
    "    ax.set(xlabel='Rounds', ylabel='Test F1 (weighted)', title=title)\n",
    "    ax.grid()\n",
    "    fig.savefig(f'{BASE_DIR}/{exp_id}/Test_F1_Weighted_{suffix}.jpg', format='jpg')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_losses(history, exp_id, title, suffix):\n",
    "    val_losses = [x['loss'] for x in history]\n",
    "    train_losses = [x['train_loss'] for x in history]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(train_losses, 'tab:orange')\n",
    "    ax.set(xlabel='Rounds', ylabel='Train Loss', title=title)\n",
    "    ax.grid()\n",
    "    fig.savefig(f'{BASE_DIR}/{exp_id}/Train_Loss_{suffix}.jpg', format='jpg')\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(val_losses, 'tab:orange')\n",
    "    ax.set(xlabel='Rounds', ylabel='Test Loss', title=title)\n",
    "    ax.grid()\n",
    "    fig.savefig(f'{BASE_DIR}/{exp_id}/Test_Loss_{suffix}.jpg', format='jpg')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c640e4NnpksE"
   },
   "source": [
    "### Systems Heterogeneity Simulations\n",
    "\n",
    "Generate epochs for selected clients based on percentage of devices that corresponds to heterogeneity. \n",
    "\n",
    "Assign x number of epochs (chosen unifirmly at random between [1, E]) to 0%, 50% or 90% of the selected devices, respectively. Settings where 0% devices perform fewer than E epochs of work correspond to the environments without system heterogeneity, while 90% of the devices sending their partial solutions corresponds to highly heterogenous system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "zuEZYnl5ot2m"
   },
   "outputs": [],
   "source": [
    "def GenerateLocalEpochs(percentage, size, max_epochs):\n",
    "  ''' Method generates list of epochs for selected clients\n",
    "  to replicate system heteroggeneity\n",
    "\n",
    "  Params:\n",
    "    percentage: percentage of clients to have fewer than E epochs\n",
    "    size:       total size of the list\n",
    "    max_epochs: maximum value for local epochs\n",
    "  \n",
    "  Returns:\n",
    "    List of size epochs for each Client Update\n",
    "\n",
    "  '''\n",
    "\n",
    "  # if percentage is 0 then each client runs for E epochs\n",
    "  if percentage == 0:\n",
    "      return np.array([max_epochs]*size)\n",
    "  else:\n",
    "    # get the number of clients to have fewer than E epochs\n",
    "    heterogenous_size = int((percentage/100) * size)\n",
    "\n",
    "    # generate random uniform epochs of heterogenous size between 1 and E\n",
    "    epoch_list = np.random.randint(1, max_epochs, heterogenous_size)\n",
    "\n",
    "    # the rest of the clients will have E epochs\n",
    "    remaining_size = size - heterogenous_size\n",
    "    rem_list = [max_epochs]*remaining_size\n",
    "\n",
    "    epoch_list = np.append(epoch_list, rem_list, axis=0)\n",
    "    \n",
    "    # shuffle the list and return\n",
    "    np.random.shuffle(epoch_list)\n",
    "\n",
    "    return epoch_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ9PZM0Gp9ve"
   },
   "source": [
    "### Local Training (Client Update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "EDJFltwdotzZ"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, dataset, idxs):\n",
    "      self.dataset = dataset\n",
    "      self.idxs = list(idxs)\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.idxs)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "      data, label = self.dataset[self.idxs[item]]\n",
    "      return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HtRzU5Yepddq"
   },
   "outputs": [],
   "source": [
    "class ClientUpdate(object):\n",
    "  def __init__(self, dataset, batchSize, learning_rate, epochs, idxs, mu, algorithm):\n",
    "    # self.train_loader = DataLoader(CustomDataset(dataset, idxs), batch_size=batchSize, shuffle=True)\n",
    "    if hasattr(dataset, 'dataloader'):\n",
    "        self.train_loader = dataset.dataloader(batch_size=batch_size, shuffle=True)\n",
    "    else:\n",
    "        self.train_loader = DataLoader(CustomDataset(dataset, idxs), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    self.algorithm = algorithm\n",
    "    self.learning_rate = learning_rate\n",
    "    self.epochs = epochs\n",
    "    self.mu = mu\n",
    "\n",
    "  def train(self, model):\n",
    "    # print(\"Client training for {} epochs.\".format(self.epochs))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    proximal_criterion = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.5)\n",
    "\n",
    "    # use the weights of global model for proximal term calculation\n",
    "    global_model = copy.deepcopy(model)\n",
    "\n",
    "    # calculate local training time\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    e_loss = []\n",
    "    for epoch in range(1, self.epochs+1):\n",
    "\n",
    "      train_loss = 0.0\n",
    "      model.train()\n",
    "      for data, labels in self.train_loader:\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "          data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # make a forward pass\n",
    "        output = model(data)\n",
    "\n",
    "        # calculate the loss + the proximal term\n",
    "        _, pred = torch.max(output, 1)\n",
    "\n",
    "        if self.algorithm == 'fedprox':\n",
    "          proximal_term = 0.0\n",
    "\n",
    "          # iterate through the current and global model parameters\n",
    "          for w, w_t in zip(model.parameters(), global_model.parameters()) :\n",
    "            # update the proximal term \n",
    "            #proximal_term += torch.sum(torch.abs((w-w_t)**2))\n",
    "            proximal_term += (w-w_t).norm(2)\n",
    "\n",
    "          loss = criterion(output, labels) + (self.mu/2)*proximal_term\n",
    "        else:\n",
    "          loss = criterion(output, labels)\n",
    "    \n",
    "        # do a backwards pass\n",
    "        loss.backward()\n",
    "        # perform a single optimization step\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "      # average losses\n",
    "      train_loss = train_loss/len(self.train_loader.dataset)\n",
    "      e_loss.append(train_loss)\n",
    "\n",
    "    total_loss = sum(e_loss)/len(e_loss)\n",
    "\n",
    "    return model.state_dict(), total_loss, (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3crFDN0xqGu6"
   },
   "source": [
    "### Server Side Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "c085xSOoqEHk"
   },
   "outputs": [],
   "source": [
    "def training(model, rounds, batch_size, lr, ds, data_dict, test_ds, C, K, E, mu, percentage, plt_title, plt_color, target_test_accuracy,\n",
    "             classes, algorithm=\"fedprox\", eval_every=1, tb_logger=None):\n",
    "  \"\"\"\n",
    "  Function implements the Federated Averaging Algorithm from the FedAvg paper.\n",
    "  Specifically, this function is used for the server side training and weight update\n",
    "\n",
    "  Params:\n",
    "    - model:           PyTorch model to train\n",
    "    - rounds:          Number of communication rounds for the client update\n",
    "    - batch_size:      Batch size for client update training\n",
    "    - lr:              Learning rate used for client update training\n",
    "    - ds:              Dataset used for training\n",
    "    - data_dict:       Type of data partition used for training (IID or non-IID)\n",
    "    - test_data_dict:  Data used for testing the model\n",
    "    - C:               Fraction of clients randomly chosen to perform computation on each round\n",
    "    - K:               Total number of clients\n",
    "    - E:               Number of training passes each client makes over its local dataset per round\n",
    "    - mu:              proximal term constant\n",
    "    - percentage:      percentage of selected client to have fewer than E epochs\n",
    "  Returns:\n",
    "    - model:           Trained model on the server\n",
    "  \"\"\"\n",
    "\n",
    "  # global model weights\n",
    "  global_weights = model.state_dict()\n",
    "\n",
    "  # training loss\n",
    "  train_loss = []\n",
    "\n",
    "  # test accuracy\n",
    "  test_accuracy = []\n",
    "\n",
    "  # test loss\n",
    "  test_loss = []\n",
    "\n",
    "  # history\n",
    "  history=[]\n",
    "\n",
    "  # store last loss for convergence\n",
    "  last_loss = 0.0\n",
    "\n",
    "  # total time taken \n",
    "  total_time = 0\n",
    "  start = time.time()\n",
    "\n",
    "  print(f\"System heterogeneity set to {percentage}% stragglers.\\n\")\n",
    "  print(f\"Picking {max(int(C*K),1 )} random clients per round.\\n\")\n",
    "\n",
    "  users_id = list(data_dict.keys())\n",
    "\n",
    "  for curr_round in range(1, rounds+1):\n",
    "    w, local_loss, lst_local_train_time = [], [], []\n",
    "\n",
    "    m = max(int(C*K), 1)\n",
    "\n",
    "    heterogenous_epoch_list = GenerateLocalEpochs(percentage, size=m, max_epochs=E)\n",
    "    heterogenous_epoch_list = np.array(heterogenous_epoch_list)\n",
    "    # print('heterogenous_epoch_list', len(heterogenous_epoch_list))\n",
    "\n",
    "    S_t = np.random.choice(range(K), m, replace=False)\n",
    "    S_t = np.array(S_t)\n",
    "    print('Clients: {}/{} -> {}'.format(len(S_t), K, S_t))\n",
    "    \n",
    "    # For Federated Averaging, drop all the clients that are stragglers\n",
    "    if algorithm == 'fedavg':\n",
    "      stragglers_indices = np.argwhere(heterogenous_epoch_list < E)\n",
    "      heterogenous_epoch_list = np.delete(heterogenous_epoch_list, stragglers_indices)\n",
    "      S_t = np.delete(S_t, stragglers_indices)\n",
    "\n",
    "    # for _, (k, epoch) in tqdm(enumerate(zip(S_t, heterogenous_epoch_list))):\n",
    "    for i in tqdm(range(len(S_t))):\n",
    "    #   print('k', k)\n",
    "      k = S_t[i]\n",
    "      epoch = heterogenous_epoch_list[i]\n",
    "      key = users_id[k]\n",
    "      ds_ = ds if ds else data_dict[key]['train_ds']\n",
    "      idxs = data_dict[key] if ds else None\n",
    "    #   print(f'Client {k}: {len(idxs) if idxs else len(ds_)} samples')\n",
    "      local_update = ClientUpdate(dataset=ds_, batchSize=batch_size, learning_rate=lr, epochs=epoch, idxs=idxs, mu=mu, algorithm=algorithm)\n",
    "      weights, loss, local_train_time = local_update.train(model=copy.deepcopy(model))\n",
    "    #   print(f'Local train time for {k} on {len(idxs) if idxs else len(ds_)} samples: {local_train_time}')\n",
    "    #   print(f'Local train time: {local_train_time}')\n",
    "\n",
    "      w.append(copy.deepcopy(weights))\n",
    "      local_loss.append(copy.deepcopy(loss))\n",
    "      lst_local_train_time.append(local_train_time)\n",
    "\n",
    "    # calculate time to update the global weights\n",
    "    global_start_time = time.time()\n",
    "\n",
    "    # updating the global weights\n",
    "    weights_avg = copy.deepcopy(w[0])\n",
    "    for k in weights_avg.keys():\n",
    "      for i in range(1, len(w)):\n",
    "        weights_avg[k] += w[i][k]\n",
    "\n",
    "      weights_avg[k] = torch.div(weights_avg[k], len(w))\n",
    "\n",
    "    global_weights = weights_avg\n",
    "\n",
    "    global_end_time = time.time()\n",
    "\n",
    "    # calculate total time \n",
    "    total_time += (global_end_time - global_start_time) + sum(lst_local_train_time)/len(lst_local_train_time)\n",
    "\n",
    "    # move the updated weights to our model state dict\n",
    "    model.load_state_dict(global_weights)\n",
    "\n",
    "    # loss\n",
    "    loss_avg = sum(local_loss) / len(local_loss)\n",
    "    print('Round: {}... \\tAverage Loss: {}'.format(curr_round, round(loss_avg, 3)))\n",
    "    train_loss.append(loss_avg)\n",
    "    if tb_logger:\n",
    "        tb_logger.add_scalar(f'Train/Loss', loss_avg, curr_round)\n",
    "\n",
    "    # testing\n",
    "    # if curr_round % eval_every == 0:\n",
    "    test_scores = testing(model, test_ds, batch_size * 2, nn.CrossEntropyLoss(), len(classes), classes)\n",
    "    test_scores['train_loss'] = loss_avg\n",
    "    test_loss_current, test_accuracy_current = test_scores['loss'], test_scores['accuracy']\n",
    "\n",
    "    history.append(test_scores)\n",
    "    test_accuracy.append(test_accuracy_current)\n",
    "    test_loss.append(test_loss_current)\n",
    "    \n",
    "    # print('Round: {}... \\tAverage Loss: {} \\tTest Loss: {} \\tTest Acc: {}'.format(curr_round, round(loss_avg, 3), round(test_loss, 3), round(test_accuracy, 3)))\n",
    "\n",
    "    if tb_logger:\n",
    "        tb_logger.add_scalar(f'Test/Loss', test_scores['loss'], curr_round)\n",
    "        tb_logger.add_scalars(f'Test/Scores', {\n",
    "            'accuracy': test_scores['accuracy'], 'f1_macro': test_scores['f1_macro'], 'f1_weighted': test_scores['f1_weighted']\n",
    "        }, curr_round)\n",
    "\n",
    "    # break if we achieve the target test accuracy\n",
    "    if test_accuracy_current >= target_test_accuracy:\n",
    "      rounds = curr_round\n",
    "      break\n",
    "\n",
    "    # break if we achieve convergence, i.e., loss between two consecutive rounds is <0.0001\n",
    "    if algorithm == 'fedprox' and abs(loss_avg - last_loss) < 1e-5:\n",
    "      rounds = curr_round\n",
    "      break\n",
    "    \n",
    "    # update the last loss\n",
    "    last_loss = loss_avg\n",
    "\n",
    "  end = time.time()\n",
    "  \n",
    "  # plot train loss\n",
    "  fig, ax = plt.subplots()\n",
    "  x_axis = np.arange(1, rounds+1)\n",
    "  y_axis = np.array(train_loss)\n",
    "  ax.plot(x_axis, y_axis)\n",
    "\n",
    "  ax.set(xlabel='Number of Rounds', ylabel='Train Loss', title=plt_title)\n",
    "  ax.grid()\n",
    "  fig.savefig(plt_title+'_Train_loss.jpg', format='jpg')\n",
    "  plt.show()\n",
    "  \n",
    "  fig, ax = plt.subplots()\n",
    "  x_axis = np.arange(1, rounds+1)\n",
    "  y_axis = np.array(test_loss)\n",
    "  ax.plot(x_axis, y_axis, 'tab:'+plt_color)\n",
    "\n",
    "  ax.set(xlabel='Number of Rounds', ylabel='Test Loss',\n",
    "       title=plt_title)\n",
    "  ax.grid()\n",
    "  fig.savefig(plt_title+'_Test_loss.jpg', format='jpg')\n",
    "  plt.show()\n",
    "\n",
    "  # plot test accuracy\n",
    "  fig, ax = plt.subplots()\n",
    "  x_axis = np.arange(1, rounds+1)\n",
    "  y_axis = np.array(test_accuracy)\n",
    "  ax.plot(x_axis, y_axis)\n",
    "\n",
    "  ax.set(xlabel='Number of Rounds', ylabel='Test Accuracy', title=plt_title)\n",
    "  ax.grid()\n",
    "  fig.savefig(plt_title+'_Test_Accuracy.jpg', format='jpg')\n",
    "  plt.show()\n",
    "\n",
    "  print(\"Training Done! Total time taken to Train: {}\".format(end-start))\n",
    "\n",
    "  return model, train_loss, test_accuracy, test_loss, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXtGLkoAqLIW"
   },
   "source": [
    "### Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "dQJIJno4qKvc"
   },
   "outputs": [],
   "source": [
    "def testing(model, dataset, bs, criterion, num_classes, classes, print_all=False):\n",
    "  #test loss \n",
    "  test_loss = 0.0\n",
    "  correct_class = list(0. for i in range(num_classes))\n",
    "  total_class = list(0. for i in range(num_classes))\n",
    "\n",
    "  test_loader = DataLoader(dataset, batch_size=bs)\n",
    "  l = len(test_loader)\n",
    "  model.eval()\n",
    "  print('running validation...')\n",
    "  for i, (data, labels) in enumerate(tqdm(test_loader)):\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "      data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "    output = model(data)\n",
    "    loss = criterion(output, labels)\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "\n",
    "    _, pred = torch.max(output, 1)\n",
    "\n",
    "    # For F1Score\n",
    "    y_true = np.append(y_true, labels.data.view_as(pred).cpu().numpy()) if i != 0 else labels.data.view_as(pred).cpu().numpy()\n",
    "    y_hat = np.append(y_hat, pred.cpu().numpy()) if i != 0 else pred.cpu().numpy()\n",
    "\n",
    "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
    "\n",
    "    #test accuracy for each object class\n",
    "    # for i in range(num_classes):\n",
    "    #   label = labels.data[i]\n",
    "    #   correct_class[label] += correct[i].item()\n",
    "    #   total_class[label] += 1\n",
    "\n",
    "    for i, lbl in enumerate(labels.data):\n",
    "    #   print('lbl', i, lbl)\n",
    "      correct_class[lbl] += correct.data[i]\n",
    "      total_class[lbl] += 1\n",
    "    \n",
    "  # avg test loss\n",
    "  test_loss = test_loss/len(test_loader.dataset)\n",
    "  print(\"Test Loss: {:.6f}\\n\".format(test_loss))\n",
    "\n",
    "  # Avg F1 Score\n",
    "  f1_macro = f1_score(y_true, y_hat, average='macro')\n",
    "  # F1-Score -> weigthed to consider class imbalance\n",
    "  f1_weighted =  f1_score(y_true, y_hat, average='weighted')\n",
    "  print(\"F1 Score: {:.6f} (macro) {:.6f} (weighted) %\\n\".format(f1_macro, f1_weighted))\n",
    "\n",
    "  # print test accuracy\n",
    "  if print_all:\n",
    "    for i in range(num_classes):\n",
    "        if total_class[i]>0:\n",
    "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % \n",
    "                    (classes[i], 100 * correct_class[i] / total_class[i],\n",
    "                    np.sum(correct_class[i]), np.sum(total_class[i])))\n",
    "        else:\n",
    "            print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "  overall_accuracy = np.sum(correct_class) / np.sum(total_class)\n",
    "\n",
    "  print('\\nFinal Test  Accuracy: {:.3f} ({}/{})'.format(overall_accuracy, np.sum(correct_class), np.sum(total_class)))\n",
    "\n",
    "  return {'loss': test_loss, 'accuracy': overall_accuracy, 'f1_macro': f1_macro, 'f1_weighted': f1_weighted}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxqXLBd8qbC2"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "c8gl5P3SMq4a"
   },
   "outputs": [],
   "source": [
    "log_dict = {}\n",
    "NUM_REPEAT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "E2CfSkNVqKtL"
   },
   "outputs": [],
   "source": [
    "seq_length = 80  # mcmahan17a, fedprox, qFFL\n",
    "embedding_dim = 8  # mcmahan17a, fedprox, qFFL\n",
    "# hidden_dim = 100  # fedprox paper\n",
    "hidden_dim = 256  # mcmahan17a, fedprox impl\n",
    "num_classes = len(corpus)\n",
    "classes = list(range(num_classes))\n",
    "lstm_layers = 2  # mcmahan17a, fedprox, qFFL\n",
    "dropout = 0.1  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPIpStyNJ-63"
   },
   "source": [
    "## LSTM FedAvg on IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "gpS1gyJ_H_MA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective 413629 / 413629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Total users:', 143)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, data_dict, test_ds = iid_partition(corpus, seq_length, val_split=True)  # Not using val_ds but makes train eval periods faster\n",
    "\n",
    "total_clients = len(data_dict.keys())\n",
    "'Total users:', total_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYoGsy05H_RC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Run Number:  0\n",
      "Objective 413629 / 413629\n",
      "System heterogeneity set to 0% stragglers.\n",
      "\n",
      "Picking 10 random clients per round.\n",
      "\n",
      "Clients: 10/143 -> [ 30  16  81  41 133 142  54  39 113  24]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a200933abf4af18a0f8f3c6df83a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 1... \tAverage Loss: 3.254\n",
      "running validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a476426e2a7484d986348a6b082e1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.989389\n",
      "\n",
      "F1 Score: 0.014116 (macro) 0.108862 (weighted) %\n",
      "\n",
      "\n",
      "Final Test  Accuracy: 0.230 (11893.0/51704.0)\n",
      "Clients: 10/143 -> [ 16  24  52  10  42 133  95 131 100  33]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8edee33c10784c298fea862ebaa7444a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 2... \tAverage Loss: 2.912\n",
      "running validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbaa8b9acf24613b15aa6062c8fa612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.666373\n",
      "\n",
      "F1 Score: 0.031196 (macro) 0.182922 (weighted) %\n",
      "\n",
      "\n",
      "Final Test  Accuracy: 0.278 (14372.0/51704.0)\n",
      "Clients: 10/143 -> [ 83  43 121  62  46  49  23  17 117  39]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6e18d0e2344721b2bb19b9af8cffa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 3... \tAverage Loss: 2.694\n",
      "running validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68837150f2b40598962b98697ac42da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2586 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
    "test_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "\n",
    "for exp_num in range(NUM_REPEAT):\n",
    "  print(\"Experiment Run Number: \", exp_num)\n",
    "\n",
    "  # partition data\n",
    "  train_ds, data_dict, test_ds = iid_partition(corpus, seq_length, val_split=True)  # Not using val_ds but makes train eval periods faster\n",
    "  total_clients = len(data_dict.keys())\n",
    "  # number of training rounds\n",
    "  rounds = 50\n",
    "  # client fraction\n",
    "  C = 0.07  # 10 clients\n",
    "  # number of clients\n",
    "  K = total_clients\n",
    "  # number of training passes on local dataset for each roung\n",
    "  E = 1\n",
    "  # batch size\n",
    "  batch_size = 10\n",
    "  # learning Rate\n",
    "  lr = 0.8\n",
    "  # proximal term constant\n",
    "  mu = 0.001\n",
    "  # percentage of clients to have fewer than E epochs\n",
    "  percentage = 0\n",
    "  # target test accuracy\n",
    "  target_test_accuracy= 101\n",
    "\n",
    "  shakespeare_lstm = ShakespeareLSTM(input_dim=seq_length,  \n",
    "                                    embedding_dim=embedding_dim,  \n",
    "                                    hidden_dim=hidden_dim,\n",
    "                                    classes=num_classes,\n",
    "                                    lstm_layers=lstm_layers,\n",
    "                                    dropout=dropout,\n",
    "                                    batch_first=True\n",
    "                                    )\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "      shakespeare_lstm.cuda()\n",
    "\n",
    "  lstm_iid_trained, train_loss, test_accuracy, test_loss, history = training(shakespeare_lstm,\n",
    "                                          rounds, batch_size, lr,\n",
    "                                          train_ds,\n",
    "                                          data_dict,\n",
    "                                          test_ds,\n",
    "                                          C, K, E, mu, percentage,\n",
    "                                          'Shakespeare LSTM on IID', \"green\",\n",
    "                                          target_test_accuracy,\n",
    "                                          corpus, # classes\n",
    "                                          algorithm='fedavg',\n",
    "                                          )\n",
    "\n",
    "  train_loss_multiple_runs[exp_num] = train_loss\n",
    "  test_accuracy_multiple_runs[exp_num] = test_accuracy\n",
    "  test_loss_multiple_runs[exp_num] = test_loss\n",
    "\n",
    "  del lstm_iid_trained\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-irvQHuNNTtB"
   },
   "outputs": [],
   "source": [
    "hyperparams = {'rounds': rounds,\n",
    "               'C': C,\n",
    "               'K': K,\n",
    "               'E': E,\n",
    "               'batch_size': batch_size,\n",
    "               'lr': lr,\n",
    "               'mu': mu,\n",
    "               'percentage': percentage,\n",
    "               'target_accuracy': target_test_accuracy,\n",
    "               }\n",
    "\n",
    "log_dict['Shakespeare LSTM on IID'] = {'train_loss': train_loss_multiple_runs, \n",
    "                                'test_loss': test_loss_multiple_runs, \n",
    "                                'test_accuracy': test_accuracy_multiple_runs,\n",
    "                                'history': history,\n",
    "                                'hyperparams': hyperparams,\n",
    "                                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QO0GkVyKEgu"
   },
   "source": [
    "## LSTM FedAvg on Non IID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wsHRfwmKGdS"
   },
   "outputs": [],
   "source": [
    "data_dict, test_ds = noniid_partition(corpus, seq_length=seq_length, val_split=False)\n",
    " \n",
    "total_clients = len(data_dict.keys())  \n",
    "'Total users:', total_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNJZG5tvKGgd"
   },
   "outputs": [],
   "source": [
    "train_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
    "test_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "\n",
    "for exp_num in range(NUM_REPEAT):\n",
    "  print(\"Experiment Run Number: \", exp_num)\n",
    "  # partition dataset\n",
    "  data_dict, test_ds = noniid_partition(corpus, seq_length=seq_length, val_split=False)\n",
    "  total_clients = len(data_dict.keys())  \n",
    "  # number of training rounds\n",
    "  rounds = 50\n",
    "  # client fraction\n",
    "  C = 0.07  # 10 clients\n",
    "  # number of clients\n",
    "  K = total_clients\n",
    "  # number of training passes on local dataset for each roung\n",
    "  E = 1\n",
    "  # batch size\n",
    "  batch_size = 10\n",
    "  # learning Rate\n",
    "  lr = 0.8\n",
    "  # proximal term constant\n",
    "  mu = 0.001\n",
    "  # percentage of clients to have fewer than E epochs\n",
    "  percentage = 0\n",
    "  # target test accuracy\n",
    "  target_test_accuracy= 101\n",
    "\n",
    "  shakespeare_lstm = ShakespeareLSTM(input_dim=seq_length,\n",
    "                                        embedding_dim=embedding_dim,\n",
    "                                        hidden_dim=hidden_dim,\n",
    "                                        classes=num_classes,\n",
    "                                        lstm_layers=lstm_layers,\n",
    "                                        dropout=dropout,\n",
    "                                        batch_first=True\n",
    "                                        )\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "      shakespeare_lstm.cuda()\n",
    "\n",
    "  lstm_non_iid_trained, train_loss, test_accuracy, test_loss, history = training(shakespeare_lstm,\n",
    "                                                rounds, batch_size, lr,\n",
    "                                                None, #  ds empty as it is included in data_dict\n",
    "                                                data_dict,\n",
    "                                                test_ds,\n",
    "                                                C, K, E, mu, percentage,\n",
    "                                                'Shakespeare LSTM on Non IID', \"green\",\n",
    "                                                target_test_accuracy,\n",
    "                                                corpus, # classes,\n",
    "                                                algorithm='fedavg',\n",
    "                                                )\n",
    "\n",
    "  train_loss_multiple_runs[exp_num] = train_loss\n",
    "  test_accuracy_multiple_runs[exp_num] = test_accuracy\n",
    "  test_loss_multiple_runs[exp_num] = test_loss\n",
    "\n",
    "  del lstm_non_iid_trained\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uANkBpTRKGkv"
   },
   "outputs": [],
   "source": [
    " hyperparams = {'rounds': rounds,\n",
    "               'C': C,\n",
    "               'K': K,\n",
    "               'E': E,\n",
    "               'batch_size': batch_size,\n",
    "               'lr': lr,\n",
    "               'mu': mu,\n",
    "               'percentage': percentage,\n",
    "               'target_accuracy': target_test_accuracy,\n",
    "               }\n",
    "\n",
    "log_dict['Shakespeare LSTM on Non IID'] = {'train_loss': train_loss_multiple_runs, \n",
    "                                'test_loss': test_loss_multiple_runs, \n",
    "                                'test_accuracy': test_accuracy_multiple_runs,\n",
    "                                'history': history,\n",
    "                                'hyperparams': hyperparams,\n",
    "                                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShdScPNuQzUQ"
   },
   "source": [
    "## Pickle Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwHt7jviQ1AV"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(path + 'Local_Round_FedAvg_1.pkl', 'wb') as file:\n",
    "  pickle.dump(log_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "qNkwXxO8Q3Ei"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Shakespeare LSTM on IID': {'history': [{'accuracy': 0.24181881479189232,\n",
       "    'f1_macro': 0.020170024767600728,\n",
       "    'f1_weighted': 0.13998161115748645,\n",
       "    'loss': 2.8704062371214865,\n",
       "    'train_loss': 3.172292667652067},\n",
       "   {'accuracy': 0.27595543865078137,\n",
       "    'f1_macro': 0.03175865166759271,\n",
       "    'f1_weighted': 0.1895768785147695,\n",
       "    'loss': 2.615885275188253,\n",
       "    'train_loss': 2.8275224430350017},\n",
       "   {'accuracy': 0.3108076744545877,\n",
       "    'f1_macro': 0.0447782339236446,\n",
       "    'f1_weighted': 0.22788394847630322,\n",
       "    'loss': 2.476736996816847,\n",
       "    'train_loss': 2.6656465706571337},\n",
       "   {'accuracy': 0.33272087265975553,\n",
       "    'f1_macro': 0.05610451932060805,\n",
       "    'f1_weighted': 0.24946606369718083,\n",
       "    'loss': 2.3935318237005436,\n",
       "    'train_loss': 2.5594668638089964},\n",
       "   {'accuracy': 0.34678168033421014,\n",
       "    'f1_macro': 0.06683024266840652,\n",
       "    'f1_weighted': 0.2755685725232554,\n",
       "    'loss': 2.3251692064863856,\n",
       "    'train_loss': 2.503795595642277},\n",
       "   {'accuracy': 0.36391768528547114,\n",
       "    'f1_macro': 0.07859244956440781,\n",
       "    'f1_weighted': 0.30026286462952384,\n",
       "    'loss': 2.274956694549218,\n",
       "    'train_loss': 2.433573953714608},\n",
       "   {'accuracy': 0.3649814327711589,\n",
       "    'f1_macro': 0.08610316616534365,\n",
       "    'f1_weighted': 0.31035371063344663,\n",
       "    'loss': 2.2288321158879683,\n",
       "    'train_loss': 2.384379834653925},\n",
       "   {'accuracy': 0.37590902057867864,\n",
       "    'f1_macro': 0.0883546895860966,\n",
       "    'f1_weighted': 0.3234725384659925,\n",
       "    'loss': 2.1837221009432857,\n",
       "    'train_loss': 2.349919871736529},\n",
       "   {'accuracy': 0.3844190004641807,\n",
       "    'f1_macro': 0.09649259742239125,\n",
       "    'f1_weighted': 0.3314566732493928,\n",
       "    'loss': 2.154629263114435,\n",
       "    'train_loss': 2.3041554461005322},\n",
       "   {'accuracy': 0.3939927278353706,\n",
       "    'f1_macro': 0.10033630382691074,\n",
       "    'f1_weighted': 0.3379173186301306,\n",
       "    'loss': 2.1209926644973334,\n",
       "    'train_loss': 2.259013009685533},\n",
       "   {'accuracy': 0.39488240755067305,\n",
       "    'f1_macro': 0.09964637325915937,\n",
       "    'f1_weighted': 0.33848850510879164,\n",
       "    'loss': 2.1017009887591285,\n",
       "    'train_loss': 2.2509910819956707},\n",
       "   {'accuracy': 0.4052297694569086,\n",
       "    'f1_macro': 0.11634224634600807,\n",
       "    'f1_weighted': 0.3542272267975484,\n",
       "    'loss': 2.0686070624126214,\n",
       "    'train_loss': 2.2247461487973217},\n",
       "   {'accuracy': 0.41261797926659444,\n",
       "    'f1_macro': 0.11136243241189912,\n",
       "    'f1_weighted': 0.3655430333376197,\n",
       "    'loss': 2.0422536000452616,\n",
       "    'train_loss': 2.194709012114326},\n",
       "   {'accuracy': 0.41433931610707103,\n",
       "    'f1_macro': 0.10945829184509179,\n",
       "    'f1_weighted': 0.36012398314759786,\n",
       "    'loss': 2.0217699961716336,\n",
       "    'train_loss': 2.16783016129071},\n",
       "   {'accuracy': 0.4174145133838775,\n",
       "    'f1_macro': 0.12270293163694462,\n",
       "    'f1_weighted': 0.36654988709695596,\n",
       "    'loss': 2.005111342084753,\n",
       "    'train_loss': 2.137752894692401},\n",
       "   {'accuracy': 0.4240871112486461,\n",
       "    'f1_macro': 0.1300620330092785,\n",
       "    'f1_weighted': 0.3737320248944591,\n",
       "    'loss': 1.989066962661683,\n",
       "    'train_loss': 2.128321695008301},\n",
       "   {'accuracy': 0.42874825932229615,\n",
       "    'f1_macro': 0.12928401658044528,\n",
       "    'f1_weighted': 0.3810367012511682,\n",
       "    'loss': 1.963774728767877,\n",
       "    'train_loss': 2.1257789585991853},\n",
       "   {'accuracy': 0.4304889370261488,\n",
       "    'f1_macro': 0.13647390796429268,\n",
       "    'f1_weighted': 0.3930332724530997,\n",
       "    'loss': 1.9531482908561764,\n",
       "    'train_loss': 2.101194934744805},\n",
       "   {'accuracy': 0.4420741141884574,\n",
       "    'f1_macro': 0.1409922004936992,\n",
       "    'f1_weighted': 0.39722687592912687,\n",
       "    'loss': 1.9286202904509628,\n",
       "    'train_loss': 2.074158604104036},\n",
       "   {'accuracy': 0.44325390685440197,\n",
       "    'f1_macro': 0.14516385083767835,\n",
       "    'f1_weighted': 0.40236925457648265,\n",
       "    'loss': 1.9166624559948093,\n",
       "    'train_loss': 2.048362573635034},\n",
       "   {'accuracy': 0.44567151477641964,\n",
       "    'f1_macro': 0.14699172134301924,\n",
       "    'f1_weighted': 0.4024636791085541,\n",
       "    'loss': 1.9014465526263955,\n",
       "    'train_loss': 2.034728294459364},\n",
       "   {'accuracy': 0.44592294600030946,\n",
       "    'f1_macro': 0.14580464844654026,\n",
       "    'f1_weighted': 0.404157108407049,\n",
       "    'loss': 1.8970308748905194,\n",
       "    'train_loss': 2.0257261932885497},\n",
       "   {'accuracy': 0.44973309608540923,\n",
       "    'f1_macro': 0.15343036073884056,\n",
       "    'f1_weighted': 0.40818532398600604,\n",
       "    'loss': 1.8810597746859952,\n",
       "    'train_loss': 2.0190272134554834},\n",
       "   {'accuracy': 0.45547733250812317,\n",
       "    'f1_macro': 0.15451363854006306,\n",
       "    'f1_weighted': 0.4133262100188849,\n",
       "    'loss': 1.86315049018014,\n",
       "    'train_loss': 2.006747446762575},\n",
       "   {'accuracy': 0.45584480891226986,\n",
       "    'f1_macro': 0.15401049102474634,\n",
       "    'f1_weighted': 0.41128439723229554,\n",
       "    'loss': 1.8572389807148042,\n",
       "    'train_loss': 1.998156090088982},\n",
       "   {'accuracy': 0.45797230388364535,\n",
       "    'f1_macro': 0.155642112629565,\n",
       "    'f1_weighted': 0.4196508580856131,\n",
       "    'loss': 1.8442246364828752,\n",
       "    'train_loss': 1.9796157358551754},\n",
       "   {'accuracy': 0.46381324462324,\n",
       "    'f1_macro': 0.15873865831147124,\n",
       "    'f1_weighted': 0.4238000468158084,\n",
       "    'loss': 1.8298262302653812,\n",
       "    'train_loss': 1.9643770977133372},\n",
       "   {'accuracy': 0.4630202692248182,\n",
       "    'f1_macro': 0.16460242536293684,\n",
       "    'f1_weighted': 0.42195307768234336,\n",
       "    'loss': 1.8266439208826857,\n",
       "    'train_loss': 1.9612925607510128},\n",
       "   {'accuracy': 0.4662695342720099,\n",
       "    'f1_macro': 0.16751734982619207,\n",
       "    'f1_weighted': 0.4247302111031032,\n",
       "    'loss': 1.8109416080246525,\n",
       "    'train_loss': 1.936425840636692},\n",
       "   {'accuracy': 0.4679521893857342,\n",
       "    'f1_macro': 0.1613459038684948,\n",
       "    'f1_weighted': 0.429797006378873,\n",
       "    'loss': 1.8008708183990991,\n",
       "    'train_loss': 1.9312426919552959},\n",
       "   {'accuracy': 0.4695381401825777,\n",
       "    'f1_macro': 0.16871189581505988,\n",
       "    'f1_weighted': 0.43035072219759307,\n",
       "    'loss': 1.7953047581361705,\n",
       "    'train_loss': 1.925921692860984},\n",
       "   {'accuracy': 0.47396719789571407,\n",
       "    'f1_macro': 0.17552374991146777,\n",
       "    'f1_weighted': 0.43936655579825024,\n",
       "    'loss': 1.7811402343876421,\n",
       "    'train_loss': 1.9101660262396556},\n",
       "   {'accuracy': 0.4754757852390531,\n",
       "    'f1_macro': 0.17427170217220003,\n",
       "    'f1_weighted': 0.43584359528646516,\n",
       "    'loss': 1.7753249212301738,\n",
       "    'train_loss': 1.8969225642550864},\n",
       "   {'accuracy': 0.4772744855330342,\n",
       "    'f1_macro': 0.1771135180480995,\n",
       "    'f1_weighted': 0.4431035274098244,\n",
       "    'loss': 1.770527926928005,\n",
       "    'train_loss': 1.8996172329681422},\n",
       "   {'accuracy': 0.47936329877765743,\n",
       "    'f1_macro': 0.17910936253087129,\n",
       "    'f1_weighted': 0.4452260909601371,\n",
       "    'loss': 1.7573459124278767,\n",
       "    'train_loss': 1.8941376614215606},\n",
       "   {'accuracy': 0.4807945226674919,\n",
       "    'f1_macro': 0.17825052431611244,\n",
       "    'f1_weighted': 0.4457653045956767,\n",
       "    'loss': 1.757931219643573,\n",
       "    'train_loss': 1.8833446032237546},\n",
       "   {'accuracy': 0.4853202846975089,\n",
       "    'f1_macro': 0.1812068863039251,\n",
       "    'f1_weighted': 0.4519881673982214,\n",
       "    'loss': 1.7413820005318117,\n",
       "    'train_loss': 1.8852482819324297},\n",
       "   {'accuracy': 0.484507968435711,\n",
       "    'f1_macro': 0.1840698837624963,\n",
       "    'f1_weighted': 0.44557909677515817,\n",
       "    'loss': 1.740199619299226,\n",
       "    'train_loss': 1.835488330056816},\n",
       "   {'accuracy': 0.48580380628191244,\n",
       "    'f1_macro': 0.1859680147088409,\n",
       "    'f1_weighted': 0.4538288449439704,\n",
       "    'loss': 1.7336851670455034,\n",
       "    'train_loss': 1.8637072051915227},\n",
       "   {'accuracy': 0.4915673835680025,\n",
       "    'f1_macro': 0.1943677718226367,\n",
       "    'f1_weighted': 0.4599602253320601,\n",
       "    'loss': 1.7222032833307792,\n",
       "    'train_loss': 1.8312147337589082},\n",
       "   {'accuracy': 0.4893238434163701,\n",
       "    'f1_macro': 0.18785147249728606,\n",
       "    'f1_weighted': 0.4541252100156915,\n",
       "    'loss': 1.7233503760636526,\n",
       "    'train_loss': 1.8272460494813914},\n",
       "   {'accuracy': 0.4911612254371035,\n",
       "    'f1_macro': 0.19007859148385195,\n",
       "    'f1_weighted': 0.46003810693267266,\n",
       "    'loss': 1.7134182807758425,\n",
       "    'train_loss': 1.8201890394318319},\n",
       "   {'accuracy': 0.4924183815565527,\n",
       "    'f1_macro': 0.1859177759191026,\n",
       "    'f1_weighted': 0.4577435662901513,\n",
       "    'loss': 1.706000501757138,\n",
       "    'train_loss': 1.8452389343068483},\n",
       "   {'accuracy': 0.4946425808448089,\n",
       "    'f1_macro': 0.1908799974126648,\n",
       "    'f1_weighted': 0.4597229754828072,\n",
       "    'loss': 1.6997894279159629,\n",
       "    'train_loss': 1.8542960902055103},\n",
       "   {'accuracy': 0.496712053226056,\n",
       "    'f1_macro': 0.19820025730010396,\n",
       "    'f1_weighted': 0.46555832515383866,\n",
       "    'loss': 1.692543137577062,\n",
       "    'train_loss': 1.8226716406241494},\n",
       "   {'accuracy': 0.5006575893547888,\n",
       "    'f1_macro': 0.20296110626629246,\n",
       "    'f1_weighted': 0.4700830322336886,\n",
       "    'loss': 1.6861111290203,\n",
       "    'train_loss': 1.8088645050098624},\n",
       "   {'accuracy': 0.5011217700758162,\n",
       "    'f1_macro': 0.20263870408431836,\n",
       "    'f1_weighted': 0.46957579169104346,\n",
       "    'loss': 1.681510297547834,\n",
       "    'train_loss': 1.8092308332646367},\n",
       "   {'accuracy': 0.4997679096394863,\n",
       "    'f1_macro': 0.1930523930841869,\n",
       "    'f1_weighted': 0.46660577265246106,\n",
       "    'loss': 1.6795614582715426,\n",
       "    'train_loss': 1.7943997773103533},\n",
       "   {'accuracy': 0.5018760637474857,\n",
       "    'f1_macro': 0.2093683493312167,\n",
       "    'f1_weighted': 0.47092775234868733,\n",
       "    'loss': 1.6761860625656115,\n",
       "    'train_loss': 1.8091744981984383},\n",
       "   {'accuracy': 0.5045451028933932,\n",
       "    'f1_macro': 0.20351552937565182,\n",
       "    'f1_weighted': 0.47270082757377185,\n",
       "    'loss': 1.6640915069593636,\n",
       "    'train_loss': 1.7888686908767102}],\n",
       "  'hyperparams': {'C': 0.07,\n",
       "   'E': 1,\n",
       "   'K': 143,\n",
       "   'batch_size': 10,\n",
       "   'lr': 0.8,\n",
       "   'mu': 0.001,\n",
       "   'percentage': 0,\n",
       "   'rounds': 50,\n",
       "   'target_accuracy': 101},\n",
       "  'test_accuracy': [[0.23002088813244623,\n",
       "    0.27796688844190004,\n",
       "    0.3155461859817422,\n",
       "    0.33190855639795763,\n",
       "    0.34736190623549434,\n",
       "    0.3560459538913817,\n",
       "    0.36888828717313943,\n",
       "    0.37761101655577906,\n",
       "    0.38325854866161224,\n",
       "    0.39298700293981126,\n",
       "    0.40321831966578986,\n",
       "    0.41157357264428285,\n",
       "    0.4134496363917685,\n",
       "    0.4189811233173449,\n",
       "    0.42534426736809533,\n",
       "    0.4274717623394708,\n",
       "    0.4354595389138171,\n",
       "    0.4398112331734489,\n",
       "    0.44395017793594305,\n",
       "    0.44663855794522667,\n",
       "    0.45052607148383106,\n",
       "    0.4565797617205632,\n",
       "    0.4558254680488937,\n",
       "    0.4577015317963794,\n",
       "    0.4628655423178091,\n",
       "    0.4665983289494043,\n",
       "    0.46657898808602816,\n",
       "    0.47114343184279744,\n",
       "    0.47189772551446696,\n",
       "    0.47503094538140184,\n",
       "    0.47814482438496053,\n",
       "    0.47961473000154725,\n",
       "    0.48048506885347364,\n",
       "    0.48031100108308833,\n",
       "    0.485881169735417,\n",
       "    0.48568776110165557,\n",
       "    0.48648073650007734,\n",
       "    0.48907241219248027,\n",
       "    0.49036825003868173,\n",
       "    0.4925344267368095,\n",
       "    0.4935014699056166,\n",
       "    0.4952808293362216,\n",
       "    0.49965186445922943,\n",
       "    0.49777580071174377,\n",
       "    0.500270772087266,\n",
       "    0.5003674764041467,\n",
       "    0.5034039919542008,\n",
       "    0.5046418072102739,\n",
       "    0.5053767600185672,\n",
       "    0.5076396410335757],\n",
       "   [0.24181881479189232,\n",
       "    0.27595543865078137,\n",
       "    0.3108076744545877,\n",
       "    0.33272087265975553,\n",
       "    0.34678168033421014,\n",
       "    0.36391768528547114,\n",
       "    0.3649814327711589,\n",
       "    0.37590902057867864,\n",
       "    0.3844190004641807,\n",
       "    0.3939927278353706,\n",
       "    0.39488240755067305,\n",
       "    0.4052297694569086,\n",
       "    0.41261797926659444,\n",
       "    0.41433931610707103,\n",
       "    0.4174145133838775,\n",
       "    0.4240871112486461,\n",
       "    0.42874825932229615,\n",
       "    0.4304889370261488,\n",
       "    0.4420741141884574,\n",
       "    0.44325390685440197,\n",
       "    0.44567151477641964,\n",
       "    0.44592294600030946,\n",
       "    0.44973309608540923,\n",
       "    0.45547733250812317,\n",
       "    0.45584480891226986,\n",
       "    0.45797230388364535,\n",
       "    0.46381324462324,\n",
       "    0.4630202692248182,\n",
       "    0.4662695342720099,\n",
       "    0.4679521893857342,\n",
       "    0.4695381401825777,\n",
       "    0.47396719789571407,\n",
       "    0.4754757852390531,\n",
       "    0.4772744855330342,\n",
       "    0.47936329877765743,\n",
       "    0.4807945226674919,\n",
       "    0.4853202846975089,\n",
       "    0.484507968435711,\n",
       "    0.48580380628191244,\n",
       "    0.4915673835680025,\n",
       "    0.4893238434163701,\n",
       "    0.4911612254371035,\n",
       "    0.4924183815565527,\n",
       "    0.4946425808448089,\n",
       "    0.496712053226056,\n",
       "    0.5006575893547888,\n",
       "    0.5011217700758162,\n",
       "    0.4997679096394863,\n",
       "    0.5018760637474857,\n",
       "    0.5045451028933932]],\n",
       "  'test_loss': [[2.9893888831876407,\n",
       "    2.666373202610953,\n",
       "    2.5093151292997526,\n",
       "    2.3988300242070606,\n",
       "    2.325429178655618,\n",
       "    2.2734840174457633,\n",
       "    2.2255187965381924,\n",
       "    2.1872282108387378,\n",
       "    2.1473685773269655,\n",
       "    2.112057555322213,\n",
       "    2.082267129457432,\n",
       "    2.058347548552822,\n",
       "    2.032891767585119,\n",
       "    2.0153886784186823,\n",
       "    1.9941444113175049,\n",
       "    1.9763137455773154,\n",
       "    1.9531385228412497,\n",
       "    1.9405155103459422,\n",
       "    1.9237798164043425,\n",
       "    1.9068576906994044,\n",
       "    1.891835618510545,\n",
       "    1.87787279984841,\n",
       "    1.8634812449320226,\n",
       "    1.8605854780164348,\n",
       "    1.841240055545526,\n",
       "    1.828770027769856,\n",
       "    1.8175845099017935,\n",
       "    1.8051576038593988,\n",
       "    1.7980856137016177,\n",
       "    1.7884705222130937,\n",
       "    1.7760124924072038,\n",
       "    1.7700328479541008,\n",
       "    1.7626814307633722,\n",
       "    1.7577037372377502,\n",
       "    1.7486625774759923,\n",
       "    1.74350697974427,\n",
       "    1.7319171229533306,\n",
       "    1.7224254963414303,\n",
       "    1.7215785103170345,\n",
       "    1.7118441840671503,\n",
       "    1.7063879982506216,\n",
       "    1.7015300457108302,\n",
       "    1.6933978584615343,\n",
       "    1.6893221527261943,\n",
       "    1.6859828720179648,\n",
       "    1.6834110935108684,\n",
       "    1.6716777466729793,\n",
       "    1.6682243623691373,\n",
       "    1.665663106116545,\n",
       "    1.6573140605737713],\n",
       "   [2.8704062371214865,\n",
       "    2.615885275188253,\n",
       "    2.476736996816847,\n",
       "    2.3935318237005436,\n",
       "    2.3251692064863856,\n",
       "    2.274956694549218,\n",
       "    2.2288321158879683,\n",
       "    2.1837221009432857,\n",
       "    2.154629263114435,\n",
       "    2.1209926644973334,\n",
       "    2.1017009887591285,\n",
       "    2.0686070624126214,\n",
       "    2.0422536000452616,\n",
       "    2.0217699961716336,\n",
       "    2.005111342084753,\n",
       "    1.989066962661683,\n",
       "    1.963774728767877,\n",
       "    1.9531482908561764,\n",
       "    1.9286202904509628,\n",
       "    1.9166624559948093,\n",
       "    1.9014465526263955,\n",
       "    1.8970308748905194,\n",
       "    1.8810597746859952,\n",
       "    1.86315049018014,\n",
       "    1.8572389807148042,\n",
       "    1.8442246364828752,\n",
       "    1.8298262302653812,\n",
       "    1.8266439208826857,\n",
       "    1.8109416080246525,\n",
       "    1.8008708183990991,\n",
       "    1.7953047581361705,\n",
       "    1.7811402343876421,\n",
       "    1.7753249212301738,\n",
       "    1.770527926928005,\n",
       "    1.7573459124278767,\n",
       "    1.757931219643573,\n",
       "    1.7413820005318117,\n",
       "    1.740199619299226,\n",
       "    1.7336851670455034,\n",
       "    1.7222032833307792,\n",
       "    1.7233503760636526,\n",
       "    1.7134182807758425,\n",
       "    1.706000501757138,\n",
       "    1.6997894279159629,\n",
       "    1.692543137577062,\n",
       "    1.6861111290203,\n",
       "    1.681510297547834,\n",
       "    1.6795614582715426,\n",
       "    1.6761860625656115,\n",
       "    1.6640915069593636]],\n",
       "  'train_loss': [[3.254269126943863,\n",
       "    2.911787488582553,\n",
       "    2.694241515185328,\n",
       "    2.582521746968168,\n",
       "    2.502881781087047,\n",
       "    2.432891350832058,\n",
       "    2.3943186212459873,\n",
       "    2.325828818171021,\n",
       "    2.30170030107884,\n",
       "    2.255214545275001,\n",
       "    2.231291260540403,\n",
       "    2.202643655567207,\n",
       "    2.170212338779915,\n",
       "    2.161473819242473,\n",
       "    2.1409148414435064,\n",
       "    2.1174457100293447,\n",
       "    2.1093136254023053,\n",
       "    2.0888625542240353,\n",
       "    2.0602841292797778,\n",
       "    2.0564280913537303,\n",
       "    2.0383588293404995,\n",
       "    2.015245838885815,\n",
       "    2.014166659932908,\n",
       "    1.9843717885338912,\n",
       "    1.986030347037282,\n",
       "    1.9707414953107656,\n",
       "    1.9488627899592719,\n",
       "    1.9350435306030849,\n",
       "    1.9260156690291186,\n",
       "    1.921390720391144,\n",
       "    1.9092282784935268,\n",
       "    1.8949274143830128,\n",
       "    1.8787555213777387,\n",
       "    1.8975187947424863,\n",
       "    1.8694748014762976,\n",
       "    1.8615987125377107,\n",
       "    1.8520926702133849,\n",
       "    1.847756600511849,\n",
       "    1.8344365318822333,\n",
       "    1.823797822783844,\n",
       "    1.8136308281603857,\n",
       "    1.8142799662946518,\n",
       "    1.8260057278585466,\n",
       "    1.7922531863774995,\n",
       "    1.8213126519305907,\n",
       "    1.7882613618230832,\n",
       "    1.7719939596446057,\n",
       "    1.7800872053533667,\n",
       "    1.7786342248918274,\n",
       "    1.7762819757146626],\n",
       "   [3.172292667652067,\n",
       "    2.8275224430350017,\n",
       "    2.6656465706571337,\n",
       "    2.5594668638089964,\n",
       "    2.503795595642277,\n",
       "    2.433573953714608,\n",
       "    2.384379834653925,\n",
       "    2.349919871736529,\n",
       "    2.3041554461005322,\n",
       "    2.259013009685533,\n",
       "    2.2509910819956707,\n",
       "    2.2247461487973217,\n",
       "    2.194709012114326,\n",
       "    2.16783016129071,\n",
       "    2.137752894692401,\n",
       "    2.128321695008301,\n",
       "    2.1257789585991853,\n",
       "    2.101194934744805,\n",
       "    2.074158604104036,\n",
       "    2.048362573635034,\n",
       "    2.034728294459364,\n",
       "    2.0257261932885497,\n",
       "    2.0190272134554834,\n",
       "    2.006747446762575,\n",
       "    1.998156090088982,\n",
       "    1.9796157358551754,\n",
       "    1.9643770977133372,\n",
       "    1.9612925607510128,\n",
       "    1.936425840636692,\n",
       "    1.9312426919552959,\n",
       "    1.925921692860984,\n",
       "    1.9101660262396556,\n",
       "    1.8969225642550864,\n",
       "    1.8996172329681422,\n",
       "    1.8941376614215606,\n",
       "    1.8833446032237546,\n",
       "    1.8852482819324297,\n",
       "    1.835488330056816,\n",
       "    1.8637072051915227,\n",
       "    1.8312147337589082,\n",
       "    1.8272460494813914,\n",
       "    1.8201890394318319,\n",
       "    1.8452389343068483,\n",
       "    1.8542960902055103,\n",
       "    1.8226716406241494,\n",
       "    1.8088645050098624,\n",
       "    1.8092308332646367,\n",
       "    1.7943997773103533,\n",
       "    1.8091744981984383,\n",
       "    1.7888686908767102]]},\n",
       " 'Shakespeare LSTM on Non IID': {'history': [{'accuracy': 0.18636025396948114,\n",
       "    'f1_macro': 0.004848508390875219,\n",
       "    'f1_weighted': 0.05863588116279558,\n",
       "    'loss': 3.10923396270465,\n",
       "    'train_loss': 3.5440204359964325},\n",
       "   {'accuracy': 0.26884235144041674,\n",
       "    'f1_macro': 0.025101967001391858,\n",
       "    'f1_weighted': 0.1661284349011997,\n",
       "    'loss': 2.6812815754237365,\n",
       "    'train_loss': 2.9497504286877376},\n",
       "   {'accuracy': 0.33010234158315377,\n",
       "    'f1_macro': 0.05407080706423348,\n",
       "    'f1_weighted': 0.25258357693384537,\n",
       "    'loss': 2.4127368387203694,\n",
       "    'train_loss': 2.5937478454196436},\n",
       "   {'accuracy': 0.3492660204683166,\n",
       "    'f1_macro': 0.0689855452282579,\n",
       "    'f1_weighted': 0.28984300459712004,\n",
       "    'loss': 2.309405872613681,\n",
       "    'train_loss': 2.486671283665883},\n",
       "   {'accuracy': 0.3713385583269712,\n",
       "    'f1_macro': 0.0821898880937195,\n",
       "    'f1_weighted': 0.31532989628374763,\n",
       "    'loss': 2.2341616233242125,\n",
       "    'train_loss': 2.3703399602454382},\n",
       "   {'accuracy': 0.37907940895078135,\n",
       "    'f1_macro': 0.08708253341775092,\n",
       "    'f1_weighted': 0.32399447364174694,\n",
       "    'loss': 2.1936219234173566,\n",
       "    'train_loss': 2.2874656147210852},\n",
       "   {'accuracy': 0.3942421987494806,\n",
       "    'f1_macro': 0.09508772859870784,\n",
       "    'f1_weighted': 0.33850487270383717,\n",
       "    'loss': 2.1197235635697464,\n",
       "    'train_loss': 2.2886956684751274},\n",
       "   {'accuracy': 0.40153850614146136,\n",
       "    'f1_macro': 0.09850749689273917,\n",
       "    'f1_weighted': 0.3498159661006828,\n",
       "    'loss': 2.0803552541695276,\n",
       "    'train_loss': 2.2483179849560484},\n",
       "   {'accuracy': 0.4063028499086754,\n",
       "    'f1_macro': 0.10320810564091432,\n",
       "    'f1_weighted': 0.3505937124448403,\n",
       "    'loss': 2.0480098166917773,\n",
       "    'train_loss': 2.15011711698749},\n",
       "   {'accuracy': 0.41800593368574657,\n",
       "    'f1_macro': 0.11115885511112975,\n",
       "    'f1_weighted': 0.37193509030108585,\n",
       "    'loss': 2.010669727633817,\n",
       "    'train_loss': 2.109405583223244},\n",
       "   {'accuracy': 0.4219585028557071,\n",
       "    'f1_macro': 0.12030781670029467,\n",
       "    'f1_weighted': 0.3730138534031221,\n",
       "    'loss': 1.990156887921503,\n",
       "    'train_loss': 2.026592873037057},\n",
       "   {'accuracy': 0.4279211805522,\n",
       "    'f1_macro': 0.1249219541886903,\n",
       "    'f1_weighted': 0.3844067385818099,\n",
       "    'loss': 1.9724928016174226,\n",
       "    'train_loss': 1.9915013670344077},\n",
       "   {'accuracy': 0.4335746107830726,\n",
       "    'f1_macro': 0.13038789564487174,\n",
       "    'f1_weighted': 0.39274515397241067,\n",
       "    'loss': 1.9531191464456694,\n",
       "    'train_loss': 2.0489809994898858},\n",
       "   {'accuracy': 0.43731457232041904,\n",
       "    'f1_macro': 0.13377965018193938,\n",
       "    'f1_weighted': 0.3946059343770261,\n",
       "    'loss': 1.9252190216379963,\n",
       "    'train_loss': 2.0423538850429774},\n",
       "   {'accuracy': 0.4415667249726992,\n",
       "    'f1_macro': 0.13472115533364695,\n",
       "    'f1_weighted': 0.39557557156612966,\n",
       "    'loss': 1.9149071472238604,\n",
       "    'train_loss': 1.8149736811799335},\n",
       "   {'accuracy': 0.4469978835876572,\n",
       "    'f1_macro': 0.13942200298316534,\n",
       "    'f1_weighted': 0.405737522099204,\n",
       "    'loss': 1.890684243559645,\n",
       "    'train_loss': 1.9471903585592838},\n",
       "   {'accuracy': 0.4498970785778482,\n",
       "    'f1_macro': 0.1447986522956187,\n",
       "    'f1_weighted': 0.4114072318919806,\n",
       "    'loss': 1.8803798986204512,\n",
       "    'train_loss': 2.0189883469035834},\n",
       "   {'accuracy': 0.4535307362988877,\n",
       "    'f1_macro': 0.14718739243825435,\n",
       "    'f1_weighted': 0.4162960279610374,\n",
       "    'loss': 1.8703668000130667,\n",
       "    'train_loss': 1.9814801081461184},\n",
       "   {'accuracy': 0.4564299312890787,\n",
       "    'f1_macro': 0.14978890174556753,\n",
       "    'f1_weighted': 0.4211064588610653,\n",
       "    'loss': 1.856158704525532,\n",
       "    'train_loss': 1.8837528221834838},\n",
       "   {'accuracy': 0.4578988567507755,\n",
       "    'f1_macro': 0.14663925205124187,\n",
       "    'f1_weighted': 0.4207013654985414,\n",
       "    'loss': 1.8458820506818747,\n",
       "    'train_loss': 1.950619160510249},\n",
       "   {'accuracy': 0.4654464277085729,\n",
       "    'f1_macro': 0.1563964256800262,\n",
       "    'f1_weighted': 0.43030058517724284,\n",
       "    'loss': 1.8300436361029317,\n",
       "    'train_loss': 1.9189855046013424},\n",
       "   {'accuracy': 0.4653401238922659,\n",
       "    'f1_macro': 0.15549615323000876,\n",
       "    'f1_weighted': 0.43042182650577454,\n",
       "    'loss': 1.8200393519175337,\n",
       "    'train_loss': 1.838511025507286},\n",
       "   {'accuracy': 0.4668573692704659,\n",
       "    'f1_macro': 0.15780553555647167,\n",
       "    'f1_weighted': 0.43398310603569107,\n",
       "    'loss': 1.8141032549917824,\n",
       "    'train_loss': 1.8885447716982995},\n",
       "   {'accuracy': 0.4702107714757869,\n",
       "    'f1_macro': 0.16229094676420744,\n",
       "    'f1_weighted': 0.4356570666043776,\n",
       "    'loss': 1.8030459614339756,\n",
       "    'train_loss': 1.8489926389949105},\n",
       "   {'accuracy': 0.4711288498893474,\n",
       "    'f1_macro': 0.1661767714770829,\n",
       "    'f1_weighted': 0.442921497095033,\n",
       "    'loss': 1.7946839369100127,\n",
       "    'train_loss': 1.7894482773370608},\n",
       "   {'accuracy': 0.47220155203571806,\n",
       "    'f1_macro': 0.16757350900891715,\n",
       "    'f1_weighted': 0.44190334223517164,\n",
       "    'loss': 1.7907534420685536,\n",
       "    'train_loss': 1.840963652503777},\n",
       "   {'accuracy': 0.47425998047875373,\n",
       "    'f1_macro': 0.1611550311183865,\n",
       "    'f1_weighted': 0.43589832678636575,\n",
       "    'loss': 1.7841372958381836,\n",
       "    'train_loss': 1.7995434325491346},\n",
       "   {'accuracy': 0.477739014466983,\n",
       "    'f1_macro': 0.16624582775812305,\n",
       "    'f1_weighted': 0.4440336858225567,\n",
       "    'loss': 1.775691216668703,\n",
       "    'train_loss': 1.8444481396460222},\n",
       "   {'accuracy': 0.479043652212569,\n",
       "    'f1_macro': 0.1682966688877727,\n",
       "    'f1_weighted': 0.44751705311416834,\n",
       "    'loss': 1.7662756157203405,\n",
       "    'train_loss': 1.8512819002444652},\n",
       "   {'accuracy': 0.4808314891231868,\n",
       "    'f1_macro': 0.16976589911099746,\n",
       "    'f1_weighted': 0.44724706420236704,\n",
       "    'loss': 1.7644402608950074,\n",
       "    'train_loss': 1.756023104394942},\n",
       "   {'accuracy': 0.4795655073108034,\n",
       "    'f1_macro': 0.16361198528024382,\n",
       "    'f1_weighted': 0.4473027561341899,\n",
       "    'loss': 1.762217802922236,\n",
       "    'train_loss': 1.8015751369688904},\n",
       "   {'accuracy': 0.4809088009895919,\n",
       "    'f1_macro': 0.17182310117546395,\n",
       "    'f1_weighted': 0.45234162213260254,\n",
       "    'loss': 1.7576428766817505,\n",
       "    'train_loss': 1.6319595003729443},\n",
       "   {'accuracy': 0.4821651188186747,\n",
       "    'f1_macro': 0.17143740675953123,\n",
       "    'f1_weighted': 0.4501899969736916,\n",
       "    'loss': 1.7497519560054786,\n",
       "    'train_loss': 1.755410917745077},\n",
       "   {'accuracy': 0.48425253921161227,\n",
       "    'f1_macro': 0.17310957816592162,\n",
       "    'f1_weighted': 0.4518122161236716,\n",
       "    'loss': 1.742335249298957,\n",
       "    'train_loss': 1.7811311839137616},\n",
       "   {'accuracy': 0.48784754099944916,\n",
       "    'f1_macro': 0.17369172893953438,\n",
       "    'f1_weighted': 0.4544475040290025,\n",
       "    'loss': 1.736763262960232,\n",
       "    'train_loss': 1.7268703626190003},\n",
       "   {'accuracy': 0.48792485286585424,\n",
       "    'f1_macro': 0.17249703291768767,\n",
       "    'f1_weighted': 0.4540024256814943,\n",
       "    'loss': 1.73216734461902,\n",
       "    'train_loss': 1.5315343699164563},\n",
       "   {'accuracy': 0.48873662746310775,\n",
       "    'f1_macro': 0.17226678064063497,\n",
       "    'f1_weighted': 0.4566107516012645,\n",
       "    'loss': 1.728480310520107,\n",
       "    'train_loss': 1.8343958469694208},\n",
       "   {'accuracy': 0.48658155918706575,\n",
       "    'f1_macro': 0.17331617675830344,\n",
       "    'f1_weighted': 0.4560309688901734,\n",
       "    'loss': 1.7284944484939295,\n",
       "    'train_loss': 1.7981986614376964},\n",
       "   {'accuracy': 0.4906017762401307,\n",
       "    'f1_macro': 0.17835930086673835,\n",
       "    'f1_weighted': 0.457497531286531,\n",
       "    'loss': 1.722924658479542,\n",
       "    'train_loss': 1.7127596386009132},\n",
       "   {'accuracy': 0.49209002966842874,\n",
       "    'f1_macro': 0.1772226355425187,\n",
       "    'f1_weighted': 0.4568730113979704,\n",
       "    'loss': 1.7142338025604131,\n",
       "    'train_loss': 1.7989872048815616},\n",
       "   {'accuracy': 0.49311441189829625,\n",
       "    'f1_macro': 0.17402911300587318,\n",
       "    'f1_weighted': 0.4580551161663166,\n",
       "    'loss': 1.7118717872295917,\n",
       "    'train_loss': 1.7605454150739004},\n",
       "   {'accuracy': 0.4918387661026122,\n",
       "    'f1_macro': 0.1758348924067924,\n",
       "    'f1_weighted': 0.45395880292118507,\n",
       "    'loss': 1.7099766875001812,\n",
       "    'train_loss': 1.8208925770716893},\n",
       "   {'accuracy': 0.497192612851165,\n",
       "    'f1_macro': 0.18596383971387515,\n",
       "    'f1_weighted': 0.4664152004092586,\n",
       "    'loss': 1.695109606056457,\n",
       "    'train_loss': 1.7108308842141966},\n",
       "   {'accuracy': 0.4973665645505765,\n",
       "    'f1_macro': 0.18492547959147507,\n",
       "    'f1_weighted': 0.46783316764048893,\n",
       "    'loss': 1.6901007817028555,\n",
       "    'train_loss': 1.6583365453669625},\n",
       "   {'accuracy': 0.4976854759994975,\n",
       "    'f1_macro': 0.185426252349389,\n",
       "    'f1_weighted': 0.468434229502382,\n",
       "    'loss': 1.6895179275474375,\n",
       "    'train_loss': 1.710585381341462},\n",
       "   {'accuracy': 0.49974390444253314,\n",
       "    'f1_macro': 0.1896010545711354,\n",
       "    'f1_weighted': 0.46867798622165474,\n",
       "    'loss': 1.6789198379272738,\n",
       "    'train_loss': 1.6945545087368168},\n",
       "   {'accuracy': 0.5015220773698503,\n",
       "    'f1_macro': 0.194247009825827,\n",
       "    'f1_weighted': 0.4740070735297489,\n",
       "    'loss': 1.6775526522700717,\n",
       "    'train_loss': 1.7379931067564272},\n",
       "   {'accuracy': 0.5024788117166133,\n",
       "    'f1_macro': 0.19376126194025325,\n",
       "    'f1_weighted': 0.4729415891785769,\n",
       "    'loss': 1.6735823360779214,\n",
       "    'train_loss': 1.7292132685692008},\n",
       "   {'accuracy': 0.5022372121340974,\n",
       "    'f1_macro': 0.1966168719121384,\n",
       "    'f1_weighted': 0.475613098919467,\n",
       "    'loss': 1.6688596427279934,\n",
       "    'train_loss': 1.7216665887561529},\n",
       "   {'accuracy': 0.5058032219720324,\n",
       "    'f1_macro': 0.19866158075978363,\n",
       "    'f1_weighted': 0.47873684862221455,\n",
       "    'loss': 1.6614504363038072,\n",
       "    'train_loss': 1.7422268285870435}],\n",
       "  'hyperparams': {'C': 0.07,\n",
       "   'E': 1,\n",
       "   'K': 143,\n",
       "   'batch_size': 10,\n",
       "   'lr': 0.8,\n",
       "   'mu': 0.001,\n",
       "   'percentage': 0,\n",
       "   'rounds': 50,\n",
       "   'target_accuracy': 101},\n",
       "  'test_accuracy': [[0.1863215980362786,\n",
       "    0.1880514510470926,\n",
       "    0.2783903669414459,\n",
       "    0.3341998705026238,\n",
       "    0.3558182011461484,\n",
       "    0.37682770084173295,\n",
       "    0.38689757144099657,\n",
       "    0.39138165969249206,\n",
       "    0.39930612599901427,\n",
       "    0.4077331194371696,\n",
       "    0.408254974535404,\n",
       "    0.4168269277230689,\n",
       "    0.42077949689302935,\n",
       "    0.42874261913275413,\n",
       "    0.431380886573828,\n",
       "    0.437981387168163,\n",
       "    0.43917972109744197,\n",
       "    0.44455289581259605,\n",
       "    0.44705586748746096,\n",
       "    0.45027397392657303,\n",
       "    0.4524000502527132,\n",
       "    0.45258366593542526,\n",
       "    0.4579085207340762,\n",
       "    0.4610106593735806,\n",
       "    0.4641224619963857,\n",
       "    0.46547541965847483,\n",
       "    0.4698725320602646,\n",
       "    0.4719696164365028,\n",
       "    0.47195995245320216,\n",
       "    0.473960396996434,\n",
       "    0.4775070788677677,\n",
       "    0.47675328817031803,\n",
       "    0.480000386559332,\n",
       "    0.4814403200711269,\n",
       "    0.48160460778723774,\n",
       "    0.48481305024304916,\n",
       "    0.48319916503184285,\n",
       "    0.48675551088647717,\n",
       "    0.48668786300337274,\n",
       "    0.4879151888825536,\n",
       "    0.49028286479120964,\n",
       "    0.4923123012843434,\n",
       "    0.4935106352136224,\n",
       "    0.49445770557708474,\n",
       "    0.49392618649554976,\n",
       "    0.49529847212424016,\n",
       "    0.4973279086173739,\n",
       "    0.49735690056727583,\n",
       "    0.4980913632981242,\n",
       "    0.4976081641330924],\n",
       "   [0.18636025396948114,\n",
       "    0.26884235144041674,\n",
       "    0.33010234158315377,\n",
       "    0.3492660204683166,\n",
       "    0.3713385583269712,\n",
       "    0.37907940895078135,\n",
       "    0.3942421987494806,\n",
       "    0.40153850614146136,\n",
       "    0.4063028499086754,\n",
       "    0.41800593368574657,\n",
       "    0.4219585028557071,\n",
       "    0.4279211805522,\n",
       "    0.4335746107830726,\n",
       "    0.43731457232041904,\n",
       "    0.4415667249726992,\n",
       "    0.4469978835876572,\n",
       "    0.4498970785778482,\n",
       "    0.4535307362988877,\n",
       "    0.4564299312890787,\n",
       "    0.4578988567507755,\n",
       "    0.4654464277085729,\n",
       "    0.4653401238922659,\n",
       "    0.4668573692704659,\n",
       "    0.4702107714757869,\n",
       "    0.4711288498893474,\n",
       "    0.47220155203571806,\n",
       "    0.47425998047875373,\n",
       "    0.477739014466983,\n",
       "    0.479043652212569,\n",
       "    0.4808314891231868,\n",
       "    0.4795655073108034,\n",
       "    0.4809088009895919,\n",
       "    0.4821651188186747,\n",
       "    0.48425253921161227,\n",
       "    0.48784754099944916,\n",
       "    0.48792485286585424,\n",
       "    0.48873662746310775,\n",
       "    0.48658155918706575,\n",
       "    0.4906017762401307,\n",
       "    0.49209002966842874,\n",
       "    0.49311441189829625,\n",
       "    0.4918387661026122,\n",
       "    0.497192612851165,\n",
       "    0.4973665645505765,\n",
       "    0.4976854759994975,\n",
       "    0.49974390444253314,\n",
       "    0.5015220773698503,\n",
       "    0.5024788117166133,\n",
       "    0.5022372121340974,\n",
       "    0.5058032219720324]],\n",
       "  'test_loss': [[3.357488761456045,\n",
       "    3.1105874106343396,\n",
       "    2.635205766899103,\n",
       "    2.3717757325176247,\n",
       "    2.286352204466838,\n",
       "    2.192308742100497,\n",
       "    2.161374158277336,\n",
       "    2.1199622682640458,\n",
       "    2.0903706466614675,\n",
       "    2.061416667944103,\n",
       "    2.0331607118423327,\n",
       "    2.011370194306543,\n",
       "    1.9991117979755633,\n",
       "    1.9749722269893628,\n",
       "    1.961618544633122,\n",
       "    1.9388154634218409,\n",
       "    1.9309396549253037,\n",
       "    1.9094232926480277,\n",
       "    1.8974677700918843,\n",
       "    1.8876795971795761,\n",
       "    1.8764076322999417,\n",
       "    1.8685221304592996,\n",
       "    1.84884320816938,\n",
       "    1.8358429800778964,\n",
       "    1.8213554641698344,\n",
       "    1.81559377107062,\n",
       "    1.8060049201410013,\n",
       "    1.7958758917892337,\n",
       "    1.7880944510964336,\n",
       "    1.7816446124248755,\n",
       "    1.7721705922681656,\n",
       "    1.7689205326186512,\n",
       "    1.7557099813473855,\n",
       "    1.7526906652364482,\n",
       "    1.7512228547376456,\n",
       "    1.7450455611518694,\n",
       "    1.7432866099203868,\n",
       "    1.7371169939199842,\n",
       "    1.732441202956233,\n",
       "    1.7230954991122238,\n",
       "    1.7204351154442301,\n",
       "    1.716168543273059,\n",
       "    1.7115048605697796,\n",
       "    1.7098781998131674,\n",
       "    1.7062304536639428,\n",
       "    1.7057139106927977,\n",
       "    1.700511057787194,\n",
       "    1.696955334009274,\n",
       "    1.689370382768383,\n",
       "    1.6879497294251915],\n",
       "   [3.10923396270465,\n",
       "    2.6812815754237365,\n",
       "    2.4127368387203694,\n",
       "    2.309405872613681,\n",
       "    2.2341616233242125,\n",
       "    2.1936219234173566,\n",
       "    2.1197235635697464,\n",
       "    2.0803552541695276,\n",
       "    2.0480098166917773,\n",
       "    2.010669727633817,\n",
       "    1.990156887921503,\n",
       "    1.9724928016174226,\n",
       "    1.9531191464456694,\n",
       "    1.9252190216379963,\n",
       "    1.9149071472238604,\n",
       "    1.890684243559645,\n",
       "    1.8803798986204512,\n",
       "    1.8703668000130667,\n",
       "    1.856158704525532,\n",
       "    1.8458820506818747,\n",
       "    1.8300436361029317,\n",
       "    1.8200393519175337,\n",
       "    1.8141032549917824,\n",
       "    1.8030459614339756,\n",
       "    1.7946839369100127,\n",
       "    1.7907534420685536,\n",
       "    1.7841372958381836,\n",
       "    1.775691216668703,\n",
       "    1.7662756157203405,\n",
       "    1.7644402608950074,\n",
       "    1.762217802922236,\n",
       "    1.7576428766817505,\n",
       "    1.7497519560054786,\n",
       "    1.742335249298957,\n",
       "    1.736763262960232,\n",
       "    1.73216734461902,\n",
       "    1.728480310520107,\n",
       "    1.7284944484939295,\n",
       "    1.722924658479542,\n",
       "    1.7142338025604131,\n",
       "    1.7118717872295917,\n",
       "    1.7099766875001812,\n",
       "    1.695109606056457,\n",
       "    1.6901007817028555,\n",
       "    1.6895179275474375,\n",
       "    1.6789198379272738,\n",
       "    1.6775526522700717,\n",
       "    1.6735823360779214,\n",
       "    1.6688596427279934,\n",
       "    1.6614504363038072]],\n",
       "  'train_loss': [[3.77189796320026,\n",
       "    3.241141840251173,\n",
       "    2.8923726276095536,\n",
       "    2.640070908848004,\n",
       "    2.416065218345138,\n",
       "    2.5339733722133997,\n",
       "    2.3280480911193386,\n",
       "    2.2969020680435035,\n",
       "    2.1761648028048883,\n",
       "    2.138402867031585,\n",
       "    2.167856065882126,\n",
       "    2.104806092096715,\n",
       "    2.2582983134138686,\n",
       "    2.077307729155371,\n",
       "    2.066270772116164,\n",
       "    2.037389644740474,\n",
       "    2.0799570848910838,\n",
       "    2.0473155172588537,\n",
       "    1.9481320071086774,\n",
       "    1.9512165238620696,\n",
       "    1.9549961204012498,\n",
       "    1.910813242158215,\n",
       "    1.920531592510771,\n",
       "    1.9975245161417987,\n",
       "    1.8549913635105706,\n",
       "    1.8590979361211364,\n",
       "    1.881150795500448,\n",
       "    2.019970044300867,\n",
       "    1.9257494577082916,\n",
       "    1.8063023523454542,\n",
       "    1.7757571038944089,\n",
       "    1.7735272138998883,\n",
       "    1.6790374510411703,\n",
       "    1.7335151320017743,\n",
       "    1.6127881835103335,\n",
       "    1.7568725109750367,\n",
       "    1.804862319706297,\n",
       "    1.7714178468301465,\n",
       "    1.7669099066669929,\n",
       "    1.7694402210098281,\n",
       "    1.7695857070989778,\n",
       "    1.7503522378688359,\n",
       "    1.7971423116340373,\n",
       "    1.7953805343357403,\n",
       "    1.8254442900328887,\n",
       "    1.6889330438011907,\n",
       "    1.6812283495006237,\n",
       "    1.7327442131721829,\n",
       "    1.7344053252339275,\n",
       "    1.7550825169280326],\n",
       "   [3.5440204359964325,\n",
       "    2.9497504286877376,\n",
       "    2.5937478454196436,\n",
       "    2.486671283665883,\n",
       "    2.3703399602454382,\n",
       "    2.2874656147210852,\n",
       "    2.2886956684751274,\n",
       "    2.2483179849560484,\n",
       "    2.15011711698749,\n",
       "    2.109405583223244,\n",
       "    2.026592873037057,\n",
       "    1.9915013670344077,\n",
       "    2.0489809994898858,\n",
       "    2.0423538850429774,\n",
       "    1.8149736811799335,\n",
       "    1.9471903585592838,\n",
       "    2.0189883469035834,\n",
       "    1.9814801081461184,\n",
       "    1.8837528221834838,\n",
       "    1.950619160510249,\n",
       "    1.9189855046013424,\n",
       "    1.838511025507286,\n",
       "    1.8885447716982995,\n",
       "    1.8489926389949105,\n",
       "    1.7894482773370608,\n",
       "    1.840963652503777,\n",
       "    1.7995434325491346,\n",
       "    1.8444481396460222,\n",
       "    1.8512819002444652,\n",
       "    1.756023104394942,\n",
       "    1.8015751369688904,\n",
       "    1.6319595003729443,\n",
       "    1.755410917745077,\n",
       "    1.7811311839137616,\n",
       "    1.7268703626190003,\n",
       "    1.5315343699164563,\n",
       "    1.8343958469694208,\n",
       "    1.7981986614376964,\n",
       "    1.7127596386009132,\n",
       "    1.7989872048815616,\n",
       "    1.7605454150739004,\n",
       "    1.8208925770716893,\n",
       "    1.7108308842141966,\n",
       "    1.6583365453669625,\n",
       "    1.710585381341462,\n",
       "    1.6945545087368168,\n",
       "    1.7379931067564272,\n",
       "    1.7292132685692008,\n",
       "    1.7216665887561529,\n",
       "    1.7422268285870435]]}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "aSs9xXfpQ3ML"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(path + 'Local_Round_FedAvg_1.pkl', 'rb') as file:\n",
    "  log_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "N8ep-MalQ3PO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5076396410335757, 0.5045451028933932]\n"
     ]
    }
   ],
   "source": [
    "print([test_acc[-1] for test_acc in log_dict['Shakespeare LSTM on IID']['test_accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_-2I2g0HQ3WA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4976081641330924, 0.5058032219720324]\n"
     ]
    }
   ],
   "source": [
    "print([test_acc[-1] for test_acc in log_dict['Shakespeare LSTM on Non IID']['test_accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfR3lDa7Q-8s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "FederatedAveraging",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "QSGD",
   "language": "python",
   "name": "qsgd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
