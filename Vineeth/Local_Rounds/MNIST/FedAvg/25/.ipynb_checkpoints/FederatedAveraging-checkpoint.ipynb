{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tushar-semwal/fedperf/blob/main/Vineeth/Local_Rounds/FedAvg/FederatedAveraging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yKcpjZLrQQJV",
    "outputId": "5fca5a43-8803-4d05-99e7-310c60b2eb17"
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    import os\n",
    "    path = '/content/drive/MyDrive/Colab Notebooks/OpenMined/'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "except:\n",
    "    path = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0_nKpfq2h1R"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLLNM9X2JbQ8",
    "outputId": "c88c97b3-b806-4e29-f4cd-02a6743f4f13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 12 23:24:05 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 17%   32C    P0    54W / 250W |      0MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "| 17%   32C    P0    56W / 250W |      0MiB / 11178MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "# Check assigned GPU\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "\n",
    "# set manual seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# general reproducibility\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# gpu training specific\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY4eWzGiL6Mj"
   },
   "source": [
    "## Load the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "G649tjTXLL8F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to ../data/mnist/MNIST/raw\n",
      "Using downloaded and verified file: ../data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 503: Service Unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7295953b39d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmnist_data_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/mnist/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms_mnist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmnist_data_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/mnist/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms_mnist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# process and save as torch files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 )\n\u001b[1;32m     83\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;31m# check integrity of downloaded file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5)\u001b[0m\n\u001b[1;32m     70\u001b[0m             urllib.request.urlretrieve(\n\u001b[1;32m     71\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_bar_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             )\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 503: Service Unavailable"
     ]
    }
   ],
   "source": [
    "# create transforms\n",
    "# We will just convert to tensor and normalize since no special transforms are mentioned in the paper\n",
    "transforms_mnist = transforms.Compose([\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                       ])\n",
    "\n",
    "mnist_data_train = datasets.MNIST('./data/mnist/', train=True, download=True, transform=transforms_mnist)\n",
    "mnist_data_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=transforms_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dm9usjn2vFkL",
    "outputId": "a76539d2-3f37-4485-e5ac-7633e785c18c"
   },
   "outputs": [],
   "source": [
    "classes = np.array(list(mnist_data_train.class_to_idx.values()))\n",
    "classes_test = np.array(list(mnist_data_test.class_to_idx.values()))\n",
    "num_classes = len(classes_test)\n",
    "print(\"Classes: {} \\tType: {}\".format(classes, type(classes)))\n",
    "print(\"Classes Test: {} \\tType: {}\".format(classes_test, type(classes)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9lvJt3Ofv2SO",
    "outputId": "d3626726-b3da-426f-9712-7a7be680d33b"
   },
   "outputs": [],
   "source": [
    "print(\"Image Shape: {}\".format(mnist_data_train.data[0].size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCOONkg-zV7Y"
   },
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9MK03TZw6Qs"
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "\timg = img/2 + 0.5 #unnormalize the image\n",
    "\tplt.imshow(img, cmap='gray') # convert from tensor to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMJ0Kx4Kw-_H"
   },
   "outputs": [],
   "source": [
    "def visualize(dataset):\n",
    "  figure = plt.figure(figsize=(25,4))\n",
    "  for i in range(20):\n",
    "    axis = figure.add_subplot(2, 20/2, i+1, xticks=[], yticks=[])\n",
    "    data = dataset.data[i]\n",
    "    data = data.numpy()\n",
    "\n",
    "    target = dataset.targets[i]\n",
    "    target = target.numpy()\n",
    "    imshow(data)\n",
    "    axis.set_title(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "8bPOwKg10Ro7",
    "outputId": "52916fa2-c981-44e5-92a7-43886e91b271"
   },
   "outputs": [],
   "source": [
    "visualize(mnist_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "RKoh5Cf70UYu",
    "outputId": "91e5ef2a-5e61-454b-a4f3-c64ff266071a"
   },
   "outputs": [],
   "source": [
    "visualize(mnist_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctjRsETiO1qO"
   },
   "source": [
    "## Partitioning the Data (IID and non-IID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_v8lyrgO5dD"
   },
   "outputs": [],
   "source": [
    "def iid_partition(dataset, clients):\n",
    "  \"\"\"\n",
    "  I.I.D paritioning of data over clients\n",
    "  Shuffle the data\n",
    "  Split it between clients\n",
    "  \n",
    "  params:\n",
    "    - dataset (torch.utils.Dataset): Dataset containing the MNIST Images\n",
    "    - clients (int): Number of Clients to split the data between\n",
    "\n",
    "  returns:\n",
    "    - Dictionary of image indexes for each client\n",
    "  \"\"\"\n",
    "\n",
    "  num_items_per_client = int(len(dataset)/clients)\n",
    "  client_dict = {}\n",
    "  image_idxs = [i for i in range(len(dataset))]\n",
    "\n",
    "  for i in range(clients):\n",
    "    client_dict[i] = set(np.random.choice(image_idxs, num_items_per_client, replace=False))\n",
    "    image_idxs = list(set(image_idxs) - client_dict[i])\n",
    "\n",
    "  return client_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zMdliGMQoSl"
   },
   "outputs": [],
   "source": [
    "def non_iid_partition(dataset, clients, total_shards, shards_size, num_shards_per_client):\n",
    "  \"\"\"\n",
    "  non I.I.D parititioning of data over clients\n",
    "  Sort the data by the digit label\n",
    "  Divide the data into N shards of size S\n",
    "  Each of the clients will get X shards\n",
    "\n",
    "  params:\n",
    "    - dataset (torch.utils.Dataset): Dataset containing the MNIST Images\n",
    "    - clients (int): Number of Clients to split the data between\n",
    "    - total_shards (int): Number of shards to partition the data in\n",
    "    - shards_size (int): Size of each shard \n",
    "    - num_shards_per_client (int): Number of shards of size shards_size that each client receives\n",
    "\n",
    "  returns:\n",
    "    - Dictionary of image indexes for each client\n",
    "  \"\"\"\n",
    "  \n",
    "  shard_idxs = [i for i in range(total_shards)]\n",
    "  client_dict = {i: np.array([], dtype='int64') for i in range(clients)}\n",
    "  idxs = np.arange(len(dataset))\n",
    "  data_labels = dataset.targets.numpy()\n",
    "\n",
    "  # sort the labels\n",
    "  label_idxs = np.vstack((idxs, data_labels))\n",
    "  label_idxs = label_idxs[:, label_idxs[1,:].argsort()]\n",
    "  idxs = label_idxs[0,:]\n",
    "\n",
    "  # divide the data into total_shards of size shards_size\n",
    "  # assign num_shards_per_client to each client\n",
    "  for i in range(clients):\n",
    "    rand_set = set(np.random.choice(shard_idxs, num_shards_per_client, replace=False))\n",
    "    shard_idxs = list(set(shard_idxs) - rand_set)\n",
    "\n",
    "    for rand in rand_set:\n",
    "      client_dict[i] = np.concatenate((client_dict[i], idxs[rand*shards_size:(rand+1)*shards_size]), axis=0)\n",
    "  \n",
    "  return client_dict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTfxv8kFoGAy"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvoDNFKbZST5"
   },
   "outputs": [],
   "source": [
    "class MNIST_2NN(nn.Module):\n",
    "  \"\"\"\n",
    "  A simple multilayer-perceptron with 2-hidden layers with 200 units each\n",
    "  using ReLu activations\n",
    "\n",
    "  Total Expected Params: 199,210\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(MNIST_2NN, self).__init__()\n",
    "\n",
    "    self.fc1 = nn.Linear(28*28, 200)\n",
    "    self.fc2 = nn.Linear(200, 200)\n",
    "    self.fc3 = nn.Linear(200, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    out = self.fc3(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ut1hZ8x3qYPZ"
   },
   "outputs": [],
   "source": [
    "class MNIST_CNN(nn.Module):\n",
    "  \"\"\"\n",
    "  CNN with two 5x5 convolution lauers(the first with 32 channels, second with 64,\n",
    "  each followed with 2x2 max pooling), a fully connected layer with 512 uunits and \n",
    "  ReLu activation, and the final Softmax output layer\n",
    "\n",
    "  Total Expected Params: 1,663,370\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(MNIST_CNN, self).__init__()\n",
    "\n",
    "    self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "    \n",
    "    self.pool = nn.MaxPool2d(2,2)\n",
    "    self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    self.fc1 = nn.Linear(1024, 512)\n",
    "    self.out = nn.Linear(512, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    x = self.dropout(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.out(x)\n",
    "    out = F.log_softmax(x, dim=1)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVv4HA9HuLtr"
   },
   "source": [
    "### Print Model Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5oTH710sJEt",
    "outputId": "3e8f2eff-a1be-45a7-a297-5963b3b4e48d"
   },
   "outputs": [],
   "source": [
    "mnist_mlp = MNIST_2NN()\n",
    "mnist_cnn = MNIST_CNN()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  mnist_mlp.cuda()\n",
    "  mnist_cnn.cuda()\n",
    "\n",
    "print(\"MNIST MLP SUMMARY\")\n",
    "print(summary(mnist_mlp, (28,28)))\n",
    "\n",
    "print(\"\\nMNIST CNN SUMMARY\")\n",
    "print(summary(mnist_cnn, (1, 28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gf_8XEXa-gZ7"
   },
   "source": [
    "## Federated Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-aNdQsQ-Kvp"
   },
   "source": [
    "### Local Training (Client Update)\n",
    "\n",
    "Local training for the model on client side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oX6OsQyO-Gz7"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, dataset, idxs):\n",
    "      self.dataset = dataset\n",
    "      self.idxs = list(idxs)\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.idxs)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "      image, label = self.dataset[self.idxs[item]]\n",
    "      return image, label\n",
    "\n",
    "\n",
    "class ClientUpdate(object):\n",
    "  def __init__(self, dataset, batchSize, learning_rate, epochs, idxs):\n",
    "    self.train_loader = DataLoader(CustomDataset(dataset, idxs), batch_size=batchSize, shuffle=True)\n",
    "\n",
    "    self.learning_rate = learning_rate\n",
    "    self.epochs = epochs\n",
    "\n",
    "  def train(self, model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.5)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    e_loss = []\n",
    "    for epoch in range(1, self.epochs+1):\n",
    "\n",
    "      train_loss = 0.0\n",
    "      model.train()\n",
    "      for data, labels in self.train_loader:\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "          data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # make a forward pass\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, labels)\n",
    "        # do a backwards pass\n",
    "        loss.backward()\n",
    "        # perform a single optimization step\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "      # average losses\n",
    "      train_loss = train_loss/len(self.train_loader.dataset)\n",
    "      e_loss.append(train_loss)\n",
    "\n",
    "    total_loss = sum(e_loss)/len(e_loss)\n",
    "\n",
    "    return model.state_dict(), total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ukgh1DVHE2Ds"
   },
   "source": [
    "### Server Side Training\n",
    "\n",
    "Following Algorithm 1 from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NF1e33BgpeL"
   },
   "outputs": [],
   "source": [
    "def training(model, rounds, batch_size, lr, ds, ds_test, data_dict, C, K, E, plt_title, plt_color):\n",
    "  \"\"\"\n",
    "  Function implements the Federated Averaging Algorithm from the FedAvg paper.\n",
    "  Specifically, this function is used for the server side training and weight update\n",
    "\n",
    "  Params:\n",
    "    - model:           PyTorch model to train\n",
    "    - rounds:          Number of communication rounds for the client update\n",
    "    - batch_size:      Batch size for client update training\n",
    "    - lr:              Learning rate used for client update training\n",
    "    - ds:              Dataset used for training\n",
    "    - ds_test:         Dataset used for testing\n",
    "    - data_dict:       Type of data partition used for training (IID or non-IID)\n",
    "    - C:               Fraction of clients randomly chosen to perform computation on each round\n",
    "    - K:               Total number of clients\n",
    "    - E:               Number of training passes each client makes over its local dataset per round\n",
    "    - tb_writer_name:  Directory name to save the tensorboard logs\n",
    "  Returns:\n",
    "    - model:           Trained model on the server\n",
    "  \"\"\"\n",
    "\n",
    "  # global model weights\n",
    "  global_weights = model.state_dict()\n",
    "\n",
    "  # training loss\n",
    "  # train_accuracy = []\n",
    "  train_loss = []\n",
    "  test_accuracy = []\n",
    "  test_loss = []\n",
    "\n",
    "\n",
    "  # measure time\n",
    "  start = time.time()\n",
    "\n",
    "  for curr_round in range(1, rounds+1):\n",
    "    w, local_loss = [], []\n",
    "\n",
    "    m = max(int(C*K), 1)\n",
    "    \n",
    "    S_t = np.random.choice(range(K), m, replace=False)\n",
    "    for k in S_t:\n",
    "      local_update = ClientUpdate(dataset=ds, batchSize=batch_size, learning_rate=lr, epochs=E, idxs=data_dict[k])\n",
    "      weights, loss = local_update.train(model=copy.deepcopy(model))\n",
    "\n",
    "      w.append(copy.deepcopy(weights))\n",
    "      local_loss.append(copy.deepcopy(loss))\n",
    "\n",
    "    # updating the global weights\n",
    "    weights_avg = copy.deepcopy(w[0])\n",
    "    for k in weights_avg.keys():\n",
    "      for i in range(1, len(w)):\n",
    "        weights_avg[k] += w[i][k]\n",
    "\n",
    "      weights_avg[k] = torch.div(weights_avg[k], len(w))\n",
    "\n",
    "    global_weights = weights_avg\n",
    "\n",
    "    # move the updated weights to our model state dict\n",
    "    model.load_state_dict(global_weights)\n",
    "\n",
    "    # loss\n",
    "    loss_avg = sum(local_loss) / len(local_loss)\n",
    "    print('Round: {}... \\tAverage Loss: {}'.format(curr_round, round(loss_avg, 3)))\n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "    # test\n",
    "    test_criterion = nn.CrossEntropyLoss()\n",
    "    test_accuracy_current, test_loss_current =  testing(copy.deepcopy(model), ds_test, 128, test_criterion, num_classes, classes_test)\n",
    "    test_accuracy.append(test_accuracy_current)\n",
    "    test_loss.append(test_loss_current)\n",
    "\n",
    "  end = time.time()\n",
    "  \n",
    "  fig, ax = plt.subplots()\n",
    "  x_axis = np.arange(1, rounds+1)\n",
    "  y_axis = np.array(train_loss)\n",
    "  ax.plot(x_axis, y_axis, 'tab:'+plt_color)\n",
    "\n",
    "  ax.set(xlabel='Number of Rounds', ylabel='Train Loss',\n",
    "       title=plt_title)\n",
    "  ax.grid()\n",
    "  fig.savefig(plt_title+'_Train_loss.jpg', format='jpg')\n",
    "  plt.show()\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  x_axis = np.arange(1, rounds+1)\n",
    "  y_axis = np.array(test_loss)\n",
    "  ax.plot(x_axis, y_axis, 'tab:'+plt_color)\n",
    "\n",
    "  ax.set(xlabel='Number of Rounds', ylabel='Test Loss',\n",
    "       title=plt_title)\n",
    "  ax.grid()\n",
    "  fig.savefig(plt_title+'_Test_loss.jpg', format='jpg')\n",
    "  plt.show()\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  x_axis = np.arange(1, rounds+1)\n",
    "  y_axis = np.array(test_accuracy)\n",
    "  ax.plot(x_axis, y_axis, 'tab:'+plt_color)\n",
    "\n",
    "  ax.set(xlabel='Number of Rounds', ylabel='Test Accuracy',\n",
    "       title=plt_title)\n",
    "  ax.grid()\n",
    "  fig.savefig(plt_title+'_Test_Accuracy.jpg', format='jpg')\n",
    "  plt.show()\n",
    "  \n",
    "  print(\"Training Done!\")\n",
    "  print(\"Total time taken to Train: {}\\n\\n\".format(end-start))\n",
    "  \n",
    "  return model, train_loss, test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUYyb4T-uXmF"
   },
   "source": [
    "## Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCcIZmO5uan9"
   },
   "outputs": [],
   "source": [
    "def testing(model, dataset, bs, criterion, num_classes, classes, print_info=False):\n",
    "  #test loss \n",
    "  test_loss = 0.0\n",
    "  correct_class = list(0. for i in range(num_classes))\n",
    "  total_class = list(0. for i in range(num_classes))\n",
    "\n",
    "  test_loader = DataLoader(dataset, batch_size=bs)\n",
    "  l = len(test_loader)\n",
    "  model.eval()\n",
    "  for data, labels in test_loader:\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "      data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "    output = model(data)\n",
    "    loss = criterion(output, labels)\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "\n",
    "    _, pred = torch.max(output, 1)\n",
    "\n",
    "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
    "\n",
    "    #test accuracy for each object class\n",
    "    for i in range(num_classes):\n",
    "      label = labels.data[i]\n",
    "      correct_class[label] += correct[i].item()\n",
    "      total_class[label] += 1\n",
    "    \n",
    "  # avg test loss\n",
    "  test_loss = test_loss/len(test_loader.dataset)\n",
    "  test_accuracy = 100. * np.sum(correct_class) / np.sum(total_class)\n",
    "\n",
    "  if print_info:\n",
    "    print(\"Test Loss: {:.6f}\\n\".format(test_loss))\n",
    "\n",
    "    for i in range(10):\n",
    "      if total_class[i]>0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % \n",
    "              (classes[i], 100 * correct_class[i] / total_class[i],\n",
    "              np.sum(correct_class[i]), np.sum(total_class[i])))\n",
    "      else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "    print('\\nFinal Test  Accuracy: {:.3f} ({}/{})'.format(\n",
    "          100. * np.sum(correct_class) / np.sum(total_class),\n",
    "          np.sum(correct_class), np.sum(total_class)))\n",
    "  \n",
    "  return test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri0FqXFeHW-V"
   },
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thZm2kSiHT4v"
   },
   "outputs": [],
   "source": [
    "log_dict = {}\n",
    "NUM_REPEAT = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hO5oV6aXqeh"
   },
   "source": [
    "## MNIST CNN on IID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flSQv_P4zCfx"
   },
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0ZalcKZtEseA",
    "outputId": "b28f6fdb-c1ce-4c59-aa58-0e383d408230"
   },
   "outputs": [],
   "source": [
    "train_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
    "test_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "\n",
    "for exp_num in range(NUM_REPEAT):\n",
    "  print(\"Experiment Run Number: \", exp_num)\n",
    "\n",
    "  # number of training rounds\n",
    "  rounds = 50\n",
    "  # client fraction\n",
    "  C = 0.1\n",
    "  # number of clients\n",
    "  K = 100\n",
    "  # number of training passes on local dataset for each roung\n",
    "  E = 25\n",
    "  # batch size\n",
    "  batch_size = 10\n",
    "  # learning Rate\n",
    "  lr=0.05\n",
    "  # data partition dictionary\n",
    "  iid_dict = iid_partition(mnist_data_train, 100)\n",
    "  # load model\n",
    "  mnist_cnn = MNIST_CNN()\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    mnist_cnn.cuda()\n",
    "\n",
    "  mnist_cnn_iid_trained, train_loss, test_accuracy, test_loss = training(mnist_cnn, rounds, batch_size, lr, mnist_data_train, mnist_data_test, iid_dict, C, K, E, \"MNIST CNN on IID Dataset\", \"orange\")\n",
    "\n",
    "  train_loss_multiple_runs[exp_num] = train_loss\n",
    "  test_accuracy_multiple_runs[exp_num] = test_accuracy\n",
    "  test_loss_multiple_runs[exp_num] = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNwC82przF6G"
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qB97BFs9we9w",
    "outputId": "42f4c586-7c3c-46f8-d9c5-7025c1ab4e24"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "acc, loss = testing(mnist_cnn_iid_trained, mnist_data_test, 128, criterion, num_classes, classes_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdQZEZmHHeqt"
   },
   "outputs": [],
   "source": [
    "hyperparams = {'rounds': rounds,\n",
    "               'C': C,\n",
    "               'K': K,\n",
    "               'E': E,\n",
    "               'batch_size': batch_size,\n",
    "               'lr': lr,\n",
    "               }\n",
    "\n",
    "log_dict['MNIST CNN on IID'] = {'train_loss': train_loss_multiple_runs, \n",
    "                                'test_loss': test_loss_multiple_runs, \n",
    "                                'test_accuracy': test_accuracy_multiple_runs,\n",
    "                                'hyperparams': hyperparams,\n",
    "                                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uF8MdSIUYcnl"
   },
   "source": [
    "## MNIST CNN on Non IID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6wXX7JW11bx"
   },
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fCD3kBCKYfBK",
    "outputId": "f47df54f-43d5-4d0a-b89b-e90161c5f516"
   },
   "outputs": [],
   "source": [
    "train_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
    "test_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "\n",
    "for exp_num in range(NUM_REPEAT):\n",
    "  print(\"Experiment Run Number: \", exp_num)\n",
    "\n",
    "  # number of training rounds\n",
    "  rounds = 50\n",
    "  # client fraction\n",
    "  C = 0.1\n",
    "  # number of clients\n",
    "  K = 100\n",
    "  # number of training passes on local dataset for each roung\n",
    "  E = 25\n",
    "  # batch size\n",
    "  batch_size = 10\n",
    "  # learning Rate\n",
    "  lr=0.05\n",
    "  # dict containing different type of data partition\n",
    "  data_dict = non_iid_partition(mnist_data_train, 100, 200, 300, 2)\n",
    "  # load model\n",
    "  mnist_cnn = MNIST_CNN()\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    mnist_cnn.cuda()\n",
    "\n",
    "  mnist_cnn_non_iid_trained, train_loss, test_accuracy, test_loss = training(mnist_cnn, rounds, batch_size, lr, mnist_data_train, mnist_data_test, data_dict, C, K, E, \"MNIST CNN on Non-IID Dataset\", \"green\")\n",
    "\n",
    "  train_loss_multiple_runs[exp_num] = train_loss\n",
    "  test_accuracy_multiple_runs[exp_num] = test_accuracy\n",
    "  test_loss_multiple_runs[exp_num] = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4C68J-Kk14dB"
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_yQ9GiAZ15jE",
    "outputId": "2793a0a1-6969-4670-dd04-53dc6b191d44"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "acc, loss = testing(mnist_cnn_non_iid_trained, mnist_data_test, 128, criterion, num_classes, classes_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxMcxgLhLvX-"
   },
   "outputs": [],
   "source": [
    "hyperparams = {'rounds': rounds,\n",
    "               'C': C,\n",
    "               'K': K,\n",
    "               'E': E,\n",
    "               'batch_size': batch_size,\n",
    "               'lr': lr,\n",
    "               }\n",
    "\n",
    "log_dict['MNIST CNN on Non IID'] = {'train_loss': train_loss_multiple_runs, \n",
    "                                'test_loss': test_loss_multiple_runs, \n",
    "                                'test_accuracy': test_accuracy_multiple_runs,\n",
    "                                'hyperparams': hyperparams,\n",
    "                                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_jifdzniuhm"
   },
   "source": [
    "## MNIST MLP on IID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh-te0Od2XGO"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UmpWmKOQixVV",
    "outputId": "a487fb22-a217-4dfc-f4d6-63c4df004b41"
   },
   "outputs": [],
   "source": [
    "train_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
    "test_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "\n",
    "for exp_num in range(NUM_REPEAT):\n",
    "  print(\"Experiment Run Number: \", exp_num)\n",
    "\n",
    "  # number of training rounds\n",
    "  rounds = 50\n",
    "  # client fraction\n",
    "  C = 0.1\n",
    "  # number of clients\n",
    "  K = 100\n",
    "  # number of training passes on local dataset for each round\n",
    "  E = 25\n",
    "  # batch size\n",
    "  batch_size = 10\n",
    "  # learning Rate\n",
    "  lr=0.05\n",
    "  # dict containing different type of data partition\n",
    "  data_dict = iid_partition(mnist_data_train, 100)\n",
    "  # load model\n",
    "  mnist_mlp = MNIST_2NN()\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    mnist_mlp.cuda()\n",
    "\n",
    "  mnist_mlp_iid_trained, train_loss, test_accuracy, test_loss = training(mnist_mlp, rounds, batch_size, lr, mnist_data_train, mnist_data_test, data_dict, C, K, E, \"MNIST MLP on IID Dataset\", \"orange\")\n",
    "  \n",
    "  train_loss_multiple_runs[exp_num] = train_loss\n",
    "  test_accuracy_multiple_runs[exp_num] = test_accuracy\n",
    "  test_loss_multiple_runs[exp_num] = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTBsL3-72PPd"
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9G4j5L62OrS",
    "outputId": "e74613f0-7e49-4865-b430-223d32519658"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "acc, loss = testing(mnist_mlp_iid_trained, mnist_data_test, 128, criterion, num_classes, classes_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWCdJFRCL_f2"
   },
   "outputs": [],
   "source": [
    "hyperparams = {'rounds': rounds,\n",
    "               'C': C,\n",
    "               'K': K,\n",
    "               'E': E,\n",
    "               'batch_size': batch_size,\n",
    "               'lr': lr,\n",
    "               }\n",
    "\n",
    "log_dict['MNIST MLP on IID'] = {'train_loss': train_loss_multiple_runs, \n",
    "                                'test_loss': test_loss_multiple_runs, \n",
    "                                'test_accuracy': test_accuracy_multiple_runs,\n",
    "                                'hyperparams': hyperparams,\n",
    "                                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8jzEyO0iywz"
   },
   "source": [
    "## MNIST MLP on Non IID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJFepr3y2bF-"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EBKO44Hgi1Uh",
    "outputId": "7d0db2bf-fa03-4916-cb89-c2055290bdbb"
   },
   "outputs": [],
   "source": [
    "train_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
    "test_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "\n",
    "for exp_num in range(NUM_REPEAT):\n",
    "  print(\"Experiment Run Number: \", exp_num)\n",
    "  \n",
    "  # number of training rounds\n",
    "  rounds = 50\n",
    "  # client fraction\n",
    "  C = 0.1\n",
    "  # number of clients\n",
    "  K = 100\n",
    "  # number of training passes on local dataset for each roung\n",
    "  E = 25\n",
    "  # batch size\n",
    "  batch_size = 10\n",
    "  # learning Rate\n",
    "  lr=0.05\n",
    "  # dict containing different type of data partition\n",
    "  data_dict = non_iid_partition(mnist_data_train, 100, 200, 300, 2)\n",
    "  # load model\n",
    "  mnist_mlp = MNIST_2NN()\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    mnist_mlp.cuda()\n",
    "\n",
    "  mnist_mlp_non_iid_trained, train_loss, test_accuracy, test_loss = training(mnist_mlp, rounds, batch_size, lr, mnist_data_train, mnist_data_test, data_dict, C, K, E, \"MNIST MLP on Non-IID Dataset\", \"green\")\n",
    "\n",
    "  train_loss_multiple_runs[exp_num] = train_loss\n",
    "  test_accuracy_multiple_runs[exp_num] = test_accuracy\n",
    "  test_loss_multiple_runs[exp_num] = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmuQYPbF2mes"
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0tKMlJyF2nGN",
    "outputId": "93f7b3e3-8165-4dc8-d40c-579225a8aa33"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "acc, loss = testing(mnist_mlp_non_iid_trained, mnist_data_test, 128, criterion, num_classes, classes_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1W5krYcSMQiu"
   },
   "outputs": [],
   "source": [
    "hyperparams = {'rounds': rounds,\n",
    "               'C': C,\n",
    "               'K': K,\n",
    "               'E': E,\n",
    "               'batch_size': batch_size,\n",
    "               'lr': lr,\n",
    "               }\n",
    "\n",
    "log_dict['MNIST MLP on Non IID'] = {'train_loss': train_loss_multiple_runs, \n",
    "                                'test_loss': test_loss_multiple_runs, \n",
    "                                'test_accuracy': test_accuracy_multiple_runs,\n",
    "                                'hyperparams': hyperparams,\n",
    "                                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emS_SaRAP6TZ"
   },
   "source": [
    "## Pickle Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "soAN38JoP0c1"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "with open(path + 'Local_Round_FedAvg_25.pkl', 'wb') as file:\n",
    "  pickle.dump(log_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UBy-JswSoCJ",
    "outputId": "13b2bc77-3443-45b3-ee10-9e2265548afa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MNIST CNN on IID': {'hyperparams': {'C': 0.1,\n",
       "   'E': 25,\n",
       "   'K': 100,\n",
       "   'batch_size': 10,\n",
       "   'lr': 0.05,\n",
       "   'rounds': 50},\n",
       "  'test_accuracy': [[94.30379746835443,\n",
       "    97.0886075949367,\n",
       "    97.59493670886076,\n",
       "    98.35443037974683,\n",
       "    98.48101265822785,\n",
       "    98.9873417721519,\n",
       "    99.11392405063292,\n",
       "    98.9873417721519,\n",
       "    99.11392405063292,\n",
       "    99.24050632911393,\n",
       "    99.11392405063292,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.36708860759494,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696],\n",
       "   [95.9493670886076,\n",
       "    96.9620253164557,\n",
       "    97.21518987341773,\n",
       "    97.59493670886076,\n",
       "    98.10126582278481,\n",
       "    98.60759493670886,\n",
       "    98.48101265822785,\n",
       "    98.73417721518987,\n",
       "    98.73417721518987,\n",
       "    98.86075949367088,\n",
       "    98.73417721518987,\n",
       "    98.73417721518987,\n",
       "    98.9873417721519,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    98.9873417721519,\n",
       "    98.86075949367088,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.11392405063292,\n",
       "    99.24050632911393,\n",
       "    98.86075949367088,\n",
       "    98.9873417721519,\n",
       "    99.11392405063292,\n",
       "    98.9873417721519,\n",
       "    99.11392405063292,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.74683544303798,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.62025316455696],\n",
       "   [95.69620253164557,\n",
       "    96.9620253164557,\n",
       "    97.46835443037975,\n",
       "    97.9746835443038,\n",
       "    98.35443037974683,\n",
       "    98.73417721518987,\n",
       "    98.60759493670886,\n",
       "    98.86075949367088,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.62025316455696,\n",
       "    99.74683544303798,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.74683544303798,\n",
       "    99.74683544303798,\n",
       "    99.49367088607595,\n",
       "    99.74683544303798,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.74683544303798,\n",
       "    99.74683544303798,\n",
       "    99.62025316455696,\n",
       "    99.74683544303798,\n",
       "    99.62025316455696,\n",
       "    99.74683544303798,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.74683544303798,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.74683544303798],\n",
       "   [95.31645569620254,\n",
       "    97.0886075949367,\n",
       "    97.72151898734177,\n",
       "    98.10126582278481,\n",
       "    98.73417721518987,\n",
       "    98.22784810126582,\n",
       "    98.48101265822785,\n",
       "    98.9873417721519,\n",
       "    98.86075949367088,\n",
       "    98.9873417721519,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    99.24050632911393,\n",
       "    98.9873417721519,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.11392405063292,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.11392405063292,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.24050632911393,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595],\n",
       "   [95.82278481012658,\n",
       "    97.21518987341773,\n",
       "    97.84810126582279,\n",
       "    98.35443037974683,\n",
       "    98.48101265822785,\n",
       "    98.22784810126582,\n",
       "    98.73417721518987,\n",
       "    98.60759493670886,\n",
       "    98.73417721518987,\n",
       "    98.86075949367088,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.11392405063292,\n",
       "    99.24050632911393,\n",
       "    99.11392405063292,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.24050632911393,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494]],\n",
       "  'test_loss': [[0.1967204383969307,\n",
       "    0.10719871742276009,\n",
       "    0.08324646438640775,\n",
       "    0.07083426029149996,\n",
       "    0.06244292540853912,\n",
       "    0.05683180267765456,\n",
       "    0.05380866471354557,\n",
       "    0.04663595933032466,\n",
       "    0.044871384125561055,\n",
       "    0.04379433302673137,\n",
       "    0.0428354934019374,\n",
       "    0.03707851417195943,\n",
       "    0.03737290014565951,\n",
       "    0.03522661231005764,\n",
       "    0.0360584180036727,\n",
       "    0.037193398906615674,\n",
       "    0.03420786373503333,\n",
       "    0.034644026332654995,\n",
       "    0.033245531961985035,\n",
       "    0.03475085099976404,\n",
       "    0.03560949512943767,\n",
       "    0.03275894459387173,\n",
       "    0.031133612827436628,\n",
       "    0.03177319489890442,\n",
       "    0.03162750736074777,\n",
       "    0.028681847992370467,\n",
       "    0.027125027843227918,\n",
       "    0.02869258952525072,\n",
       "    0.027697737116504684,\n",
       "    0.029123742160532855,\n",
       "    0.028509528913596933,\n",
       "    0.027345622553963382,\n",
       "    0.028956988738813742,\n",
       "    0.02770834173213716,\n",
       "    0.0268213523922705,\n",
       "    0.028799779780278367,\n",
       "    0.026448451947802504,\n",
       "    0.025711597277436295,\n",
       "    0.026472268246390103,\n",
       "    0.02519805903616816,\n",
       "    0.027506231657189347,\n",
       "    0.027298900345981973,\n",
       "    0.02628873781518355,\n",
       "    0.02778073076785985,\n",
       "    0.026093723216288982,\n",
       "    0.026420497312467022,\n",
       "    0.025707112064381886,\n",
       "    0.026105542169695444,\n",
       "    0.0257424465208519,\n",
       "    0.025483153573108576],\n",
       "   [0.15805242479443551,\n",
       "    0.10397565263379366,\n",
       "    0.09391841972189141,\n",
       "    0.07053102266066708,\n",
       "    0.06370194370308017,\n",
       "    0.05562261998582398,\n",
       "    0.05141415025233364,\n",
       "    0.04979240105402896,\n",
       "    0.047599193646405726,\n",
       "    0.045001227467562376,\n",
       "    0.04374110444615035,\n",
       "    0.0425963005642388,\n",
       "    0.04124418041452891,\n",
       "    0.040346419656620856,\n",
       "    0.0399992220921642,\n",
       "    0.03763287528161564,\n",
       "    0.03718524161205737,\n",
       "    0.03845483317089413,\n",
       "    0.03656199822165399,\n",
       "    0.0355332012384732,\n",
       "    0.035133911464489805,\n",
       "    0.03421900246286596,\n",
       "    0.0316670118591252,\n",
       "    0.031305202477793136,\n",
       "    0.03196497117162344,\n",
       "    0.032606407214861154,\n",
       "    0.03142244168999386,\n",
       "    0.03167717550345128,\n",
       "    0.030582283039990853,\n",
       "    0.029436160545645726,\n",
       "    0.029320022433225585,\n",
       "    0.029055636657047218,\n",
       "    0.02805918479991169,\n",
       "    0.030009141607985474,\n",
       "    0.027343930680257564,\n",
       "    0.026028115852738027,\n",
       "    0.026458528117091873,\n",
       "    0.027142825099445655,\n",
       "    0.027155891987115523,\n",
       "    0.027102467294095094,\n",
       "    0.027987424427555016,\n",
       "    0.02830959654863027,\n",
       "    0.027315605571477317,\n",
       "    0.026782268683019902,\n",
       "    0.02606859068515339,\n",
       "    0.02714326655512417,\n",
       "    0.02519428649898373,\n",
       "    0.025429609215742176,\n",
       "    0.02522088748537799,\n",
       "    0.02702833981395861],\n",
       "   [0.15030737602710723,\n",
       "    0.10029514275820693,\n",
       "    0.07580572540204958,\n",
       "    0.06550674307410373,\n",
       "    0.0567574865525472,\n",
       "    0.04912183492039949,\n",
       "    0.048205592339306894,\n",
       "    0.0450738732852391,\n",
       "    0.0418983633410895,\n",
       "    0.040408325759983925,\n",
       "    0.038563891412182964,\n",
       "    0.03766123756187917,\n",
       "    0.03666110856685245,\n",
       "    0.03530355963773327,\n",
       "    0.03603805610250165,\n",
       "    0.033071200311920984,\n",
       "    0.03378648532003426,\n",
       "    0.0307369981756121,\n",
       "    0.03095336792460912,\n",
       "    0.031280234389282485,\n",
       "    0.029551637881484204,\n",
       "    0.02976958200720892,\n",
       "    0.029454506959714126,\n",
       "    0.028618433471791375,\n",
       "    0.02630696216920951,\n",
       "    0.027119721316909363,\n",
       "    0.026937090531646255,\n",
       "    0.02775355236992591,\n",
       "    0.026573951690452485,\n",
       "    0.026343902755156433,\n",
       "    0.026930128686298575,\n",
       "    0.027436350540875083,\n",
       "    0.027122156776408293,\n",
       "    0.0281468677708011,\n",
       "    0.02485621547275259,\n",
       "    0.025459792870117782,\n",
       "    0.02667756662630809,\n",
       "    0.026073722579720106,\n",
       "    0.027958372800500227,\n",
       "    0.02487248802442982,\n",
       "    0.025553469358931987,\n",
       "    0.0253139206673564,\n",
       "    0.027615950777692886,\n",
       "    0.02587154564613379,\n",
       "    0.026498381560618737,\n",
       "    0.025628352142803443,\n",
       "    0.025906214212935402,\n",
       "    0.025758130655706167,\n",
       "    0.023921389109300754,\n",
       "    0.023668159924435123],\n",
       "   [0.1946623146057129,\n",
       "    0.12442467858423478,\n",
       "    0.09765373887204697,\n",
       "    0.08063912495163822,\n",
       "    0.0706920640763823,\n",
       "    0.0640313367118928,\n",
       "    0.06110369387270439,\n",
       "    0.05471507778382511,\n",
       "    0.051579792635470946,\n",
       "    0.04963053500414153,\n",
       "    0.04890169048757816,\n",
       "    0.04757134776538897,\n",
       "    0.04233424424104669,\n",
       "    0.0396997357429499,\n",
       "    0.038674796215029984,\n",
       "    0.039234767094612925,\n",
       "    0.03665634096320405,\n",
       "    0.0343327907502101,\n",
       "    0.03568462072295115,\n",
       "    0.03518603905759392,\n",
       "    0.03536067044801221,\n",
       "    0.03642536852102153,\n",
       "    0.034640402387436736,\n",
       "    0.03326395233171265,\n",
       "    0.032984621981685225,\n",
       "    0.03295392750032727,\n",
       "    0.03284074062458365,\n",
       "    0.03219518886093992,\n",
       "    0.031422122236168755,\n",
       "    0.03115033612447769,\n",
       "    0.03327723444117073,\n",
       "    0.033713945123920806,\n",
       "    0.031119996443187484,\n",
       "    0.03126103561684081,\n",
       "    0.031860981318477914,\n",
       "    0.03035368572657944,\n",
       "    0.030092822599926423,\n",
       "    0.02956982962116142,\n",
       "    0.029773090021614053,\n",
       "    0.029352371518948314,\n",
       "    0.030553370382981074,\n",
       "    0.02977477771468674,\n",
       "    0.03075773813904244,\n",
       "    0.02890662194680124,\n",
       "    0.02770340378238492,\n",
       "    0.028141889071279708,\n",
       "    0.02989008861371324,\n",
       "    0.028152049641892597,\n",
       "    0.02769335083142903,\n",
       "    0.02803896802512327],\n",
       "   [0.14753474572300912,\n",
       "    0.1132031561863143,\n",
       "    0.09334310403184791,\n",
       "    0.07883274305154628,\n",
       "    0.06835057424362603,\n",
       "    0.06529031875565561,\n",
       "    0.059429013394480174,\n",
       "    0.05709113562147277,\n",
       "    0.04961251541755373,\n",
       "    0.05122763031623653,\n",
       "    0.04148261145034848,\n",
       "    0.042826584943482474,\n",
       "    0.04061307559956229,\n",
       "    0.03652435690985967,\n",
       "    0.03753771043997665,\n",
       "    0.03616879923497007,\n",
       "    0.033558468950669704,\n",
       "    0.033350412251514465,\n",
       "    0.031024398596981473,\n",
       "    0.03261467563460428,\n",
       "    0.03264722167197162,\n",
       "    0.03103731462898702,\n",
       "    0.03206213542950931,\n",
       "    0.030483636422131484,\n",
       "    0.02887397315397602,\n",
       "    0.02923228334113951,\n",
       "    0.029758129571031793,\n",
       "    0.028427310170688325,\n",
       "    0.030199175762413506,\n",
       "    0.030204936772622988,\n",
       "    0.02711278053892356,\n",
       "    0.028003735408110946,\n",
       "    0.028428026248424793,\n",
       "    0.028113117891706587,\n",
       "    0.027509640268426245,\n",
       "    0.026882614633037316,\n",
       "    0.025334003955852176,\n",
       "    0.02600965147486158,\n",
       "    0.026921204465307057,\n",
       "    0.02842449112344161,\n",
       "    0.028536841964105737,\n",
       "    0.027444012107235398,\n",
       "    0.027341806835181677,\n",
       "    0.02706194824719222,\n",
       "    0.02607451753595562,\n",
       "    0.026274068487579325,\n",
       "    0.027303157424328168,\n",
       "    0.02476809591789679,\n",
       "    0.02586029171807836,\n",
       "    0.02634473809223124]],\n",
       "  'train_loss': [[0.12052958095110275,\n",
       "    0.025849540249228885,\n",
       "    0.020607957784856724,\n",
       "    0.01220050480109135,\n",
       "    0.013391746989962136,\n",
       "    0.00887732286306053,\n",
       "    0.009801448136609658,\n",
       "    0.008675247896949716,\n",
       "    0.006956841719616973,\n",
       "    0.005902705231644098,\n",
       "    0.006308150641271476,\n",
       "    0.006308311092313448,\n",
       "    0.005974519054621073,\n",
       "    0.005896978229843323,\n",
       "    0.005875155640130427,\n",
       "    0.005135449046289517,\n",
       "    0.004516432229955956,\n",
       "    0.004449301759071309,\n",
       "    0.0035537429751089636,\n",
       "    0.0052146816603343225,\n",
       "    0.004007613217992686,\n",
       "    0.0036200468703921976,\n",
       "    0.0027723741944806557,\n",
       "    0.0037817410442948633,\n",
       "    0.004088510966877106,\n",
       "    0.0036941517670644034,\n",
       "    0.0036387533586891944,\n",
       "    0.002775264835072505,\n",
       "    0.0037033004144298243,\n",
       "    0.0031473329285746435,\n",
       "    0.0034442837111805986,\n",
       "    0.0035434566072301984,\n",
       "    0.0030140512750300033,\n",
       "    0.003483158745377743,\n",
       "    0.0023718897983268797,\n",
       "    0.0023785066670438653,\n",
       "    0.003052209431151113,\n",
       "    0.0032930592421640606,\n",
       "    0.0024107910352906036,\n",
       "    0.003234633305222415,\n",
       "    0.0024408735977682577,\n",
       "    0.0019349891258046595,\n",
       "    0.002662063671848063,\n",
       "    0.0023247243084426814,\n",
       "    0.0030905863371864218,\n",
       "    0.0028714020082041066,\n",
       "    0.0017505900862509732,\n",
       "    0.0018961888555031892,\n",
       "    0.0016901738595337837,\n",
       "    0.003077875874872187],\n",
       "   [0.11612861416724829,\n",
       "    0.03190636202443749,\n",
       "    0.014082237939101521,\n",
       "    0.016548273595964287,\n",
       "    0.010414542527772129,\n",
       "    0.009019611231293169,\n",
       "    0.008916866313917731,\n",
       "    0.0076026013093863296,\n",
       "    0.0066757998368440295,\n",
       "    0.007379645975234122,\n",
       "    0.007740745801079014,\n",
       "    0.006217374333693747,\n",
       "    0.005396618739446064,\n",
       "    0.005913531410869709,\n",
       "    0.004442985310194733,\n",
       "    0.005529753180564013,\n",
       "    0.004841029028208755,\n",
       "    0.0042238257356605555,\n",
       "    0.0041833688465371666,\n",
       "    0.0036371636867419135,\n",
       "    0.004914638206006239,\n",
       "    0.0032375555213421696,\n",
       "    0.0037192271451433347,\n",
       "    0.0037010768899993413,\n",
       "    0.004222395385699665,\n",
       "    0.004214117946646065,\n",
       "    0.002530038480458266,\n",
       "    0.0029605602648186463,\n",
       "    0.0028563140507743287,\n",
       "    0.003776367858728066,\n",
       "    0.0031344759257634107,\n",
       "    0.003077887671599237,\n",
       "    0.003301225317443395,\n",
       "    0.0028949506738304874,\n",
       "    0.0032572827826328,\n",
       "    0.0020658581785011803,\n",
       "    0.002808607545039987,\n",
       "    0.002493820826024281,\n",
       "    0.003173443523707073,\n",
       "    0.0021585273551025233,\n",
       "    0.0029440188047810145,\n",
       "    0.002768821974341916,\n",
       "    0.0025581956989582863,\n",
       "    0.0023929764793012604,\n",
       "    0.0021338774614387238,\n",
       "    0.0026589023489292404,\n",
       "    0.0026645136708996933,\n",
       "    0.0014685819353055587,\n",
       "    0.00244691410152548,\n",
       "    0.0022617472411212217],\n",
       "   [0.11629468538029916,\n",
       "    0.025320919785832667,\n",
       "    0.017671891860996342,\n",
       "    0.011854077902647298,\n",
       "    0.011747135611247957,\n",
       "    0.00980904751972573,\n",
       "    0.010245473125553612,\n",
       "    0.007574031129464083,\n",
       "    0.007215724317309251,\n",
       "    0.006907308514027544,\n",
       "    0.007302829987126116,\n",
       "    0.006538714158017207,\n",
       "    0.00514992529671048,\n",
       "    0.006432606721657195,\n",
       "    0.005785970781642424,\n",
       "    0.0053528516271679856,\n",
       "    0.005408703341864354,\n",
       "    0.004776727915822196,\n",
       "    0.005157563947060826,\n",
       "    0.004844438806982441,\n",
       "    0.004070716602675309,\n",
       "    0.004551522305224765,\n",
       "    0.00487507770424269,\n",
       "    0.002974250175113209,\n",
       "    0.004343590372441572,\n",
       "    0.00362150452933162,\n",
       "    0.002812707254434396,\n",
       "    0.00365015118600494,\n",
       "    0.003903911262252871,\n",
       "    0.0030502403498966595,\n",
       "    0.0029944410068063047,\n",
       "    0.0026907398487225685,\n",
       "    0.0033441985200918997,\n",
       "    0.0031198875228916415,\n",
       "    0.004050209364167447,\n",
       "    0.0026549188255937553,\n",
       "    0.0034264314502504556,\n",
       "    0.002462568224790862,\n",
       "    0.002065370154506331,\n",
       "    0.002445888482095745,\n",
       "    0.002887524900636325,\n",
       "    0.0021334865184275986,\n",
       "    0.0031161951558293477,\n",
       "    0.0022095357982250667,\n",
       "    0.0028338179472601204,\n",
       "    0.0025122472615697325,\n",
       "    0.00181568859381051,\n",
       "    0.002833048957510749,\n",
       "    0.0025273742565534454,\n",
       "    0.0010896241773823104],\n",
       "   [0.11750037870515202,\n",
       "    0.026245737808192948,\n",
       "    0.01823003752321265,\n",
       "    0.013304066585978588,\n",
       "    0.011914531967566293,\n",
       "    0.009894604500384803,\n",
       "    0.009012220059004924,\n",
       "    0.008953382783322196,\n",
       "    0.006253893404968955,\n",
       "    0.008463414984691808,\n",
       "    0.006076350814028654,\n",
       "    0.005821669446223974,\n",
       "    0.006132746745140145,\n",
       "    0.0049781150318367025,\n",
       "    0.005812818072957575,\n",
       "    0.005775522172682937,\n",
       "    0.005337090490044006,\n",
       "    0.006570325239241807,\n",
       "    0.004813200351393877,\n",
       "    0.003990641788863514,\n",
       "    0.005191937167806555,\n",
       "    0.003613631443824368,\n",
       "    0.004087718604221283,\n",
       "    0.004212752931142568,\n",
       "    0.0039930102560992945,\n",
       "    0.003225815856001863,\n",
       "    0.004248290528208037,\n",
       "    0.002688541649128664,\n",
       "    0.003583713674551418,\n",
       "    0.0027399802912092585,\n",
       "    0.0029278059213299866,\n",
       "    0.003273893861287535,\n",
       "    0.0045786455651685345,\n",
       "    0.0030416097697939107,\n",
       "    0.0027634794626208723,\n",
       "    0.003278496460135001,\n",
       "    0.002548217958600147,\n",
       "    0.00237736220740241,\n",
       "    0.0048643343144258505,\n",
       "    0.0028178944312621944,\n",
       "    0.002520959155396701,\n",
       "    0.0033872415616996456,\n",
       "    0.0013342996506655342,\n",
       "    0.002726280070503441,\n",
       "    0.0026984505229571466,\n",
       "    0.0023409653702575134,\n",
       "    0.001725087747777836,\n",
       "    0.002009340426060618,\n",
       "    0.002795439021518451,\n",
       "    0.0017088243001745756],\n",
       "   [0.1188719004573607,\n",
       "    0.025631692132398243,\n",
       "    0.016414152130189447,\n",
       "    0.014567654410249245,\n",
       "    0.01120010481509294,\n",
       "    0.00916856074481361,\n",
       "    0.00920298347209826,\n",
       "    0.0074686662189922485,\n",
       "    0.008445189444888607,\n",
       "    0.006854187457791125,\n",
       "    0.0075116882837851975,\n",
       "    0.005555087002519245,\n",
       "    0.006950409168428419,\n",
       "    0.006282534998602015,\n",
       "    0.004204560268440891,\n",
       "    0.004287237036552279,\n",
       "    0.005557841697065044,\n",
       "    0.003910906672173134,\n",
       "    0.005542143402819217,\n",
       "    0.003886320124710011,\n",
       "    0.00408005742920699,\n",
       "    0.003828039019495913,\n",
       "    0.0035825161011060146,\n",
       "    0.0036845406633062806,\n",
       "    0.004756639522343649,\n",
       "    0.003009918383830678,\n",
       "    0.0029239674590500033,\n",
       "    0.003074116413343697,\n",
       "    0.0034748078503342556,\n",
       "    0.0021955419506401183,\n",
       "    0.004784338785820815,\n",
       "    0.002505818790746079,\n",
       "    0.003932916530961684,\n",
       "    0.0029777012873420374,\n",
       "    0.0028560958444512783,\n",
       "    0.003270545517026781,\n",
       "    0.003653763508951758,\n",
       "    0.0023769249328653977,\n",
       "    0.002829647752659741,\n",
       "    0.0028326480403430957,\n",
       "    0.0018084019651052326,\n",
       "    0.002797066307446561,\n",
       "    0.0014106835193538952,\n",
       "    0.002303399864899268,\n",
       "    0.002623811828314685,\n",
       "    0.002410555206160919,\n",
       "    0.0028251017609336225,\n",
       "    0.0035632579739691495,\n",
       "    0.0018122477452765292,\n",
       "    0.002025273245142547]]},\n",
       " 'MNIST CNN on Non IID': {'hyperparams': {'C': 0.1,\n",
       "   'E': 25,\n",
       "   'K': 100,\n",
       "   'batch_size': 10,\n",
       "   'lr': 0.05,\n",
       "   'rounds': 50},\n",
       "  'test_accuracy': [[20.632911392405063,\n",
       "    69.24050632911393,\n",
       "    65.44303797468355,\n",
       "    64.9367088607595,\n",
       "    76.07594936708861,\n",
       "    81.0126582278481,\n",
       "    82.15189873417721,\n",
       "    81.51898734177215,\n",
       "    82.9113924050633,\n",
       "    91.13924050632912,\n",
       "    83.16455696202532,\n",
       "    95.31645569620254,\n",
       "    97.21518987341773,\n",
       "    95.56962025316456,\n",
       "    90.25316455696202,\n",
       "    94.17721518987342,\n",
       "    95.69620253164557,\n",
       "    96.20253164556962,\n",
       "    93.16455696202532,\n",
       "    95.9493670886076,\n",
       "    97.46835443037975,\n",
       "    95.9493670886076,\n",
       "    96.58227848101266,\n",
       "    96.9620253164557,\n",
       "    96.45569620253164,\n",
       "    94.68354430379746,\n",
       "    97.72151898734177,\n",
       "    97.84810126582279,\n",
       "    97.21518987341773,\n",
       "    95.82278481012658,\n",
       "    97.21518987341773,\n",
       "    97.59493670886076,\n",
       "    96.45569620253164,\n",
       "    97.46835443037975,\n",
       "    97.34177215189874,\n",
       "    98.22784810126582,\n",
       "    97.0886075949367,\n",
       "    98.35443037974683,\n",
       "    98.10126582278481,\n",
       "    98.22784810126582,\n",
       "    98.60759493670886,\n",
       "    98.86075949367088,\n",
       "    97.59493670886076,\n",
       "    97.21518987341773,\n",
       "    98.60759493670886,\n",
       "    98.60759493670886,\n",
       "    98.86075949367088,\n",
       "    98.73417721518987,\n",
       "    98.60759493670886,\n",
       "    98.86075949367088],\n",
       "   [25.063291139240505,\n",
       "    60.12658227848101,\n",
       "    63.41772151898734,\n",
       "    53.037974683544306,\n",
       "    80.0,\n",
       "    82.53164556962025,\n",
       "    87.46835443037975,\n",
       "    91.39240506329114,\n",
       "    91.77215189873418,\n",
       "    95.44303797468355,\n",
       "    93.79746835443038,\n",
       "    93.29113924050633,\n",
       "    93.67088607594937,\n",
       "    96.83544303797468,\n",
       "    95.31645569620254,\n",
       "    96.20253164556962,\n",
       "    97.0886075949367,\n",
       "    95.69620253164557,\n",
       "    97.21518987341773,\n",
       "    97.0886075949367,\n",
       "    96.70886075949367,\n",
       "    97.21518987341773,\n",
       "    95.82278481012658,\n",
       "    96.70886075949367,\n",
       "    97.34177215189874,\n",
       "    97.72151898734177,\n",
       "    97.84810126582279,\n",
       "    96.9620253164557,\n",
       "    98.22784810126582,\n",
       "    97.59493670886076,\n",
       "    98.48101265822785,\n",
       "    97.84810126582279,\n",
       "    98.48101265822785,\n",
       "    98.35443037974683,\n",
       "    97.9746835443038,\n",
       "    97.72151898734177,\n",
       "    98.48101265822785,\n",
       "    97.9746835443038,\n",
       "    98.60759493670886,\n",
       "    98.9873417721519,\n",
       "    98.86075949367088,\n",
       "    98.48101265822785,\n",
       "    98.9873417721519,\n",
       "    99.24050632911393,\n",
       "    98.73417721518987,\n",
       "    98.9873417721519,\n",
       "    98.86075949367088,\n",
       "    98.73417721518987,\n",
       "    98.73417721518987,\n",
       "    98.60759493670886],\n",
       "   [15.69620253164557,\n",
       "    27.72151898734177,\n",
       "    59.620253164556964,\n",
       "    70.88607594936708,\n",
       "    73.16455696202532,\n",
       "    90.0,\n",
       "    84.9367088607595,\n",
       "    90.12658227848101,\n",
       "    93.0379746835443,\n",
       "    81.89873417721519,\n",
       "    93.16455696202532,\n",
       "    93.16455696202532,\n",
       "    96.07594936708861,\n",
       "    96.9620253164557,\n",
       "    95.44303797468355,\n",
       "    94.9367088607595,\n",
       "    96.20253164556962,\n",
       "    96.32911392405063,\n",
       "    97.46835443037975,\n",
       "    96.45569620253164,\n",
       "    97.21518987341773,\n",
       "    96.45569620253164,\n",
       "    97.34177215189874,\n",
       "    98.48101265822785,\n",
       "    98.35443037974683,\n",
       "    97.21518987341773,\n",
       "    97.84810126582279,\n",
       "    98.86075949367088,\n",
       "    98.73417721518987,\n",
       "    98.35443037974683,\n",
       "    98.10126582278481,\n",
       "    97.21518987341773,\n",
       "    98.60759493670886,\n",
       "    98.10126582278481,\n",
       "    98.35443037974683,\n",
       "    98.35443037974683,\n",
       "    98.9873417721519,\n",
       "    98.48101265822785,\n",
       "    98.86075949367088,\n",
       "    98.86075949367088,\n",
       "    97.84810126582279,\n",
       "    98.35443037974683,\n",
       "    98.48101265822785,\n",
       "    98.9873417721519,\n",
       "    98.86075949367088,\n",
       "    98.48101265822785,\n",
       "    98.22784810126582,\n",
       "    97.72151898734177,\n",
       "    98.86075949367088,\n",
       "    99.24050632911393],\n",
       "   [27.974683544303797,\n",
       "    45.822784810126585,\n",
       "    48.9873417721519,\n",
       "    62.53164556962025,\n",
       "    79.74683544303798,\n",
       "    79.24050632911393,\n",
       "    83.92405063291139,\n",
       "    84.68354430379746,\n",
       "    92.15189873417721,\n",
       "    94.9367088607595,\n",
       "    95.44303797468355,\n",
       "    95.69620253164557,\n",
       "    91.26582278481013,\n",
       "    95.82278481012658,\n",
       "    96.20253164556962,\n",
       "    97.34177215189874,\n",
       "    96.58227848101266,\n",
       "    97.0886075949367,\n",
       "    95.56962025316456,\n",
       "    97.59493670886076,\n",
       "    97.21518987341773,\n",
       "    97.9746835443038,\n",
       "    97.84810126582279,\n",
       "    97.0886075949367,\n",
       "    97.9746835443038,\n",
       "    96.58227848101266,\n",
       "    98.10126582278481,\n",
       "    98.35443037974683,\n",
       "    98.48101265822785,\n",
       "    97.59493670886076,\n",
       "    96.07594936708861,\n",
       "    97.84810126582279,\n",
       "    98.22784810126582,\n",
       "    98.10126582278481,\n",
       "    98.10126582278481,\n",
       "    98.48101265822785,\n",
       "    98.60759493670886,\n",
       "    97.9746835443038,\n",
       "    98.48101265822785,\n",
       "    98.73417721518987,\n",
       "    98.73417721518987,\n",
       "    97.9746835443038,\n",
       "    98.86075949367088,\n",
       "    98.9873417721519,\n",
       "    98.48101265822785,\n",
       "    98.73417721518987,\n",
       "    98.9873417721519,\n",
       "    98.60759493670886,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292],\n",
       "   [20.0,\n",
       "    36.20253164556962,\n",
       "    62.278481012658226,\n",
       "    71.39240506329114,\n",
       "    68.86075949367088,\n",
       "    87.72151898734177,\n",
       "    79.62025316455696,\n",
       "    73.67088607594937,\n",
       "    80.88607594936708,\n",
       "    81.51898734177215,\n",
       "    83.16455696202532,\n",
       "    89.49367088607595,\n",
       "    95.0632911392405,\n",
       "    91.0126582278481,\n",
       "    93.41772151898734,\n",
       "    90.0,\n",
       "    95.0632911392405,\n",
       "    96.70886075949367,\n",
       "    89.74683544303798,\n",
       "    88.22784810126582,\n",
       "    89.62025316455696,\n",
       "    94.0506329113924,\n",
       "    97.9746835443038,\n",
       "    95.56962025316456,\n",
       "    97.9746835443038,\n",
       "    96.07594936708861,\n",
       "    94.30379746835443,\n",
       "    93.41772151898734,\n",
       "    95.82278481012658,\n",
       "    92.27848101265823,\n",
       "    98.10126582278481,\n",
       "    97.84810126582279,\n",
       "    96.83544303797468,\n",
       "    97.34177215189874,\n",
       "    98.48101265822785,\n",
       "    98.48101265822785,\n",
       "    97.21518987341773,\n",
       "    96.07594936708861,\n",
       "    96.70886075949367,\n",
       "    98.73417721518987,\n",
       "    98.48101265822785,\n",
       "    97.72151898734177,\n",
       "    98.35443037974683,\n",
       "    98.22784810126582,\n",
       "    97.84810126582279,\n",
       "    97.46835443037975,\n",
       "    98.60759493670886,\n",
       "    97.34177215189874,\n",
       "    98.22784810126582,\n",
       "    98.10126582278481]],\n",
       "  'test_loss': [[2.153059111404419,\n",
       "    1.5906728286743164,\n",
       "    0.9542296494483947,\n",
       "    0.9271274547576904,\n",
       "    0.725140052318573,\n",
       "    0.4966754144191742,\n",
       "    0.8055668889045715,\n",
       "    0.6186511972904205,\n",
       "    0.4361247542381287,\n",
       "    0.24306684663295747,\n",
       "    0.6682337231636047,\n",
       "    0.14654830263257027,\n",
       "    0.12335347858071327,\n",
       "    0.12570072142779828,\n",
       "    0.2296567997455597,\n",
       "    0.1886374885559082,\n",
       "    0.1572774866938591,\n",
       "    0.11829053934812546,\n",
       "    0.16606133990883828,\n",
       "    0.10171294039487838,\n",
       "    0.0799241408571601,\n",
       "    0.10346935544461013,\n",
       "    0.09807285853773355,\n",
       "    0.08839920492470264,\n",
       "    0.09765889206826688,\n",
       "    0.1638527781009674,\n",
       "    0.07779146658480167,\n",
       "    0.07525641474872828,\n",
       "    0.08005515745989979,\n",
       "    0.09493796744681894,\n",
       "    0.08222998734097928,\n",
       "    0.06391742370435968,\n",
       "    0.10721472233776003,\n",
       "    0.08852911024689675,\n",
       "    0.09345365906953812,\n",
       "    0.05858176416307688,\n",
       "    0.08071480892561377,\n",
       "    0.05862174603361636,\n",
       "    0.055690957584697755,\n",
       "    0.05542103775776923,\n",
       "    0.05617479444835335,\n",
       "    0.05053714061700739,\n",
       "    0.07551919496655464,\n",
       "    0.07847732558846474,\n",
       "    0.055299842234142124,\n",
       "    0.04796317321602255,\n",
       "    0.05158204144833144,\n",
       "    0.0474325309405569,\n",
       "    0.05222810741874855,\n",
       "    0.0497814619560726],\n",
       "   [2.1414064346313477,\n",
       "    1.6021820066452026,\n",
       "    1.149544812488556,\n",
       "    1.2288790073394775,\n",
       "    0.7070956543922424,\n",
       "    0.4559186569213867,\n",
       "    0.3979144086360931,\n",
       "    0.25045002192258836,\n",
       "    0.24002688471078873,\n",
       "    0.16464737070798874,\n",
       "    0.20201591149568557,\n",
       "    0.19566456590890885,\n",
       "    0.18376069251298904,\n",
       "    0.10918958177268505,\n",
       "    0.13022149132490157,\n",
       "    0.12740772380530835,\n",
       "    0.10005484143868089,\n",
       "    0.10192878036350012,\n",
       "    0.10665984205901623,\n",
       "    0.10211247695386409,\n",
       "    0.11032949574887753,\n",
       "    0.10438604984283448,\n",
       "    0.11198710401058197,\n",
       "    0.09712797934412956,\n",
       "    0.07132071413323283,\n",
       "    0.07337213150709868,\n",
       "    0.06360213232040406,\n",
       "    0.0855294696509838,\n",
       "    0.06497683540582656,\n",
       "    0.07048116761296988,\n",
       "    0.06036600740477443,\n",
       "    0.06325012617968023,\n",
       "    0.054052572028525175,\n",
       "    0.07337222756156699,\n",
       "    0.07105527743250131,\n",
       "    0.07073885753899813,\n",
       "    0.059494409430027005,\n",
       "    0.065532472711429,\n",
       "    0.05273013776279986,\n",
       "    0.04797597794011235,\n",
       "    0.051563000508025286,\n",
       "    0.055271220438182354,\n",
       "    0.041078620071522895,\n",
       "    0.037465578967891634,\n",
       "    0.04096470781601966,\n",
       "    0.03805906490795314,\n",
       "    0.04799265975952149,\n",
       "    0.04734190737567842,\n",
       "    0.040040995538979766,\n",
       "    0.04438304212857038],\n",
       "   [2.260773098373413,\n",
       "    1.9935661693573,\n",
       "    1.2559218883514405,\n",
       "    0.7301154842853547,\n",
       "    0.7549185608863831,\n",
       "    0.29867297919988633,\n",
       "    0.5023606861114502,\n",
       "    0.24070274069309236,\n",
       "    0.2217484240412712,\n",
       "    0.4164490668773651,\n",
       "    0.19730236191749573,\n",
       "    0.1667896499156952,\n",
       "    0.13415001679360866,\n",
       "    0.11409999428987504,\n",
       "    0.12201607443094253,\n",
       "    0.1528521985113621,\n",
       "    0.10527884367257356,\n",
       "    0.10857658792585134,\n",
       "    0.08197404959350825,\n",
       "    0.10485498107969761,\n",
       "    0.08625301366746425,\n",
       "    0.08403596233595162,\n",
       "    0.09545515270084143,\n",
       "    0.060144749345257875,\n",
       "    0.0643717829477042,\n",
       "    0.07398782145157456,\n",
       "    0.06866799102840014,\n",
       "    0.058926210611127315,\n",
       "    0.05431120878215879,\n",
       "    0.06591187729686498,\n",
       "    0.06453058008402586,\n",
       "    0.08459868415892124,\n",
       "    0.05864523445945233,\n",
       "    0.05146450001448393,\n",
       "    0.055863205959647895,\n",
       "    0.04903139356356114,\n",
       "    0.04850654485970735,\n",
       "    0.05434358047842979,\n",
       "    0.04623199786543846,\n",
       "    0.05999978220462799,\n",
       "    0.07041705005392432,\n",
       "    0.0776710368707776,\n",
       "    0.049387923747487364,\n",
       "    0.039302743587829175,\n",
       "    0.05736429178379476,\n",
       "    0.05010062181502581,\n",
       "    0.05664792943317443,\n",
       "    0.08251426491886377,\n",
       "    0.04322302157972008,\n",
       "    0.0388075205205474],\n",
       "   [2.0505604583740236,\n",
       "    1.6183387208938598,\n",
       "    1.5712505815505982,\n",
       "    1.1181652381896974,\n",
       "    0.742175221157074,\n",
       "    0.6584634589672088,\n",
       "    0.4810940243721008,\n",
       "    0.4314817524909973,\n",
       "    0.2436287609219551,\n",
       "    0.14740591908693312,\n",
       "    0.13658089507818222,\n",
       "    0.13679712338149547,\n",
       "    0.2393154622554779,\n",
       "    0.12566389574706555,\n",
       "    0.11972732961922884,\n",
       "    0.09387388154156506,\n",
       "    0.09609809648096561,\n",
       "    0.09631613847315311,\n",
       "    0.14675660922080278,\n",
       "    0.08321562304869294,\n",
       "    0.08829273084700108,\n",
       "    0.07114619767926633,\n",
       "    0.0705641629576683,\n",
       "    0.09274809226542712,\n",
       "    0.061123901753313836,\n",
       "    0.11308184394147247,\n",
       "    0.06158660237267614,\n",
       "    0.05392120754206553,\n",
       "    0.059474564862810074,\n",
       "    0.06390082516325638,\n",
       "    0.10923827156424522,\n",
       "    0.06349644147315994,\n",
       "    0.05731613604724407,\n",
       "    0.05728950511664152,\n",
       "    0.04963948031291365,\n",
       "    0.055377987331338224,\n",
       "    0.04895921640321613,\n",
       "    0.05092667491417378,\n",
       "    0.054169751353934405,\n",
       "    0.054399670620448884,\n",
       "    0.044101153360540046,\n",
       "    0.055571216135285796,\n",
       "    0.04401695164355915,\n",
       "    0.04871494982540608,\n",
       "    0.07061778731495141,\n",
       "    0.049760915991291406,\n",
       "    0.03891836194042116,\n",
       "    0.0585903374340618,\n",
       "    0.0431315282831667,\n",
       "    0.04042463162066415],\n",
       "   [2.2345109024047853,\n",
       "    1.5563353553771972,\n",
       "    1.351379051208496,\n",
       "    0.7966210693359375,\n",
       "    0.8831192952156067,\n",
       "    0.3791278344631195,\n",
       "    0.6625652897834777,\n",
       "    0.7004230128288269,\n",
       "    0.4699062826871872,\n",
       "    0.5719990481376648,\n",
       "    0.4068357618331909,\n",
       "    0.23882842257022857,\n",
       "    0.1644193941861391,\n",
       "    0.25515735350847246,\n",
       "    0.27121310614943506,\n",
       "    0.2800970545530319,\n",
       "    0.16379415113925933,\n",
       "    0.143690733730793,\n",
       "    0.219197927069664,\n",
       "    0.32496721048355104,\n",
       "    0.2661551751971245,\n",
       "    0.1605436922132969,\n",
       "    0.08794650903344155,\n",
       "    0.1359518706679344,\n",
       "    0.09019842139966786,\n",
       "    0.1132629365131259,\n",
       "    0.14811714173555374,\n",
       "    0.17969813235998153,\n",
       "    0.13560855577290057,\n",
       "    0.21029129492640494,\n",
       "    0.06438809860087931,\n",
       "    0.08860607471764087,\n",
       "    0.08904269347116352,\n",
       "    0.094791873934865,\n",
       "    0.06252665930278599,\n",
       "    0.05963083681520075,\n",
       "    0.07837427372299134,\n",
       "    0.11193049659542739,\n",
       "    0.09655325757265092,\n",
       "    0.04567149291113019,\n",
       "    0.0659829253767617,\n",
       "    0.06116798945628107,\n",
       "    0.05249774458874017,\n",
       "    0.06483356867115944,\n",
       "    0.08213342721313238,\n",
       "    0.06888268938995898,\n",
       "    0.053222391960630194,\n",
       "    0.09535529520697891,\n",
       "    0.06522833161060698,\n",
       "    0.06221011116504669]],\n",
       "  'train_loss': [[0.02328958629730593,\n",
       "    0.013968363388361629,\n",
       "    0.006924264210256383,\n",
       "    0.0049907998174369885,\n",
       "    0.006437872317329898,\n",
       "    0.0038735119272537677,\n",
       "    0.0043254348108723705,\n",
       "    0.0024101341707214153,\n",
       "    0.005170153390060939,\n",
       "    0.005372826830284404,\n",
       "    0.002074557818551191,\n",
       "    0.0038586715914593435,\n",
       "    0.0024038772799006977,\n",
       "    0.0035149973666722236,\n",
       "    0.0016173366536159927,\n",
       "    0.0020399614469270464,\n",
       "    0.0028406975941768936,\n",
       "    0.0024918784377702392,\n",
       "    0.002233345229957954,\n",
       "    0.001919001840937037,\n",
       "    0.0016572884588107206,\n",
       "    0.0022545919641109077,\n",
       "    0.0023116036955918963,\n",
       "    0.00194484729201417,\n",
       "    0.002852206723442161,\n",
       "    0.0012299108232785491,\n",
       "    0.0009431220083749734,\n",
       "    0.0023168199016950198,\n",
       "    0.0015608704590264928,\n",
       "    0.0008893170770709466,\n",
       "    0.0015108892406635243,\n",
       "    0.0014176676868992368,\n",
       "    0.0012940145395019475,\n",
       "    0.001364797766477422,\n",
       "    0.002520648232320679,\n",
       "    0.0013046875301517202,\n",
       "    0.003017797596575973,\n",
       "    0.0015239939502833103,\n",
       "    0.0018392236624081193,\n",
       "    0.0018121811000575603,\n",
       "    0.0017668103514582995,\n",
       "    0.0010412827873931673,\n",
       "    0.0008074195243041263,\n",
       "    0.0023074216346422077,\n",
       "    0.0011822227975519348,\n",
       "    0.0016839965364478925,\n",
       "    0.0008590068462182337,\n",
       "    0.0007317330168093511,\n",
       "    0.0009727336761957333,\n",
       "    0.001022365973676594],\n",
       "   [0.022929847965866685,\n",
       "    0.014043401075737271,\n",
       "    0.009403992274556239,\n",
       "    0.005732867174814785,\n",
       "    0.007499745720902992,\n",
       "    0.0024801288336643587,\n",
       "    0.005829437073913132,\n",
       "    0.004840687609891825,\n",
       "    0.004499568189358873,\n",
       "    0.003569497213134347,\n",
       "    0.006233298996598134,\n",
       "    0.0020246978476647394,\n",
       "    0.0032760626088408655,\n",
       "    0.004198905447301229,\n",
       "    0.003125633649318226,\n",
       "    0.002239833660776102,\n",
       "    0.0025252248101048295,\n",
       "    0.0036592731095929946,\n",
       "    0.0016386252176072563,\n",
       "    0.0017232408401189141,\n",
       "    0.0020465197721428853,\n",
       "    0.002853515230292469,\n",
       "    0.0020023792508057376,\n",
       "    0.0019193476948379483,\n",
       "    0.003543477156839457,\n",
       "    0.00201065655562227,\n",
       "    0.0027196613049460016,\n",
       "    0.0012614302537741178,\n",
       "    0.0019213632961379186,\n",
       "    0.0020231603792312753,\n",
       "    0.002828515282504667,\n",
       "    0.001761137421113917,\n",
       "    0.0018747139958985044,\n",
       "    0.00161097160653611,\n",
       "    0.002130085780260449,\n",
       "    0.0012938206987324044,\n",
       "    0.002443920654847053,\n",
       "    0.0022397439673357434,\n",
       "    0.0018068264466464597,\n",
       "    0.0014869661633159143,\n",
       "    0.0027151192543169883,\n",
       "    0.0014817355721798682,\n",
       "    0.0014930632736010522,\n",
       "    0.0010729345698810933,\n",
       "    0.0006883253961844805,\n",
       "    0.0021415591480301527,\n",
       "    0.001357985626652056,\n",
       "    0.002000112430048528,\n",
       "    0.0028859642927272296,\n",
       "    0.0009580195014560354],\n",
       "   [0.025213012965790472,\n",
       "    0.011997123766970163,\n",
       "    0.009764905499789962,\n",
       "    0.006534065278281151,\n",
       "    0.005710591876183338,\n",
       "    0.002606541985996333,\n",
       "    0.004619826516976474,\n",
       "    0.0024657777175356712,\n",
       "    0.004721803846953742,\n",
       "    0.0030899781141019954,\n",
       "    0.00534382904052958,\n",
       "    0.0028680308539778447,\n",
       "    0.0032266961577339416,\n",
       "    0.004739868937270344,\n",
       "    0.0019313328138647883,\n",
       "    0.0029450964331753553,\n",
       "    0.0029785862328783595,\n",
       "    0.0034375294692585356,\n",
       "    0.002343537562264486,\n",
       "    0.003913515436047337,\n",
       "    0.0017165154364386964,\n",
       "    0.0020012346568074033,\n",
       "    0.0030521192376569894,\n",
       "    0.0012683326418718086,\n",
       "    0.002515869842812818,\n",
       "    0.0017307444006557167,\n",
       "    0.0015878730874578435,\n",
       "    0.002212330731916775,\n",
       "    0.0018942779297335863,\n",
       "    0.0016271670793818547,\n",
       "    0.0020179068670632618,\n",
       "    0.0016124680873842196,\n",
       "    0.001654126063917767,\n",
       "    0.0021843068202063443,\n",
       "    0.002213815608128459,\n",
       "    0.0018885493284189682,\n",
       "    0.001306603945723374,\n",
       "    0.0011105938258397347,\n",
       "    0.001260020814148092,\n",
       "    0.0010122861624783362,\n",
       "    0.0009137591527237391,\n",
       "    0.0011276368576233291,\n",
       "    0.0014246765038228814,\n",
       "    0.0014316613232438966,\n",
       "    0.002110168982834353,\n",
       "    0.0010197165783093645,\n",
       "    0.0017872999117015376,\n",
       "    0.001014221605744278,\n",
       "    0.0019039927712897746,\n",
       "    0.0019629327849084274],\n",
       "   [0.016224554949234634,\n",
       "    0.01628723622996143,\n",
       "    0.007269901737076222,\n",
       "    0.007033267387552394,\n",
       "    0.003908160450267899,\n",
       "    0.0038561671689189697,\n",
       "    0.005529554306711348,\n",
       "    0.00421594950388027,\n",
       "    0.0042623204566508265,\n",
       "    0.0025595906161150336,\n",
       "    0.003560916868810602,\n",
       "    0.002859706372674062,\n",
       "    0.006102129967596006,\n",
       "    0.002135218415629066,\n",
       "    0.0015854390945981463,\n",
       "    0.002889286728352331,\n",
       "    0.0027941619539110004,\n",
       "    0.003589785894557395,\n",
       "    0.002289324920255069,\n",
       "    0.0018321129715213162,\n",
       "    0.0024355452608667227,\n",
       "    0.0024299405591990935,\n",
       "    0.0026684509690241935,\n",
       "    0.0016261293569615125,\n",
       "    0.0019512719343605124,\n",
       "    0.0008177401994044051,\n",
       "    0.002440569398874254,\n",
       "    0.0026577421567848293,\n",
       "    0.0012238994174772083,\n",
       "    0.0015349504160961434,\n",
       "    0.0023033134845815172,\n",
       "    0.0017518946264495753,\n",
       "    0.0018327140909584687,\n",
       "    0.00216132529223175,\n",
       "    0.0009562149397386772,\n",
       "    0.0012298817400078826,\n",
       "    0.000990316755235778,\n",
       "    0.0006869088797666452,\n",
       "    0.0022876464962767514,\n",
       "    0.0007007561756241531,\n",
       "    0.0014150436169528293,\n",
       "    0.0007052876108154238,\n",
       "    0.0005949069456368761,\n",
       "    0.0019868506417043486,\n",
       "    0.0008161666485470105,\n",
       "    0.0005792266307152214,\n",
       "    0.0009306460866654265,\n",
       "    0.0015625276068895245,\n",
       "    0.001739620123551034,\n",
       "    0.0021622029895486147],\n",
       "   [0.027307983103051565,\n",
       "    0.014043306425932794,\n",
       "    0.011784323950514253,\n",
       "    0.006689584431528141,\n",
       "    0.004156599586407893,\n",
       "    0.004463353170420588,\n",
       "    0.003748166246048533,\n",
       "    0.0059994689335557605,\n",
       "    0.0035177245379803073,\n",
       "    0.004333791122288028,\n",
       "    0.00266888699148424,\n",
       "    0.002670489696508134,\n",
       "    0.002880829545674493,\n",
       "    0.0021900769369719844,\n",
       "    0.001992333209908627,\n",
       "    0.003324640559364984,\n",
       "    0.004329347228460801,\n",
       "    0.002357266293682942,\n",
       "    0.003852172935134885,\n",
       "    0.0023004017582863153,\n",
       "    0.0022169594527397763,\n",
       "    0.002102867215211151,\n",
       "    0.0023258007390183975,\n",
       "    0.0022582923656970584,\n",
       "    0.0014696522832102613,\n",
       "    0.002033844482283322,\n",
       "    0.0021086861224478746,\n",
       "    0.001714303884962452,\n",
       "    0.0019008910482089477,\n",
       "    0.0024736650356334414,\n",
       "    0.0013376313171019182,\n",
       "    0.001715400967876929,\n",
       "    0.001513427484758949,\n",
       "    0.0018161356528952018,\n",
       "    0.0009593343769643775,\n",
       "    0.002603763091412414,\n",
       "    0.0022392353850657336,\n",
       "    0.0019071144925662986,\n",
       "    0.0016588708806855046,\n",
       "    0.0010980881181432158,\n",
       "    0.0011394631134844535,\n",
       "    0.0015650630457506045,\n",
       "    0.001163281888366472,\n",
       "    0.0014985449028984623,\n",
       "    0.002050987443111405,\n",
       "    0.0019039225241178257,\n",
       "    0.0015107027685987613,\n",
       "    0.001503868310037423,\n",
       "    0.0013899458933517646,\n",
       "    0.001698606392733505]]},\n",
       " 'MNIST MLP on IID': {'hyperparams': {'C': 0.1,\n",
       "   'E': 25,\n",
       "   'K': 100,\n",
       "   'batch_size': 10,\n",
       "   'lr': 0.05,\n",
       "   'rounds': 50},\n",
       "  'test_accuracy': [[89.49367088607595,\n",
       "    93.54430379746836,\n",
       "    94.0506329113924,\n",
       "    94.81012658227849,\n",
       "    94.81012658227849,\n",
       "    95.44303797468355,\n",
       "    95.18987341772151,\n",
       "    95.69620253164557,\n",
       "    96.07594936708861,\n",
       "    96.70886075949367,\n",
       "    96.20253164556962,\n",
       "    96.20253164556962,\n",
       "    96.58227848101266,\n",
       "    96.70886075949367,\n",
       "    96.83544303797468,\n",
       "    97.21518987341773,\n",
       "    96.9620253164557,\n",
       "    97.46835443037975,\n",
       "    97.0886075949367,\n",
       "    97.34177215189874,\n",
       "    97.59493670886076,\n",
       "    97.46835443037975,\n",
       "    97.34177215189874,\n",
       "    97.21518987341773,\n",
       "    97.21518987341773,\n",
       "    97.46835443037975,\n",
       "    97.59493670886076,\n",
       "    97.84810126582279,\n",
       "    97.84810126582279,\n",
       "    97.9746835443038,\n",
       "    97.72151898734177,\n",
       "    97.9746835443038,\n",
       "    97.84810126582279,\n",
       "    98.10126582278481,\n",
       "    98.22784810126582,\n",
       "    98.35443037974683,\n",
       "    97.84810126582279,\n",
       "    97.72151898734177,\n",
       "    98.35443037974683,\n",
       "    97.9746835443038,\n",
       "    98.22784810126582,\n",
       "    97.84810126582279,\n",
       "    98.35443037974683,\n",
       "    98.60759493670886,\n",
       "    98.22784810126582,\n",
       "    97.9746835443038,\n",
       "    98.10126582278481,\n",
       "    97.84810126582279,\n",
       "    97.72151898734177,\n",
       "    97.9746835443038],\n",
       "   [90.50632911392405,\n",
       "    94.17721518987342,\n",
       "    94.30379746835443,\n",
       "    95.0632911392405,\n",
       "    95.0632911392405,\n",
       "    95.44303797468355,\n",
       "    95.82278481012658,\n",
       "    95.56962025316456,\n",
       "    96.07594936708861,\n",
       "    96.32911392405063,\n",
       "    96.20253164556962,\n",
       "    96.45569620253164,\n",
       "    96.32911392405063,\n",
       "    96.70886075949367,\n",
       "    97.34177215189874,\n",
       "    97.0886075949367,\n",
       "    97.0886075949367,\n",
       "    96.70886075949367,\n",
       "    97.0886075949367,\n",
       "    96.9620253164557,\n",
       "    97.34177215189874,\n",
       "    97.34177215189874,\n",
       "    97.46835443037975,\n",
       "    97.72151898734177,\n",
       "    97.46835443037975,\n",
       "    97.59493670886076,\n",
       "    97.34177215189874,\n",
       "    97.72151898734177,\n",
       "    97.46835443037975,\n",
       "    97.21518987341773,\n",
       "    97.9746835443038,\n",
       "    97.72151898734177,\n",
       "    98.22784810126582,\n",
       "    97.9746835443038,\n",
       "    97.84810126582279,\n",
       "    97.46835443037975,\n",
       "    97.59493670886076,\n",
       "    97.46835443037975,\n",
       "    97.59493670886076,\n",
       "    97.46835443037975,\n",
       "    97.59493670886076,\n",
       "    97.46835443037975,\n",
       "    97.84810126582279,\n",
       "    97.9746835443038,\n",
       "    97.46835443037975,\n",
       "    97.84810126582279,\n",
       "    98.22784810126582,\n",
       "    97.84810126582279,\n",
       "    97.9746835443038,\n",
       "    98.10126582278481],\n",
       "   [91.0126582278481,\n",
       "    93.79746835443038,\n",
       "    94.30379746835443,\n",
       "    95.18987341772151,\n",
       "    95.31645569620254,\n",
       "    95.69620253164557,\n",
       "    96.07594936708861,\n",
       "    95.9493670886076,\n",
       "    95.9493670886076,\n",
       "    96.45569620253164,\n",
       "    96.32911392405063,\n",
       "    96.20253164556962,\n",
       "    96.45569620253164,\n",
       "    96.58227848101266,\n",
       "    96.70886075949367,\n",
       "    96.70886075949367,\n",
       "    96.9620253164557,\n",
       "    96.83544303797468,\n",
       "    97.34177215189874,\n",
       "    96.9620253164557,\n",
       "    96.83544303797468,\n",
       "    96.83544303797468,\n",
       "    96.9620253164557,\n",
       "    96.83544303797468,\n",
       "    96.70886075949367,\n",
       "    96.9620253164557,\n",
       "    97.34177215189874,\n",
       "    97.34177215189874,\n",
       "    97.21518987341773,\n",
       "    97.34177215189874,\n",
       "    97.0886075949367,\n",
       "    97.0886075949367,\n",
       "    97.0886075949367,\n",
       "    97.34177215189874,\n",
       "    97.21518987341773,\n",
       "    97.21518987341773,\n",
       "    97.34177215189874,\n",
       "    97.0886075949367,\n",
       "    97.59493670886076,\n",
       "    97.72151898734177,\n",
       "    97.84810126582279,\n",
       "    97.46835443037975,\n",
       "    97.59493670886076,\n",
       "    97.72151898734177,\n",
       "    97.59493670886076,\n",
       "    97.59493670886076,\n",
       "    97.84810126582279,\n",
       "    97.59493670886076,\n",
       "    97.59493670886076,\n",
       "    97.72151898734177],\n",
       "   [77.46835443037975,\n",
       "    93.54430379746836,\n",
       "    94.17721518987342,\n",
       "    94.43037974683544,\n",
       "    94.81012658227849,\n",
       "    95.0632911392405,\n",
       "    95.31645569620254,\n",
       "    95.31645569620254,\n",
       "    95.69620253164557,\n",
       "    96.20253164556962,\n",
       "    96.07594936708861,\n",
       "    95.82278481012658,\n",
       "    96.07594936708861,\n",
       "    96.45569620253164,\n",
       "    95.9493670886076,\n",
       "    96.32911392405063,\n",
       "    95.9493670886076,\n",
       "    96.45569620253164,\n",
       "    96.45569620253164,\n",
       "    96.70886075949367,\n",
       "    96.58227848101266,\n",
       "    96.70886075949367,\n",
       "    96.70886075949367,\n",
       "    97.0886075949367,\n",
       "    97.0886075949367,\n",
       "    97.21518987341773,\n",
       "    96.83544303797468,\n",
       "    96.83544303797468,\n",
       "    97.34177215189874,\n",
       "    96.83544303797468,\n",
       "    97.72151898734177,\n",
       "    96.9620253164557,\n",
       "    97.0886075949367,\n",
       "    97.0886075949367,\n",
       "    97.21518987341773,\n",
       "    97.46835443037975,\n",
       "    97.46835443037975,\n",
       "    97.46835443037975,\n",
       "    97.9746835443038,\n",
       "    97.59493670886076,\n",
       "    97.59493670886076,\n",
       "    97.84810126582279,\n",
       "    97.59493670886076,\n",
       "    98.10126582278481,\n",
       "    97.84810126582279,\n",
       "    98.22784810126582,\n",
       "    97.84810126582279,\n",
       "    97.72151898734177,\n",
       "    97.72151898734177,\n",
       "    97.9746835443038],\n",
       "   [92.0253164556962,\n",
       "    94.17721518987342,\n",
       "    94.68354430379746,\n",
       "    94.55696202531645,\n",
       "    95.0632911392405,\n",
       "    96.07594936708861,\n",
       "    96.07594936708861,\n",
       "    95.82278481012658,\n",
       "    96.07594936708861,\n",
       "    96.32911392405063,\n",
       "    96.32911392405063,\n",
       "    96.45569620253164,\n",
       "    96.45569620253164,\n",
       "    96.45569620253164,\n",
       "    96.70886075949367,\n",
       "    96.58227848101266,\n",
       "    96.20253164556962,\n",
       "    96.83544303797468,\n",
       "    97.21518987341773,\n",
       "    96.9620253164557,\n",
       "    96.9620253164557,\n",
       "    97.0886075949367,\n",
       "    97.0886075949367,\n",
       "    97.0886075949367,\n",
       "    97.0886075949367,\n",
       "    97.34177215189874,\n",
       "    97.34177215189874,\n",
       "    96.9620253164557,\n",
       "    96.9620253164557,\n",
       "    97.59493670886076,\n",
       "    97.46835443037975,\n",
       "    97.59493670886076,\n",
       "    97.59493670886076,\n",
       "    97.84810126582279,\n",
       "    97.21518987341773,\n",
       "    97.72151898734177,\n",
       "    97.46835443037975,\n",
       "    97.72151898734177,\n",
       "    97.72151898734177,\n",
       "    97.72151898734177,\n",
       "    97.72151898734177,\n",
       "    97.59493670886076,\n",
       "    97.59493670886076,\n",
       "    98.10126582278481,\n",
       "    97.72151898734177,\n",
       "    98.48101265822785,\n",
       "    98.10126582278481,\n",
       "    97.9746835443038,\n",
       "    97.9746835443038,\n",
       "    98.10126582278481]],\n",
       "  'test_loss': [[0.3998874323487282,\n",
       "    0.29849631009250877,\n",
       "    0.26366255294047297,\n",
       "    0.23501063798889518,\n",
       "    0.2043720680627972,\n",
       "    0.19075716662500053,\n",
       "    0.18138547869501637,\n",
       "    0.16972688426570967,\n",
       "    0.15877126628425903,\n",
       "    0.15198901269789786,\n",
       "    0.14768370666247793,\n",
       "    0.1402444644194562,\n",
       "    0.13868144053234718,\n",
       "    0.13478586453148164,\n",
       "    0.12573534572727513,\n",
       "    0.1220393750645453,\n",
       "    0.11591247373899678,\n",
       "    0.11796428770608036,\n",
       "    0.12049446818177821,\n",
       "    0.11561779060491827,\n",
       "    0.11298444755985401,\n",
       "    0.11112723077789415,\n",
       "    0.10688478525881656,\n",
       "    0.10673196095720631,\n",
       "    0.1087639197946468,\n",
       "    0.10577813849769299,\n",
       "    0.1007143512143055,\n",
       "    0.10115071863048361,\n",
       "    0.10223387448204449,\n",
       "    0.09693130307415268,\n",
       "    0.09914651307169116,\n",
       "    0.1009302409766009,\n",
       "    0.09567809555813438,\n",
       "    0.0955778200218454,\n",
       "    0.09861369981989847,\n",
       "    0.09680794378201245,\n",
       "    0.09970772780858096,\n",
       "    0.09740538050577743,\n",
       "    0.09624703405051259,\n",
       "    0.09531569670060126,\n",
       "    0.09536729142330878,\n",
       "    0.0909946096866508,\n",
       "    0.08962565177989891,\n",
       "    0.08910584795433679,\n",
       "    0.09263319768889341,\n",
       "    0.09308923865136458,\n",
       "    0.09244548854075837,\n",
       "    0.09180879139525641,\n",
       "    0.09328478444542852,\n",
       "    0.09182671376223735],\n",
       "   [0.38613013032674787,\n",
       "    0.29506724101901055,\n",
       "    0.2586322177454829,\n",
       "    0.2252899452995509,\n",
       "    0.20474605783941224,\n",
       "    0.19334751071352513,\n",
       "    0.17096276761423795,\n",
       "    0.16114067930122838,\n",
       "    0.15778577597523108,\n",
       "    0.1476578231123276,\n",
       "    0.14396513945274056,\n",
       "    0.13344658225029707,\n",
       "    0.12911148289185947,\n",
       "    0.1290062235496007,\n",
       "    0.1265626229001209,\n",
       "    0.12207141177803278,\n",
       "    0.12256794278756715,\n",
       "    0.12034497650358826,\n",
       "    0.11766075585605576,\n",
       "    0.11052010097638704,\n",
       "    0.10909750785834621,\n",
       "    0.1062930135234259,\n",
       "    0.10400905939671211,\n",
       "    0.10653397575832205,\n",
       "    0.10115164842519443,\n",
       "    0.10250754237554502,\n",
       "    0.09931905516701518,\n",
       "    0.10318589479731163,\n",
       "    0.0984970510994899,\n",
       "    0.10063463989500887,\n",
       "    0.09688631337628467,\n",
       "    0.09585033002191921,\n",
       "    0.09430710767755518,\n",
       "    0.09474589951445232,\n",
       "    0.0969669850447739,\n",
       "    0.09673592530989554,\n",
       "    0.09886514754914678,\n",
       "    0.09902301326539309,\n",
       "    0.09655935979813221,\n",
       "    0.0955483202695148,\n",
       "    0.09662355841113604,\n",
       "    0.09656526169874996,\n",
       "    0.09667033875728666,\n",
       "    0.09442767793818785,\n",
       "    0.09195353856619767,\n",
       "    0.0937305129627377,\n",
       "    0.09390604358374112,\n",
       "    0.0920084165166576,\n",
       "    0.09106805447631196,\n",
       "    0.09179488492308155],\n",
       "   [0.3672346967816353,\n",
       "    0.30779059766083955,\n",
       "    0.26648512004464864,\n",
       "    0.2356060826331377,\n",
       "    0.2047323587924242,\n",
       "    0.1890041177354753,\n",
       "    0.17531682275012136,\n",
       "    0.16654525052066893,\n",
       "    0.15702685817396267,\n",
       "    0.1462884272721596,\n",
       "    0.14280826326897367,\n",
       "    0.14018302194993013,\n",
       "    0.13894267923925072,\n",
       "    0.13244349802639335,\n",
       "    0.12611207460770382,\n",
       "    0.12282506812830689,\n",
       "    0.12129629833748332,\n",
       "    0.11753362236979883,\n",
       "    0.11397380456682295,\n",
       "    0.11270118051113096,\n",
       "    0.11229200739698718,\n",
       "    0.11472263943186263,\n",
       "    0.11094325302119833,\n",
       "    0.10861572528039105,\n",
       "    0.11032771236139816,\n",
       "    0.10529146446656086,\n",
       "    0.10371081379650277,\n",
       "    0.10641477073708666,\n",
       "    0.09890812550459523,\n",
       "    0.09864236602780875,\n",
       "    0.10146055869582923,\n",
       "    0.10262491580735077,\n",
       "    0.09963464642394974,\n",
       "    0.09988795563648746,\n",
       "    0.09441280778717483,\n",
       "    0.09267274838465674,\n",
       "    0.0937310987189645,\n",
       "    0.09371264422960231,\n",
       "    0.091077097002964,\n",
       "    0.09378829620883626,\n",
       "    0.09060365959436167,\n",
       "    0.09261187526791764,\n",
       "    0.09279579445088165,\n",
       "    0.09105982665971678,\n",
       "    0.0900001229282585,\n",
       "    0.09083330909725627,\n",
       "    0.08973717180626117,\n",
       "    0.09145766969094984,\n",
       "    0.09277705435367098,\n",
       "    0.0895121483032839],\n",
       "   [1.232120061302185,\n",
       "    0.3313026885330677,\n",
       "    0.27623827598392964,\n",
       "    0.23535203642565758,\n",
       "    0.2140451546959579,\n",
       "    0.1940296437099576,\n",
       "    0.18026979054464026,\n",
       "    0.17182693483373151,\n",
       "    0.16542089852504432,\n",
       "    0.15658403881900013,\n",
       "    0.15143554815882818,\n",
       "    0.14374375598644837,\n",
       "    0.13893948971740902,\n",
       "    0.1367464759866707,\n",
       "    0.13288212066857377,\n",
       "    0.132606286961399,\n",
       "    0.13246691613108852,\n",
       "    0.1275463868068764,\n",
       "    0.11981791426439303,\n",
       "    0.11432354965377599,\n",
       "    0.11528531143909786,\n",
       "    0.1158618887604447,\n",
       "    0.11523104427735088,\n",
       "    0.11082406360840541,\n",
       "    0.10887521066399058,\n",
       "    0.10768696407673414,\n",
       "    0.10704414970058715,\n",
       "    0.10532242990402738,\n",
       "    0.10721921764067374,\n",
       "    0.10581956619468401,\n",
       "    0.10208259596479474,\n",
       "    0.10069452522589127,\n",
       "    0.10230280244204332,\n",
       "    0.10164515409003361,\n",
       "    0.09747604658990167,\n",
       "    0.09847241176408134,\n",
       "    0.09815940203821519,\n",
       "    0.09656549713576969,\n",
       "    0.09484994381825963,\n",
       "    0.09385579251126328,\n",
       "    0.0943698338293063,\n",
       "    0.09607685849436966,\n",
       "    0.0915304372383398,\n",
       "    0.0925476775086252,\n",
       "    0.09454439486832707,\n",
       "    0.09080621326606779,\n",
       "    0.09259981459476985,\n",
       "    0.09502438936108375,\n",
       "    0.09196935601925943,\n",
       "    0.09104801610166906],\n",
       "   [0.33662374634742737,\n",
       "    0.28132598460912706,\n",
       "    0.23977525258883833,\n",
       "    0.2164192764353007,\n",
       "    0.18983789681773633,\n",
       "    0.17162248500436544,\n",
       "    0.16277187077961863,\n",
       "    0.1600945379110053,\n",
       "    0.15162733076876028,\n",
       "    0.14560784139265306,\n",
       "    0.13972749305816834,\n",
       "    0.13231161668472924,\n",
       "    0.12822607913671527,\n",
       "    0.12188907552892342,\n",
       "    0.11915981882647611,\n",
       "    0.1173425824950682,\n",
       "    0.11726866283603013,\n",
       "    0.11119083055527881,\n",
       "    0.11148580514383502,\n",
       "    0.10756944337068125,\n",
       "    0.10984854447075632,\n",
       "    0.10607828082863707,\n",
       "    0.10598070629006252,\n",
       "    0.10699312103505945,\n",
       "    0.10479510732747149,\n",
       "    0.10162363748257049,\n",
       "    0.10294954734481872,\n",
       "    0.10272801384679042,\n",
       "    0.10293743504718877,\n",
       "    0.10006714655491523,\n",
       "    0.09755333575259428,\n",
       "    0.09843124524032464,\n",
       "    0.09688268909459002,\n",
       "    0.09588869287541602,\n",
       "    0.09565266707602423,\n",
       "    0.09590280916731572,\n",
       "    0.09598697109799832,\n",
       "    0.09337944281727541,\n",
       "    0.09176935396254994,\n",
       "    0.09455052606225946,\n",
       "    0.09454875155878253,\n",
       "    0.09383379315123894,\n",
       "    0.09386908052411745,\n",
       "    0.09271088231947505,\n",
       "    0.09060684870160185,\n",
       "    0.09355745469233953,\n",
       "    0.08989140870772536,\n",
       "    0.08953178959218785,\n",
       "    0.09106640524925896,\n",
       "    0.09062258916908177]],\n",
       "  'train_loss': [[0.13768185851029968,\n",
       "    0.03860434186558966,\n",
       "    0.02808000974059499,\n",
       "    0.021896094611774487,\n",
       "    0.019804063207489418,\n",
       "    0.018992910089823806,\n",
       "    0.016144475030809808,\n",
       "    0.014521931056831302,\n",
       "    0.013349700612569107,\n",
       "    0.013449584541104124,\n",
       "    0.013281429217022142,\n",
       "    0.011399903263406985,\n",
       "    0.010356067976782055,\n",
       "    0.00923314413560898,\n",
       "    0.01408776072809848,\n",
       "    0.012059839654492964,\n",
       "    0.00962449006027959,\n",
       "    0.007954706920907679,\n",
       "    0.008148450622399515,\n",
       "    0.009539333446511504,\n",
       "    0.00807695008359107,\n",
       "    0.007265141460439906,\n",
       "    0.00860029114287817,\n",
       "    0.006708016477098383,\n",
       "    0.00666122853667418,\n",
       "    0.007429730738715526,\n",
       "    0.006397818367735389,\n",
       "    0.0064567159720290215,\n",
       "    0.005886094577205045,\n",
       "    0.00596568468442118,\n",
       "    0.005053776640701583,\n",
       "    0.005935827199300705,\n",
       "    0.006350693122829071,\n",
       "    0.004479372227631197,\n",
       "    0.005834677035848411,\n",
       "    0.005102998566746465,\n",
       "    0.005008662821522862,\n",
       "    0.00533131506885909,\n",
       "    0.004478874370993064,\n",
       "    0.004631557752336616,\n",
       "    0.005804176996940421,\n",
       "    0.006208571648948069,\n",
       "    0.0052289408216713365,\n",
       "    0.003995435317911898,\n",
       "    0.004913070381491912,\n",
       "    0.0030815971692198206,\n",
       "    0.004193119528221763,\n",
       "    0.004360050733690111,\n",
       "    0.004517499891481802,\n",
       "    0.0030017378866468874],\n",
       "   [0.14295565672617522,\n",
       "    0.03622299518952023,\n",
       "    0.029051377506816422,\n",
       "    0.021782439874948902,\n",
       "    0.02092764911064823,\n",
       "    0.017232822416070315,\n",
       "    0.01638507235293744,\n",
       "    0.01493667820374776,\n",
       "    0.012984050992370924,\n",
       "    0.01409794181764537,\n",
       "    0.012652979037702305,\n",
       "    0.01145064675367265,\n",
       "    0.012007713023962568,\n",
       "    0.009210354833035925,\n",
       "    0.010041825946921441,\n",
       "    0.009970764023095866,\n",
       "    0.0077693085935757146,\n",
       "    0.009293005060246624,\n",
       "    0.008567616014299265,\n",
       "    0.009627331778186733,\n",
       "    0.00843487062595286,\n",
       "    0.008467390485806654,\n",
       "    0.007835154855839323,\n",
       "    0.007267603341139348,\n",
       "    0.006835126438805933,\n",
       "    0.0073707463296568045,\n",
       "    0.007800772899251765,\n",
       "    0.0055338956822553855,\n",
       "    0.006960008358804268,\n",
       "    0.005620087656503399,\n",
       "    0.006686043549265173,\n",
       "    0.00560632745069547,\n",
       "    0.006656586464158526,\n",
       "    0.005470703905031802,\n",
       "    0.005935476906735933,\n",
       "    0.005877993292545182,\n",
       "    0.0036398527595214065,\n",
       "    0.005453214779852322,\n",
       "    0.005033592690694439,\n",
       "    0.004237073425176683,\n",
       "    0.004353545145188956,\n",
       "    0.004487727034917879,\n",
       "    0.004069962589665573,\n",
       "    0.005251511860171858,\n",
       "    0.004817267939876262,\n",
       "    0.003118865991022965,\n",
       "    0.002739135433308783,\n",
       "    0.00391375965198259,\n",
       "    0.004055875941470368,\n",
       "    0.0045618365396546635],\n",
       "   [0.10540897663636024,\n",
       "    0.035969500035100174,\n",
       "    0.028423388682521544,\n",
       "    0.022383901688774487,\n",
       "    0.021259155974716244,\n",
       "    0.015368118787950757,\n",
       "    0.01823019606534866,\n",
       "    0.014601554033591053,\n",
       "    0.015688117996750715,\n",
       "    0.014678702744886946,\n",
       "    0.01421456441379378,\n",
       "    0.010118048430253802,\n",
       "    0.009859434070999873,\n",
       "    0.00980261969815199,\n",
       "    0.011385703576617173,\n",
       "    0.009495828353005876,\n",
       "    0.0076084482649692335,\n",
       "    0.00863825792803715,\n",
       "    0.010270740013045575,\n",
       "    0.007954084907097408,\n",
       "    0.009259491189826531,\n",
       "    0.005808848791044689,\n",
       "    0.008339318137515287,\n",
       "    0.00738402924277656,\n",
       "    0.0073893148935101375,\n",
       "    0.0074094547980113525,\n",
       "    0.005691348298107459,\n",
       "    0.006243868055023609,\n",
       "    0.007191757707665752,\n",
       "    0.007108124589645615,\n",
       "    0.006683518462159903,\n",
       "    0.004484049309785536,\n",
       "    0.00597105851301105,\n",
       "    0.005428617524094102,\n",
       "    0.00546259430563386,\n",
       "    0.005410027177610732,\n",
       "    0.004886991167362047,\n",
       "    0.005211406530381805,\n",
       "    0.0064206215037873795,\n",
       "    0.0053639557562035695,\n",
       "    0.005524823705545021,\n",
       "    0.003444329890420942,\n",
       "    0.004785918261327576,\n",
       "    0.004767004954287092,\n",
       "    0.0043438830365316984,\n",
       "    0.004211574931564782,\n",
       "    0.00388852374123374,\n",
       "    0.00488938298604519,\n",
       "    0.004143694066791414,\n",
       "    0.0038410018303381655],\n",
       "   [0.916932559174336,\n",
       "    0.047533020410999174,\n",
       "    0.031861343512792725,\n",
       "    0.022556747842169322,\n",
       "    0.021870844321917077,\n",
       "    0.018455499078293025,\n",
       "    0.017139515529423922,\n",
       "    0.015237147300894283,\n",
       "    0.012716574309852286,\n",
       "    0.012483806243608706,\n",
       "    0.011807128420025379,\n",
       "    0.012127380228684266,\n",
       "    0.011695147332271902,\n",
       "    0.011062641258521324,\n",
       "    0.011517519089041982,\n",
       "    0.010029405589828259,\n",
       "    0.0070230386253504916,\n",
       "    0.00883017694309208,\n",
       "    0.00882736442296501,\n",
       "    0.012390611466007062,\n",
       "    0.008338019899401219,\n",
       "    0.006621278324284904,\n",
       "    0.007060155805492756,\n",
       "    0.007413435563320058,\n",
       "    0.008321820612028838,\n",
       "    0.00604031584044931,\n",
       "    0.006867216438013043,\n",
       "    0.007027391987484431,\n",
       "    0.006354750603132836,\n",
       "    0.004716506493560715,\n",
       "    0.006705272554294916,\n",
       "    0.0066458517939327905,\n",
       "    0.006372491918580985,\n",
       "    0.005874004790682918,\n",
       "    0.005329519779407441,\n",
       "    0.007726472781897267,\n",
       "    0.004421187854468166,\n",
       "    0.004870747325045402,\n",
       "    0.005809840663847017,\n",
       "    0.0048218620550794945,\n",
       "    0.005341457388551104,\n",
       "    0.006116659019282791,\n",
       "    0.00542462811673507,\n",
       "    0.005123995047835962,\n",
       "    0.005404184720580049,\n",
       "    0.004878704547373912,\n",
       "    0.005182747834393243,\n",
       "    0.003908318948045483,\n",
       "    0.004390705033274337,\n",
       "    0.004597311968768597],\n",
       "   [0.11079484599365826,\n",
       "    0.03741104747582043,\n",
       "    0.025801678235190223,\n",
       "    0.02188767255977115,\n",
       "    0.020098399766813053,\n",
       "    0.01926896021051583,\n",
       "    0.01644111040511225,\n",
       "    0.014353921284299334,\n",
       "    0.014750382813270835,\n",
       "    0.014278616626804283,\n",
       "    0.011990338857936414,\n",
       "    0.01065995732144267,\n",
       "    0.01121570775011913,\n",
       "    0.011683881322921206,\n",
       "    0.010633183104446803,\n",
       "    0.010142263745450754,\n",
       "    0.00792297952257689,\n",
       "    0.010406320001356844,\n",
       "    0.009344756037575862,\n",
       "    0.010936629658736662,\n",
       "    0.0083952513958436,\n",
       "    0.007564556252555185,\n",
       "    0.0058972587056450865,\n",
       "    0.0055486984501900965,\n",
       "    0.007794471960188021,\n",
       "    0.007396329683541652,\n",
       "    0.007001579668511166,\n",
       "    0.005677530283985908,\n",
       "    0.007880438759852035,\n",
       "    0.005632222789771615,\n",
       "    0.00587066982486977,\n",
       "    0.005324772476468576,\n",
       "    0.005822225017856237,\n",
       "    0.005907604035794049,\n",
       "    0.005023499508395445,\n",
       "    0.006772335095238141,\n",
       "    0.004852075728064712,\n",
       "    0.005541037771065713,\n",
       "    0.004463447306416099,\n",
       "    0.004848866608452174,\n",
       "    0.005685460560836955,\n",
       "    0.004958857664687048,\n",
       "    0.004473452703463046,\n",
       "    0.005022175040585011,\n",
       "    0.00670964545051574,\n",
       "    0.002849694433378569,\n",
       "    0.0047609197260250966,\n",
       "    0.003605993821180805,\n",
       "    0.004563832522123483,\n",
       "    0.004608088446423228]]},\n",
       " 'MNIST MLP on Non IID': {'hyperparams': {'C': 0.1,\n",
       "   'E': 25,\n",
       "   'K': 100,\n",
       "   'batch_size': 10,\n",
       "   'lr': 0.05,\n",
       "   'rounds': 50},\n",
       "  'test_accuracy': [[24.430379746835442,\n",
       "    36.20253164556962,\n",
       "    46.962025316455694,\n",
       "    49.620253164556964,\n",
       "    72.15189873417721,\n",
       "    78.22784810126582,\n",
       "    65.18987341772151,\n",
       "    83.29113924050633,\n",
       "    87.84810126582279,\n",
       "    84.55696202531645,\n",
       "    78.9873417721519,\n",
       "    86.32911392405063,\n",
       "    82.15189873417721,\n",
       "    90.50632911392405,\n",
       "    89.49367088607595,\n",
       "    86.45569620253164,\n",
       "    92.40506329113924,\n",
       "    91.64556962025317,\n",
       "    89.74683544303798,\n",
       "    90.12658227848101,\n",
       "    93.29113924050633,\n",
       "    88.60759493670886,\n",
       "    90.25316455696202,\n",
       "    94.0506329113924,\n",
       "    91.51898734177215,\n",
       "    91.0126582278481,\n",
       "    86.70886075949367,\n",
       "    88.60759493670886,\n",
       "    89.36708860759494,\n",
       "    93.92405063291139,\n",
       "    88.73417721518987,\n",
       "    93.29113924050633,\n",
       "    93.67088607594937,\n",
       "    95.0632911392405,\n",
       "    95.0632911392405,\n",
       "    94.30379746835443,\n",
       "    94.30379746835443,\n",
       "    94.0506329113924,\n",
       "    93.67088607594937,\n",
       "    94.9367088607595,\n",
       "    92.65822784810126,\n",
       "    93.29113924050633,\n",
       "    90.0,\n",
       "    92.53164556962025,\n",
       "    89.74683544303798,\n",
       "    94.0506329113924,\n",
       "    92.53164556962025,\n",
       "    95.82278481012658,\n",
       "    94.68354430379746,\n",
       "    94.43037974683544],\n",
       "   [37.721518987341774,\n",
       "    44.936708860759495,\n",
       "    49.36708860759494,\n",
       "    63.79746835443038,\n",
       "    58.9873417721519,\n",
       "    85.69620253164557,\n",
       "    69.24050632911393,\n",
       "    65.44303797468355,\n",
       "    76.83544303797468,\n",
       "    84.68354430379746,\n",
       "    85.18987341772151,\n",
       "    89.74683544303798,\n",
       "    88.9873417721519,\n",
       "    88.10126582278481,\n",
       "    90.25316455696202,\n",
       "    88.9873417721519,\n",
       "    91.39240506329114,\n",
       "    88.10126582278481,\n",
       "    88.86075949367088,\n",
       "    92.9113924050633,\n",
       "    89.11392405063292,\n",
       "    83.41772151898734,\n",
       "    90.50632911392405,\n",
       "    88.35443037974683,\n",
       "    92.78481012658227,\n",
       "    91.0126582278481,\n",
       "    91.89873417721519,\n",
       "    90.0,\n",
       "    92.15189873417721,\n",
       "    90.37974683544304,\n",
       "    93.67088607594937,\n",
       "    92.65822784810126,\n",
       "    94.9367088607595,\n",
       "    93.92405063291139,\n",
       "    94.81012658227849,\n",
       "    92.65822784810126,\n",
       "    85.0632911392405,\n",
       "    93.79746835443038,\n",
       "    94.0506329113924,\n",
       "    94.55696202531645,\n",
       "    93.92405063291139,\n",
       "    89.49367088607595,\n",
       "    90.75949367088607,\n",
       "    95.31645569620254,\n",
       "    94.68354430379746,\n",
       "    95.82278481012658,\n",
       "    95.82278481012658,\n",
       "    94.17721518987342,\n",
       "    92.65822784810126,\n",
       "    91.26582278481013],\n",
       "   [32.78481012658228,\n",
       "    46.70886075949367,\n",
       "    58.10126582278481,\n",
       "    65.18987341772151,\n",
       "    64.30379746835443,\n",
       "    68.10126582278481,\n",
       "    78.48101265822785,\n",
       "    82.78481012658227,\n",
       "    76.45569620253164,\n",
       "    84.81012658227849,\n",
       "    86.32911392405063,\n",
       "    89.24050632911393,\n",
       "    89.36708860759494,\n",
       "    83.92405063291139,\n",
       "    88.60759493670886,\n",
       "    85.82278481012658,\n",
       "    85.82278481012658,\n",
       "    92.65822784810126,\n",
       "    92.65822784810126,\n",
       "    86.32911392405063,\n",
       "    89.87341772151899,\n",
       "    91.13924050632912,\n",
       "    89.74683544303798,\n",
       "    86.70886075949367,\n",
       "    91.13924050632912,\n",
       "    85.31645569620254,\n",
       "    91.89873417721519,\n",
       "    93.29113924050633,\n",
       "    90.63291139240506,\n",
       "    91.39240506329114,\n",
       "    92.27848101265823,\n",
       "    91.89873417721519,\n",
       "    87.9746835443038,\n",
       "    91.13924050632912,\n",
       "    93.54430379746836,\n",
       "    93.67088607594937,\n",
       "    92.27848101265823,\n",
       "    92.40506329113924,\n",
       "    91.26582278481013,\n",
       "    89.74683544303798,\n",
       "    94.81012658227849,\n",
       "    88.86075949367088,\n",
       "    93.67088607594937,\n",
       "    93.79746835443038,\n",
       "    95.69620253164557,\n",
       "    94.30379746835443,\n",
       "    92.9113924050633,\n",
       "    95.31645569620254,\n",
       "    91.89873417721519,\n",
       "    88.9873417721519],\n",
       "   [35.44303797468354,\n",
       "    37.34177215189873,\n",
       "    52.278481012658226,\n",
       "    65.56962025316456,\n",
       "    66.70886075949367,\n",
       "    70.37974683544304,\n",
       "    73.79746835443038,\n",
       "    74.0506329113924,\n",
       "    79.11392405063292,\n",
       "    86.9620253164557,\n",
       "    71.39240506329114,\n",
       "    81.39240506329114,\n",
       "    88.73417721518987,\n",
       "    88.22784810126582,\n",
       "    84.68354430379746,\n",
       "    89.11392405063292,\n",
       "    90.37974683544304,\n",
       "    88.9873417721519,\n",
       "    90.12658227848101,\n",
       "    87.34177215189874,\n",
       "    84.30379746835443,\n",
       "    83.29113924050633,\n",
       "    90.75949367088607,\n",
       "    91.39240506329114,\n",
       "    85.56962025316456,\n",
       "    87.72151898734177,\n",
       "    86.9620253164557,\n",
       "    91.0126582278481,\n",
       "    93.41772151898734,\n",
       "    88.73417721518987,\n",
       "    90.25316455696202,\n",
       "    94.30379746835443,\n",
       "    94.68354430379746,\n",
       "    94.9367088607595,\n",
       "    92.27848101265823,\n",
       "    90.37974683544304,\n",
       "    93.54430379746836,\n",
       "    92.15189873417721,\n",
       "    93.0379746835443,\n",
       "    92.27848101265823,\n",
       "    93.92405063291139,\n",
       "    94.17721518987342,\n",
       "    92.40506329113924,\n",
       "    94.43037974683544,\n",
       "    94.0506329113924,\n",
       "    92.78481012658227,\n",
       "    93.0379746835443,\n",
       "    91.26582278481013,\n",
       "    95.31645569620254,\n",
       "    91.77215189873418],\n",
       "   [27.848101265822784,\n",
       "    29.746835443037973,\n",
       "    41.89873417721519,\n",
       "    59.11392405063291,\n",
       "    80.25316455696202,\n",
       "    81.51898734177215,\n",
       "    86.58227848101266,\n",
       "    84.9367088607595,\n",
       "    80.75949367088607,\n",
       "    85.9493670886076,\n",
       "    87.9746835443038,\n",
       "    88.48101265822785,\n",
       "    83.79746835443038,\n",
       "    87.72151898734177,\n",
       "    90.75949367088607,\n",
       "    87.34177215189874,\n",
       "    88.60759493670886,\n",
       "    92.0253164556962,\n",
       "    91.13924050632912,\n",
       "    87.72151898734177,\n",
       "    85.31645569620254,\n",
       "    89.49367088607595,\n",
       "    91.77215189873418,\n",
       "    92.65822784810126,\n",
       "    93.67088607594937,\n",
       "    91.89873417721519,\n",
       "    93.54430379746836,\n",
       "    92.40506329113924,\n",
       "    93.54430379746836,\n",
       "    92.15189873417721,\n",
       "    89.36708860759494,\n",
       "    93.29113924050633,\n",
       "    91.64556962025317,\n",
       "    91.0126582278481,\n",
       "    93.67088607594937,\n",
       "    94.30379746835443,\n",
       "    93.92405063291139,\n",
       "    95.18987341772151,\n",
       "    93.41772151898734,\n",
       "    92.9113924050633,\n",
       "    93.67088607594937,\n",
       "    95.44303797468355,\n",
       "    92.65822784810126,\n",
       "    95.82278481012658,\n",
       "    93.16455696202532,\n",
       "    95.69620253164557,\n",
       "    93.41772151898734,\n",
       "    93.16455696202532,\n",
       "    94.30379746835443,\n",
       "    95.31645569620254]],\n",
       "  'test_loss': [[3.211466808319092,\n",
       "    2.440037929916382,\n",
       "    1.5806957279205323,\n",
       "    1.9710524589538574,\n",
       "    1.0925534722328185,\n",
       "    0.8721920816421509,\n",
       "    1.124656436920166,\n",
       "    0.578322723531723,\n",
       "    0.41881385073661803,\n",
       "    0.4697197283267975,\n",
       "    0.5960139557361602,\n",
       "    0.4391121573925018,\n",
       "    0.4934382332324982,\n",
       "    0.3226312026977539,\n",
       "    0.31757719683647156,\n",
       "    0.41442549369335174,\n",
       "    0.28412466917037965,\n",
       "    0.27921352021694185,\n",
       "    0.3210821277618408,\n",
       "    0.29795858500003813,\n",
       "    0.22071932743787764,\n",
       "    0.34133923817873,\n",
       "    0.31885679161548613,\n",
       "    0.22029717984199523,\n",
       "    0.26367933568954466,\n",
       "    0.23274084544181825,\n",
       "    0.42659221816062926,\n",
       "    0.3412103718757629,\n",
       "    0.31451859169006346,\n",
       "    0.23748286077976227,\n",
       "    0.3184240950345993,\n",
       "    0.2243468879878521,\n",
       "    0.1898471672773361,\n",
       "    0.16788790804743767,\n",
       "    0.1714109170436859,\n",
       "    0.1921802517950535,\n",
       "    0.18718309346437453,\n",
       "    0.2091276217699051,\n",
       "    0.21030041553378107,\n",
       "    0.16303039983510972,\n",
       "    0.20833103876113893,\n",
       "    0.1970717754483223,\n",
       "    0.2541792788505554,\n",
       "    0.20496943622231484,\n",
       "    0.25103090643882753,\n",
       "    0.19227300561070443,\n",
       "    0.2519197392463684,\n",
       "    0.16041624946594238,\n",
       "    0.1701305652678013,\n",
       "    0.18040124955773354],\n",
       "   [3.2000826309204102,\n",
       "    2.226947417831421,\n",
       "    2.0020593399047852,\n",
       "    1.3287826843261719,\n",
       "    1.4438793169021606,\n",
       "    0.5074618000984192,\n",
       "    0.8559747619628906,\n",
       "    0.9641824111938476,\n",
       "    0.6996998889446259,\n",
       "    0.5007975887298584,\n",
       "    0.4829730361700058,\n",
       "    0.34974532074928283,\n",
       "    0.34678960003852843,\n",
       "    0.37896441721916196,\n",
       "    0.3463399283409119,\n",
       "    0.3113425261259079,\n",
       "    0.25699844308495523,\n",
       "    0.40157281436920167,\n",
       "    0.3510239269733429,\n",
       "    0.24237874324321748,\n",
       "    0.31558289338946344,\n",
       "    0.43710850846767424,\n",
       "    0.2598809309959412,\n",
       "    0.32179854460954666,\n",
       "    0.21124550480246543,\n",
       "    0.28424318362474443,\n",
       "    0.2419725682258606,\n",
       "    0.28650082042217256,\n",
       "    0.22530275136232375,\n",
       "    0.26417937111854556,\n",
       "    0.18504366322159768,\n",
       "    0.21759239350557327,\n",
       "    0.1688619830727577,\n",
       "    0.24284532675743103,\n",
       "    0.1732129417002201,\n",
       "    0.2152087636232376,\n",
       "    0.38812638387680054,\n",
       "    0.18891328518390654,\n",
       "    0.2026258935213089,\n",
       "    0.15729790651798248,\n",
       "    0.17830314026772975,\n",
       "    0.287006956410408,\n",
       "    0.2681313404202461,\n",
       "    0.15745329610705375,\n",
       "    0.18497257348895074,\n",
       "    0.14230346356630325,\n",
       "    0.154361477291584,\n",
       "    0.163649317497015,\n",
       "    0.22618477203249931,\n",
       "    0.249847438621521],\n",
       "   [3.180377710723877,\n",
       "    2.0005974281311034,\n",
       "    1.6424777109146118,\n",
       "    1.1616586089134215,\n",
       "    1.0400219336509704,\n",
       "    1.0266050877571107,\n",
       "    0.5977907775878907,\n",
       "    0.5280208016157151,\n",
       "    0.6754839933395386,\n",
       "    0.43735485587120054,\n",
       "    0.433089275932312,\n",
       "    0.3204993253350258,\n",
       "    0.31851834950447083,\n",
       "    0.4955167452335358,\n",
       "    0.3702236823797226,\n",
       "    0.4198135456562042,\n",
       "    0.4120107635498047,\n",
       "    0.2483434973835945,\n",
       "    0.2454462722301483,\n",
       "    0.46440382442474365,\n",
       "    0.3265052290558815,\n",
       "    0.24710234010219573,\n",
       "    0.29728901295661925,\n",
       "    0.3610599673748016,\n",
       "    0.2854137184858322,\n",
       "    0.41511388993263243,\n",
       "    0.23684893556237221,\n",
       "    0.21178017471432686,\n",
       "    0.2894425059199333,\n",
       "    0.26754733721017837,\n",
       "    0.2413768935918808,\n",
       "    0.2600574659705162,\n",
       "    0.3440701526045799,\n",
       "    0.2978893373966217,\n",
       "    0.23630887985229493,\n",
       "    0.2210839517146349,\n",
       "    0.2588820510149002,\n",
       "    0.23723834364414215,\n",
       "    0.2505045457482338,\n",
       "    0.32962028679847716,\n",
       "    0.19890056524276734,\n",
       "    0.3612187172859907,\n",
       "    0.21423447493314743,\n",
       "    0.17866832474768163,\n",
       "    0.15117650768756866,\n",
       "    0.20009826224446298,\n",
       "    0.21261879947781562,\n",
       "    0.17116418641805647,\n",
       "    0.22113812076449393,\n",
       "    0.3572878188252449],\n",
       "   [3.1243315204620363,\n",
       "    2.396944733428955,\n",
       "    1.9594605354309083,\n",
       "    1.1964530221939087,\n",
       "    1.1702513277053832,\n",
       "    0.879296981048584,\n",
       "    0.8078449784278869,\n",
       "    1.018781034564972,\n",
       "    0.5870121677875518,\n",
       "    0.43845161657333376,\n",
       "    0.8526120369911194,\n",
       "    0.521252805519104,\n",
       "    0.3824421925067902,\n",
       "    0.3939905118227005,\n",
       "    0.44413011240959166,\n",
       "    0.3241149557590485,\n",
       "    0.29116639063358307,\n",
       "    0.2968659033536911,\n",
       "    0.2982568356990814,\n",
       "    0.35474302740097047,\n",
       "    0.41622312841415404,\n",
       "    0.4479220235824585,\n",
       "    0.282890891289711,\n",
       "    0.2738590249419212,\n",
       "    0.45849047694206235,\n",
       "    0.36690534796714785,\n",
       "    0.4186299406528473,\n",
       "    0.283189230632782,\n",
       "    0.20849934638738632,\n",
       "    0.3382218882083893,\n",
       "    0.24285341296195984,\n",
       "    0.1921694076061249,\n",
       "    0.19559880497455598,\n",
       "    0.20731978957653047,\n",
       "    0.2644314572572708,\n",
       "    0.31461697850227355,\n",
       "    0.23188830622434617,\n",
       "    0.2356364013671875,\n",
       "    0.22298549790382385,\n",
       "    0.24732575545310975,\n",
       "    0.1949216500043869,\n",
       "    0.21351965707540513,\n",
       "    0.1932534511089325,\n",
       "    0.17918334275484085,\n",
       "    0.17735672479867934,\n",
       "    0.22252087020874023,\n",
       "    0.2130664847612381,\n",
       "    0.25088282051086425,\n",
       "    0.16732856793999673,\n",
       "    0.2349668442964554],\n",
       "   [3.2489788696289064,\n",
       "    2.419785970687866,\n",
       "    2.6788314746856687,\n",
       "    1.3444061614990235,\n",
       "    0.7161176432132721,\n",
       "    0.5658573331832886,\n",
       "    0.4341959025144577,\n",
       "    0.4957139376401901,\n",
       "    0.5913721939086914,\n",
       "    0.43893785724639894,\n",
       "    0.39178344962596895,\n",
       "    0.3457958120346069,\n",
       "    0.4793424766540527,\n",
       "    0.37267420127391815,\n",
       "    0.3364988082408905,\n",
       "    0.4196980010032654,\n",
       "    0.34313871121406553,\n",
       "    0.28550146903991697,\n",
       "    0.2560589779019356,\n",
       "    0.3404467246294022,\n",
       "    0.4268066470146179,\n",
       "    0.325323734498024,\n",
       "    0.26033172507286073,\n",
       "    0.23040164712667466,\n",
       "    0.2255926067829132,\n",
       "    0.2403895515561104,\n",
       "    0.2273740167617798,\n",
       "    0.23702668489813805,\n",
       "    0.23246423612833023,\n",
       "    0.22226416083574296,\n",
       "    0.3497328232228756,\n",
       "    0.19938192981481553,\n",
       "    0.24900724618434905,\n",
       "    0.2731777068912983,\n",
       "    0.19862347947359085,\n",
       "    0.1883162725687027,\n",
       "    0.2230018889605999,\n",
       "    0.19271220880150794,\n",
       "    0.2076555464863777,\n",
       "    0.20320232090950013,\n",
       "    0.216143581828475,\n",
       "    0.16692207483649255,\n",
       "    0.229095681643486,\n",
       "    0.151613486880064,\n",
       "    0.20963833580613137,\n",
       "    0.14058562388420104,\n",
       "    0.22914663844108582,\n",
       "    0.24150931482315063,\n",
       "    0.17242481821775438,\n",
       "    0.13801209561824798]],\n",
       "  'train_loss': [[0.022798414951183378,\n",
       "    0.01359925831987241,\n",
       "    0.007990645528477218,\n",
       "    0.008970081785549679,\n",
       "    0.009390328726420209,\n",
       "    0.0053349674796104914,\n",
       "    0.005036465027091572,\n",
       "    0.003445506334839058,\n",
       "    0.005796503940306138,\n",
       "    0.005910515066812402,\n",
       "    0.007102865861705375,\n",
       "    0.005846981493821808,\n",
       "    0.003654190468945024,\n",
       "    0.00399083417097142,\n",
       "    0.0051794359845155616,\n",
       "    0.0027341286890908436,\n",
       "    0.0035010550085427515,\n",
       "    0.0038738360126851024,\n",
       "    0.0050598036673461634,\n",
       "    0.0025940250585101856,\n",
       "    0.002981116854785516,\n",
       "    0.0033760050978272034,\n",
       "    0.0021279804570652015,\n",
       "    0.0020285061757371907,\n",
       "    0.002470307630613085,\n",
       "    0.0034124682399118166,\n",
       "    0.002533185390663235,\n",
       "    0.0032969114108347118,\n",
       "    0.0030282099243051372,\n",
       "    0.003284765437390471,\n",
       "    0.0037047620539909626,\n",
       "    0.003465244245070019,\n",
       "    0.003699404978465517,\n",
       "    0.003362866502835683,\n",
       "    0.0020948355055106635,\n",
       "    0.003796742749505889,\n",
       "    0.004245046162331513,\n",
       "    0.003050581093667049,\n",
       "    0.002973928718396548,\n",
       "    0.003157229011932776,\n",
       "    0.0032536517054900715,\n",
       "    0.0027215863628660573,\n",
       "    0.0016952177813542048,\n",
       "    0.0019085331906816834,\n",
       "    0.0022532077096027747,\n",
       "    0.001879406968339591,\n",
       "    0.0020553342560758773,\n",
       "    0.0026728406419516096,\n",
       "    0.0032005207710744275,\n",
       "    0.001467976099645421],\n",
       "   [0.026553182363359146,\n",
       "    0.009943400296497672,\n",
       "    0.00866890078224097,\n",
       "    0.007384181098092205,\n",
       "    0.005317223895631458,\n",
       "    0.008513224508849792,\n",
       "    0.011815633159997679,\n",
       "    0.005253226066185742,\n",
       "    0.006286930893050237,\n",
       "    0.0068113078978347215,\n",
       "    0.005955664251621201,\n",
       "    0.003973785683982811,\n",
       "    0.006385945954893295,\n",
       "    0.004539190453429471,\n",
       "    0.0030472922889107886,\n",
       "    0.002923614091891575,\n",
       "    0.005173773007774547,\n",
       "    0.004539184163064644,\n",
       "    0.0029960501131690784,\n",
       "    0.0024450166175561906,\n",
       "    0.003039423677773157,\n",
       "    0.0026004292432274343,\n",
       "    0.00290356360428294,\n",
       "    0.0028207039223084925,\n",
       "    0.004669763724435963,\n",
       "    0.001739190609372572,\n",
       "    0.002677113932507056,\n",
       "    0.00484316202561915,\n",
       "    0.0021342860704621756,\n",
       "    0.0032684665871529767,\n",
       "    0.003952725594719432,\n",
       "    0.0022906898353563425,\n",
       "    0.0020707360216439814,\n",
       "    0.0020546900856866354,\n",
       "    0.0025297686153838072,\n",
       "    0.004674598318420361,\n",
       "    0.0031700114660791453,\n",
       "    0.002696169028796373,\n",
       "    0.0016787800023151205,\n",
       "    0.00100929195173706,\n",
       "    0.0034357097270415033,\n",
       "    0.0013704329623179107,\n",
       "    0.0029892436148000086,\n",
       "    0.0019926034885386686,\n",
       "    0.0012005469268491485,\n",
       "    0.0026716854842589476,\n",
       "    0.0026800038097791563,\n",
       "    0.0011658009410921227,\n",
       "    0.002743349403130978,\n",
       "    0.0023504200615942334],\n",
       "   [0.022102680695013664,\n",
       "    0.012886353256507383,\n",
       "    0.008648990481024885,\n",
       "    0.007838307757318393,\n",
       "    0.006096382205685822,\n",
       "    0.007260271244678121,\n",
       "    0.006288628711052463,\n",
       "    0.003715996470190175,\n",
       "    0.007272905276683958,\n",
       "    0.006710506429079959,\n",
       "    0.0036411216597662536,\n",
       "    0.004952484435526313,\n",
       "    0.005611831189102853,\n",
       "    0.004067393099610058,\n",
       "    0.0031590556514272824,\n",
       "    0.0050933075329561844,\n",
       "    0.0028745310285125526,\n",
       "    0.004580810546448965,\n",
       "    0.0019003979274477413,\n",
       "    0.005259213749785703,\n",
       "    0.0035849263278416553,\n",
       "    0.002110589736992094,\n",
       "    0.002255729974786641,\n",
       "    0.004807922778647176,\n",
       "    0.002792703699551106,\n",
       "    0.002617312173543804,\n",
       "    0.0019802760928714206,\n",
       "    0.003606211032439829,\n",
       "    0.002065210325098527,\n",
       "    0.002089369048331335,\n",
       "    0.0025562178900950263,\n",
       "    0.002791486767227356,\n",
       "    0.003608288222677682,\n",
       "    0.002453126767089032,\n",
       "    0.0036783700560007473,\n",
       "    0.0022726152924916953,\n",
       "    0.0017441167098088167,\n",
       "    0.0027505601496637068,\n",
       "    0.004111790840681898,\n",
       "    0.002093331107020988,\n",
       "    0.0022776831254955146,\n",
       "    0.0017646622070449148,\n",
       "    0.002199576330355504,\n",
       "    0.003243253318085134,\n",
       "    0.0027948436613531935,\n",
       "    0.0022399293904104126,\n",
       "    0.0014845987883344782,\n",
       "    0.0022983388739332973,\n",
       "    0.002089438812884519,\n",
       "    0.0033652466155451727],\n",
       "   [0.012621463208823952,\n",
       "    0.005887349915123618,\n",
       "    0.010919035960119772,\n",
       "    0.0075775338149027885,\n",
       "    0.0065197009194160604,\n",
       "    0.0075722493839559864,\n",
       "    0.005549872119170617,\n",
       "    0.007180621275837036,\n",
       "    0.005717257345862295,\n",
       "    0.005957876050058022,\n",
       "    0.004548639278358533,\n",
       "    0.006458862092657673,\n",
       "    0.004613946454248322,\n",
       "    0.0047456982447228275,\n",
       "    0.006169918339054755,\n",
       "    0.004253581030311532,\n",
       "    0.003381058418015514,\n",
       "    0.004451772443305002,\n",
       "    0.00431821298294352,\n",
       "    0.0027343435110759986,\n",
       "    0.004500317315454856,\n",
       "    0.00441179522558669,\n",
       "    0.0028584930920030516,\n",
       "    0.0026938713892935436,\n",
       "    0.003176211525238225,\n",
       "    0.0021377010462264073,\n",
       "    0.0025429933519279723,\n",
       "    0.0019209840584092906,\n",
       "    0.002536972444689318,\n",
       "    0.0026994342399511108,\n",
       "    0.0025010268051420164,\n",
       "    0.002842142939121337,\n",
       "    0.001425885373008708,\n",
       "    0.0014205734704206257,\n",
       "    0.002921450975256139,\n",
       "    0.00213440194846761,\n",
       "    0.0027751899965084655,\n",
       "    0.0034508805995948433,\n",
       "    0.0018666972868128108,\n",
       "    0.0010779709619299466,\n",
       "    0.0021875167566477263,\n",
       "    0.002313688944031881,\n",
       "    0.0012098997273815267,\n",
       "    0.004505170401863127,\n",
       "    0.0015439948325357959,\n",
       "    0.0029712425105419106,\n",
       "    0.0017341555732303753,\n",
       "    0.0018689373326077452,\n",
       "    0.0020977815292546166,\n",
       "    0.0020684703526490564],\n",
       "   [0.025451131951982963,\n",
       "    0.012125370248863638,\n",
       "    0.008401503315885829,\n",
       "    0.010987821746730932,\n",
       "    0.005481215011410355,\n",
       "    0.005965461728862783,\n",
       "    0.006233727803718501,\n",
       "    0.0027633527589458738,\n",
       "    0.006458563608598806,\n",
       "    0.003714238484716967,\n",
       "    0.007785437934163907,\n",
       "    0.004372773366650591,\n",
       "    0.003481483981015827,\n",
       "    0.004265325453467687,\n",
       "    0.006091707297557285,\n",
       "    0.004773895230688602,\n",
       "    0.0021705950599436763,\n",
       "    0.002669494075168668,\n",
       "    0.004600608600297793,\n",
       "    0.0036986641777586107,\n",
       "    0.00381171157586776,\n",
       "    0.0022727941289234677,\n",
       "    0.0026162132866780594,\n",
       "    0.00346699951282342,\n",
       "    0.0033102253597544055,\n",
       "    0.004474581683422731,\n",
       "    0.002279419207756625,\n",
       "    0.0017242188036675633,\n",
       "    0.0026890137395989325,\n",
       "    0.0029163399841460546,\n",
       "    0.003908220921394484,\n",
       "    0.0029620478998803866,\n",
       "    0.0023602991714213014,\n",
       "    0.0026438845003074075,\n",
       "    0.002150145632367794,\n",
       "    0.003002294934859183,\n",
       "    0.0018628013632644553,\n",
       "    0.003071281802168185,\n",
       "    0.001846199982366667,\n",
       "    0.002806139655063157,\n",
       "    0.0025925175569210725,\n",
       "    0.0009882133755748017,\n",
       "    0.0009698657810932515,\n",
       "    0.0028786737986641483,\n",
       "    0.0016703663880604867,\n",
       "    0.0018168979303694575,\n",
       "    0.002105733583231235,\n",
       "    0.002242496936077086,\n",
       "    0.0031353400851846652,\n",
       "    0.0026809176109228883]]}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "with open(path + 'Local_Round_FedAvg_25.pkl', 'rb') as file:\n",
    "  log_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "se_04JMaVJPg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99.62025316455696, 99.62025316455696, 99.74683544303798, 99.49367088607595, 99.36708860759494]\n"
     ]
    }
   ],
   "source": [
    "print([test_acc[-1] for test_acc in log_dict['MNIST CNN on IID']['test_accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "5sv0rv-G4Sqa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[98.86075949367088, 98.60759493670886, 99.24050632911393, 99.11392405063292, 98.10126582278481]\n"
     ]
    }
   ],
   "source": [
    "print([test_acc[-1] for test_acc in log_dict['MNIST CNN on Non IID']['test_accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "b2a2_Az-4nal"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97.9746835443038, 98.10126582278481, 97.72151898734177, 97.9746835443038, 98.10126582278481]\n"
     ]
    }
   ],
   "source": [
    "print([test_acc[-1] for test_acc in log_dict['MNIST MLP on IID']['test_accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "YoRqZyvD4p2K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94.43037974683544, 91.26582278481013, 88.9873417721519, 91.77215189873418, 95.31645569620254]\n"
     ]
    }
   ],
   "source": [
    "print([test_acc[-1] for test_acc in log_dict['MNIST MLP on Non IID']['test_accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B87eGKJnAHIh"
   },
   "outputs": [],
   "source": [
    "output.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "FederatedAveraging",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "QSGD",
   "language": "python",
   "name": "qsgd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
