{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tushar-semwal/fedperf/blob/main/Vineeth/Local_Rounds/FedAvg/FederatedAveraging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yKcpjZLrQQJV",
    "outputId": "5fca5a43-8803-4d05-99e7-310c60b2eb17"
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    import os\n",
    "    path = '/content/drive/MyDrive/Colab Notebooks/OpenMined/'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "except:\n",
    "    path = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0_nKpfq2h1R"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLLNM9X2JbQ8",
    "outputId": "c88c97b3-b806-4e29-f4cd-02a6743f4f13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 12 23:24:00 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 17%   32C    P0    54W / 250W |      0MiB / 11178MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:82:00.0 Off |                  N/A |\n",
      "| 17%   32C    P0    56W / 250W |      0MiB / 11178MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "# Check assigned GPU\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "\n",
    "# set manual seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# general reproducibility\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# gpu training specific\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY4eWzGiL6Mj"
   },
   "source": [
    "## Load the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "G649tjTXLL8F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to ../data/mnist/MNIST/raw\n",
      "Using downloaded and verified file: ../data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/mnist/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 503: Service Unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7295953b39d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmnist_data_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/mnist/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms_mnist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmnist_data_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/mnist/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms_mnist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresources\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# process and save as torch files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 )\n\u001b[1;32m     83\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;31m# check integrity of downloaded file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5)\u001b[0m\n\u001b[1;32m     70\u001b[0m             urllib.request.urlretrieve(\n\u001b[1;32m     71\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_bar_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             )\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/envs/QSGD-PT/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 503: Service Unavailable"
     ]
    }
   ],
   "source": [
    "# create transforms\n",
    "# We will just convert to tensor and normalize since no special transforms are mentioned in the paper\n",
    "transforms_mnist = transforms.Compose([\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                       ])\n",
    "\n",
    "mnist_data_train = datasets.MNIST('./data/mnist/', train=True, download=True, transform=transforms_mnist)\n",
    "mnist_data_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=transforms_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dm9usjn2vFkL",
    "outputId": "a76539d2-3f37-4485-e5ac-7633e785c18c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist_data_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c3677d73e171>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_data_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclasses_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist_data_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_to_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Classes: {} \\tType: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Classes Test: {} \\tType: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mnist_data_test' is not defined"
     ]
    }
   ],
   "source": [
    "classes = np.array(list(mnist_data_train.class_to_idx.values()))\n",
    "classes_test = np.array(list(mnist_data_test.class_to_idx.values()))\n",
    "num_classes = len(classes_test)\n",
    "print(\"Classes: {} \\tType: {}\".format(classes, type(classes)))\n",
    "print(\"Classes Test: {} \\tType: {}\".format(classes_test, type(classes)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9lvJt3Ofv2SO",
    "outputId": "d3626726-b3da-426f-9712-7a7be680d33b"
   },
   "outputs": [],
   "source": [
    "print(\"Image Shape: {}\".format(mnist_data_train.data[0].size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCOONkg-zV7Y"
   },
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9MK03TZw6Qs"
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "\timg = img/2 + 0.5 #unnormalize the image\n",
    "\tplt.imshow(img, cmap='gray') # convert from tensor to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMJ0Kx4Kw-_H"
   },
   "outputs": [],
   "source": [
    "def visualize(dataset):\n",
    "  figure = plt.figure(figsize=(25,4))\n",
    "  for i in range(20):\n",
    "    axis = figure.add_subplot(2, 20/2, i+1, xticks=[], yticks=[])\n",
    "    data = dataset.data[i]\n",
    "    data = data.numpy()\n",
    "\n",
    "    target = dataset.targets[i]\n",
    "    target = target.numpy()\n",
    "    imshow(data)\n",
    "    axis.set_title(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "8bPOwKg10Ro7",
    "outputId": "52916fa2-c981-44e5-92a7-43886e91b271"
   },
   "outputs": [],
   "source": [
    "visualize(mnist_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "RKoh5Cf70UYu",
    "outputId": "91e5ef2a-5e61-454b-a4f3-c64ff266071a"
   },
   "outputs": [],
   "source": [
    "visualize(mnist_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctjRsETiO1qO"
   },
   "source": [
    "## Partitioning the Data (IID and non-IID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_v8lyrgO5dD"
   },
   "outputs": [],
   "source": [
    "def iid_partition(dataset, clients):\n",
    "  \"\"\"\n",
    "  I.I.D paritioning of data over clients\n",
    "  Shuffle the data\n",
    "  Split it between clients\n",
    "  \n",
    "  params:\n",
    "    - dataset (torch.utils.Dataset): Dataset containing the MNIST Images\n",
    "    - clients (int): Number of Clients to split the data between\n",
    "\n",
    "  returns:\n",
    "    - Dictionary of image indexes for each client\n",
    "  \"\"\"\n",
    "\n",
    "  num_items_per_client = int(len(dataset)/clients)\n",
    "  client_dict = {}\n",
    "  image_idxs = [i for i in range(len(dataset))]\n",
    "\n",
    "  for i in range(clients):\n",
    "    client_dict[i] = set(np.random.choice(image_idxs, num_items_per_client, replace=False))\n",
    "    image_idxs = list(set(image_idxs) - client_dict[i])\n",
    "\n",
    "  return client_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zMdliGMQoSl"
   },
   "outputs": [],
   "source": [
    "def non_iid_partition(dataset, clients, total_shards, shards_size, num_shards_per_client):\n",
    "  \"\"\"\n",
    "  non I.I.D parititioning of data over clients\n",
    "  Sort the data by the digit label\n",
    "  Divide the data into N shards of size S\n",
    "  Each of the clients will get X shards\n",
    "\n",
    "  params:\n",
    "    - dataset (torch.utils.Dataset): Dataset containing the MNIST Images\n",
    "    - clients (int): Number of Clients to split the data between\n",
    "    - total_shards (int): Number of shards to partition the data in\n",
    "    - shards_size (int): Size of each shard \n",
    "    - num_shards_per_client (int): Number of shards of size shards_size that each client receives\n",
    "\n",
    "  returns:\n",
    "    - Dictionary of image indexes for each client\n",
    "  \"\"\"\n",
    "  \n",
    "  shard_idxs = [i for i in range(total_shards)]\n",
    "  client_dict = {i: np.array([], dtype='int64') for i in range(clients)}\n",
    "  idxs = np.arange(len(dataset))\n",
    "  data_labels = dataset.targets.numpy()\n",
    "\n",
    "  # sort the labels\n",
    "  label_idxs = np.vstack((idxs, data_labels))\n",
    "  label_idxs = label_idxs[:, label_idxs[1,:].argsort()]\n",
    "  idxs = label_idxs[0,:]\n",
    "\n",
    "  # divide the data into total_shards of size shards_size\n",
    "  # assign num_shards_per_client to each client\n",
    "  for i in range(clients):\n",
    "    rand_set = set(np.random.choice(shard_idxs, num_shards_per_client, replace=False))\n",
    "    shard_idxs = list(set(shard_idxs) - rand_set)\n",
    "\n",
    "    for rand in rand_set:\n",
    "      client_dict[i] = np.concatenate((client_dict[i], idxs[rand*shards_size:(rand+1)*shards_size]), axis=0)\n",
    "  \n",
    "  return client_dict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTfxv8kFoGAy"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CvoDNFKbZST5"
   },
   "outputs": [],
   "source": [
    "class MNIST_2NN(nn.Module):\n",
    "  \"\"\"\n",
    "  A simple multilayer-perceptron with 2-hidden layers with 200 units each\n",
    "  using ReLu activations\n",
    "\n",
    "  Total Expected Params: 199,210\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(MNIST_2NN, self).__init__()\n",
    "\n",
    "    self.fc1 = nn.Linear(28*28, 200)\n",
    "    self.fc2 = nn.Linear(200, 200)\n",
    "    self.fc3 = nn.Linear(200, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    out = self.fc3(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ut1hZ8x3qYPZ"
   },
   "outputs": [],
   "source": [
    "class MNIST_CNN(nn.Module):\n",
    "  \"\"\"\n",
    "  CNN with two 5x5 convolution lauers(the first with 32 channels, second with 64,\n",
    "  each followed with 2x2 max pooling), a fully connected layer with 512 uunits and \n",
    "  ReLu activation, and the final Softmax output layer\n",
    "\n",
    "  Total Expected Params: 1,663,370\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(MNIST_CNN, self).__init__()\n",
    "\n",
    "    self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "    \n",
    "    self.pool = nn.MaxPool2d(2,2)\n",
    "    self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    self.fc1 = nn.Linear(1024, 512)\n",
    "    self.out = nn.Linear(512, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    x = self.dropout(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.out(x)\n",
    "    out = F.log_softmax(x, dim=1)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVv4HA9HuLtr"
   },
   "source": [
    "### Print Model Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5oTH710sJEt",
    "outputId": "3e8f2eff-a1be-45a7-a297-5963b3b4e48d"
   },
   "outputs": [],
   "source": [
    "mnist_mlp = MNIST_2NN()\n",
    "mnist_cnn = MNIST_CNN()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  mnist_mlp.cuda()\n",
    "  mnist_cnn.cuda()\n",
    "\n",
    "print(\"MNIST MLP SUMMARY\")\n",
    "print(summary(mnist_mlp, (28,28)))\n",
    "\n",
    "print(\"\\nMNIST CNN SUMMARY\")\n",
    "print(summary(mnist_cnn, (1, 28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gf_8XEXa-gZ7"
   },
   "source": [
    "## Federated Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-aNdQsQ-Kvp"
   },
   "source": [
    "### Local Training (Client Update)\n",
    "\n",
    "Local training for the model on client side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oX6OsQyO-Gz7"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, dataset, idxs):\n",
    "      self.dataset = dataset\n",
    "      self.idxs = list(idxs)\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.idxs)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "      image, label = self.dataset[self.idxs[item]]\n",
    "      return image, label\n",
    "\n",
    "\n",
    "class ClientUpdate(object):\n",
    "  def __init__(self, dataset, batchSize, learning_rate, epochs, idxs):\n",
    "    self.train_loader = DataLoader(CustomDataset(dataset, idxs), batch_size=batchSize, shuffle=True)\n",
    "\n",
    "    self.learning_rate = learning_rate\n",
    "    self.epochs = epochs\n",
    "\n",
    "  def train(self, model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.5)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    e_loss = []\n",
    "    for epoch in range(1, self.epochs+1):\n",
    "\n",
    "      train_loss = 0.0\n",
    "      model.train()\n",
    "      for data, labels in self.train_loader:\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "          data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "        # clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # make a forward pass\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, labels)\n",
    "        # do a backwards pass\n",
    "        loss.backward()\n",
    "        # perform a single optimization step\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "\n",
    "      # average losses\n",
    "      train_loss = train_loss/len(self.train_loader.dataset)\n",
    "      e_loss.append(train_loss)\n",
    "\n",
    "    total_loss = sum(e_loss)/len(e_loss)\n",
    "\n",
    "    return model.state_dict(), total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ukgh1DVHE2Ds"
   },
   "source": [
    "### Server Side Training\n",
    "\n",
    "Following Algorithm 1 from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NF1e33BgpeL"
   },
   "outputs": [],
   "source": [
    "def training(model, rounds, batch_size, lr, ds, ds_test, data_dict, C, K, E, plt_title, plt_color):\n",
    "  \"\"\"\n",
    "  Function implements the Federated Averaging Algorithm from the FedAvg paper.\n",
    "  Specifically, this function is used for the server side training and weight update\n",
    "\n",
    "  Params:\n",
    "    - model:           PyTorch model to train\n",
    "    - rounds:          Number of communication rounds for the client update\n",
    "    - batch_size:      Batch size for client update training\n",
    "    - lr:              Learning rate used for client update training\n",
    "    - ds:              Dataset used for training\n",
    "    - ds_test:         Dataset used for testing\n",
    "    - data_dict:       Type of data partition used for training (IID or non-IID)\n",
    "    - C:               Fraction of clients randomly chosen to perform computation on each round\n",
    "    - K:               Total number of clients\n",
    "    - E:               Number of training passes each client makes over its local dataset per round\n",
    "    - tb_writer_name:  Directory name to save the tensorboard logs\n",
    "  Returns:\n",
    "    - model:           Trained model on the server\n",
    "  \"\"\"\n",
    "\n",
    "  # global model weights\n",
    "  global_weights = model.state_dict()\n",
    "\n",
    "  # training loss\n",
    "  # train_accuracy = []\n",
    "  train_loss = []\n",
    "  test_accuracy = []\n",
    "  test_loss = []\n",
    "\n",
    "\n",
    "  # measure time\n",
    "  start = time.time()\n",
    "\n",
    "  for curr_round in range(1, rounds+1):\n",
    "    w, local_loss = [], []\n",
    "\n",
    "    m = max(int(C*K), 1)\n",
    "    \n",
    "    S_t = np.random.choice(range(K), m, replace=False)\n",
    "    for k in S_t:\n",
    "      local_update = ClientUpdate(dataset=ds, batchSize=batch_size, learning_rate=lr, epochs=E, idxs=data_dict[k])\n",
    "      weights, loss = local_update.train(model=copy.deepcopy(model))\n",
    "\n",
    "      w.append(copy.deepcopy(weights))\n",
    "      local_loss.append(copy.deepcopy(loss))\n",
    "\n",
    "    # updating the global weights\n",
    "    weights_avg = copy.deepcopy(w[0])\n",
    "    for k in weights_avg.keys():\n",
    "      for i in range(1, len(w)):\n",
    "        weights_avg[k] += w[i][k]\n",
    "\n",
    "      weights_avg[k] = torch.div(weights_avg[k], len(w))\n",
    "\n",
    "    global_weights = weights_avg\n",
    "\n",
    "    # move the updated weights to our model state dict\n",
    "    model.load_state_dict(global_weights)\n",
    "\n",
    "    # loss\n",
    "    loss_avg = sum(local_loss) / len(local_loss)\n",
    "    print('Round: {}... \\tAverage Loss: {}'.format(curr_round, round(loss_avg, 3)))\n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "    # test\n",
    "    test_criterion = nn.CrossEntropyLoss()\n",
    "    test_accuracy_current, test_loss_current =  testing(copy.deepcopy(model), ds_test, 128, test_criterion, num_classes, classes_test)\n",
    "    test_accuracy.append(test_accuracy_current)\n",
    "    test_loss.append(test_loss_current)\n",
    "\n",
    "  end = time.time()\n",
    "  \n",
    "  fig, ax = plt.subplots()\n",
    "  x_axis = np.arange(1, rounds+1)\n",
    "  y_axis = np.array(train_loss)\n",
    "  ax.plot(x_axis, y_axis, 'tab:'+plt_color)\n",
    "\n",
    "  ax.set(xlabel='Number of Rounds', ylabel='Train Loss',\n",
    "       title=plt_title)\n",
    "  ax.grid()\n",
    "  fig.savefig(plt_title+'_Train_loss.jpg', format='jpg')\n",
    "  plt.show()\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  x_axis = np.arange(1, rounds+1)\n",
    "  y_axis = np.array(test_loss)\n",
    "  ax.plot(x_axis, y_axis, 'tab:'+plt_color)\n",
    "\n",
    "  ax.set(xlabel='Number of Rounds', ylabel='Test Loss',\n",
    "       title=plt_title)\n",
    "  ax.grid()\n",
    "  fig.savefig(plt_title+'_Test_loss.jpg', format='jpg')\n",
    "  plt.show()\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  x_axis = np.arange(1, rounds+1)\n",
    "  y_axis = np.array(test_accuracy)\n",
    "  ax.plot(x_axis, y_axis, 'tab:'+plt_color)\n",
    "\n",
    "  ax.set(xlabel='Number of Rounds', ylabel='Test Accuracy',\n",
    "       title=plt_title)\n",
    "  ax.grid()\n",
    "  fig.savefig(plt_title+'_Test_Accuracy.jpg', format='jpg')\n",
    "  plt.show()\n",
    "  \n",
    "  print(\"Training Done!\")\n",
    "  print(\"Total time taken to Train: {}\\n\\n\".format(end-start))\n",
    "  \n",
    "  return model, train_loss, test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUYyb4T-uXmF"
   },
   "source": [
    "## Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCcIZmO5uan9"
   },
   "outputs": [],
   "source": [
    "def testing(model, dataset, bs, criterion, num_classes, classes, print_info=False):\n",
    "  #test loss \n",
    "  test_loss = 0.0\n",
    "  correct_class = list(0. for i in range(num_classes))\n",
    "  total_class = list(0. for i in range(num_classes))\n",
    "\n",
    "  test_loader = DataLoader(dataset, batch_size=bs)\n",
    "  l = len(test_loader)\n",
    "  model.eval()\n",
    "  for data, labels in test_loader:\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "      data, labels = data.cuda(), labels.cuda()\n",
    "\n",
    "    output = model(data)\n",
    "    loss = criterion(output, labels)\n",
    "    test_loss += loss.item()*data.size(0)\n",
    "\n",
    "    _, pred = torch.max(output, 1)\n",
    "\n",
    "    correct_tensor = pred.eq(labels.data.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
    "\n",
    "    #test accuracy for each object class\n",
    "    for i in range(num_classes):\n",
    "      label = labels.data[i]\n",
    "      correct_class[label] += correct[i].item()\n",
    "      total_class[label] += 1\n",
    "    \n",
    "  # avg test loss\n",
    "  test_loss = test_loss/len(test_loader.dataset)\n",
    "  test_accuracy = 100. * np.sum(correct_class) / np.sum(total_class)\n",
    "\n",
    "  if print_info:\n",
    "    print(\"Test Loss: {:.6f}\\n\".format(test_loss))\n",
    "\n",
    "    for i in range(10):\n",
    "      if total_class[i]>0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % \n",
    "              (classes[i], 100 * correct_class[i] / total_class[i],\n",
    "              np.sum(correct_class[i]), np.sum(total_class[i])))\n",
    "      else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "    print('\\nFinal Test  Accuracy: {:.3f} ({}/{})'.format(\n",
    "          100. * np.sum(correct_class) / np.sum(total_class),\n",
    "          np.sum(correct_class), np.sum(total_class)))\n",
    "  \n",
    "  return test_accuracy, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri0FqXFeHW-V"
   },
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thZm2kSiHT4v"
   },
   "outputs": [],
   "source": [
    "log_dict = {}\n",
    "NUM_REPEAT = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hO5oV6aXqeh"
   },
   "source": [
    "## MNIST CNN on IID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flSQv_P4zCfx"
   },
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0ZalcKZtEseA",
    "outputId": "b28f6fdb-c1ce-4c59-aa58-0e383d408230"
   },
   "outputs": [],
   "source": [
    "train_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
    "test_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "\n",
    "for exp_num in range(NUM_REPEAT):\n",
    "  print(\"Experiment Run Number: \", exp_num)\n",
    "\n",
    "  # number of training rounds\n",
    "  rounds = 50\n",
    "  # client fraction\n",
    "  C = 0.1\n",
    "  # number of clients\n",
    "  K = 100\n",
    "  # number of training passes on local dataset for each roung\n",
    "  E = 10\n",
    "  # batch size\n",
    "  batch_size = 10\n",
    "  # learning Rate\n",
    "  lr=0.05\n",
    "  # data partition dictionary\n",
    "  iid_dict = iid_partition(mnist_data_train, 100)\n",
    "  # load model\n",
    "  mnist_cnn = MNIST_CNN()\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    mnist_cnn.cuda()\n",
    "\n",
    "  mnist_cnn_iid_trained, train_loss, test_accuracy, test_loss = training(mnist_cnn, rounds, batch_size, lr, mnist_data_train, mnist_data_test, iid_dict, C, K, E, \"MNIST CNN on IID Dataset\", \"orange\")\n",
    "\n",
    "  train_loss_multiple_runs[exp_num] = train_loss\n",
    "  test_accuracy_multiple_runs[exp_num] = test_accuracy\n",
    "  test_loss_multiple_runs[exp_num] = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNwC82przF6G"
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qB97BFs9we9w",
    "outputId": "42f4c586-7c3c-46f8-d9c5-7025c1ab4e24"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "acc, loss = testing(mnist_cnn_iid_trained, mnist_data_test, 128, criterion, num_classes, classes_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdQZEZmHHeqt"
   },
   "outputs": [],
   "source": [
    "hyperparams = {'rounds': rounds,\n",
    "               'C': C,\n",
    "               'K': K,\n",
    "               'E': E,\n",
    "               'batch_size': batch_size,\n",
    "               'lr': lr,\n",
    "               }\n",
    "\n",
    "log_dict['MNIST CNN on IID'] = {'train_loss': train_loss_multiple_runs, \n",
    "                                'test_loss': test_loss_multiple_runs, \n",
    "                                'test_accuracy': test_accuracy_multiple_runs,\n",
    "                                'hyperparams': hyperparams,\n",
    "                                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uF8MdSIUYcnl"
   },
   "source": [
    "## MNIST CNN on Non IID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6wXX7JW11bx"
   },
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fCD3kBCKYfBK",
    "outputId": "f47df54f-43d5-4d0a-b89b-e90161c5f516"
   },
   "outputs": [],
   "source": [
    "train_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
    "test_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "\n",
    "for exp_num in range(NUM_REPEAT):\n",
    "  print(\"Experiment Run Number: \", exp_num)\n",
    "\n",
    "  # number of training rounds\n",
    "  rounds = 50\n",
    "  # client fraction\n",
    "  C = 0.1\n",
    "  # number of clients\n",
    "  K = 100\n",
    "  # number of training passes on local dataset for each roung\n",
    "  E = 10\n",
    "  # batch size\n",
    "  batch_size = 10\n",
    "  # learning Rate\n",
    "  lr=0.05\n",
    "  # dict containing different type of data partition\n",
    "  data_dict = non_iid_partition(mnist_data_train, 100, 200, 300, 2)\n",
    "  # load model\n",
    "  mnist_cnn = MNIST_CNN()\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    mnist_cnn.cuda()\n",
    "\n",
    "  mnist_cnn_non_iid_trained, train_loss, test_accuracy, test_loss = training(mnist_cnn, rounds, batch_size, lr, mnist_data_train, mnist_data_test, data_dict, C, K, E, \"MNIST CNN on Non-IID Dataset\", \"green\")\n",
    "\n",
    "  train_loss_multiple_runs[exp_num] = train_loss\n",
    "  test_accuracy_multiple_runs[exp_num] = test_accuracy\n",
    "  test_loss_multiple_runs[exp_num] = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4C68J-Kk14dB"
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_yQ9GiAZ15jE",
    "outputId": "2793a0a1-6969-4670-dd04-53dc6b191d44"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "acc, loss = testing(mnist_cnn_non_iid_trained, mnist_data_test, 128, criterion, num_classes, classes_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxMcxgLhLvX-"
   },
   "outputs": [],
   "source": [
    "hyperparams = {'rounds': rounds,\n",
    "               'C': C,\n",
    "               'K': K,\n",
    "               'E': E,\n",
    "               'batch_size': batch_size,\n",
    "               'lr': lr,\n",
    "               }\n",
    "\n",
    "log_dict['MNIST CNN on Non IID'] = {'train_loss': train_loss_multiple_runs, \n",
    "                                'test_loss': test_loss_multiple_runs, \n",
    "                                'test_accuracy': test_accuracy_multiple_runs,\n",
    "                                'hyperparams': hyperparams,\n",
    "                                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_jifdzniuhm"
   },
   "source": [
    "## MNIST MLP on IID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uh-te0Od2XGO"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UmpWmKOQixVV",
    "outputId": "a487fb22-a217-4dfc-f4d6-63c4df004b41"
   },
   "outputs": [],
   "source": [
    "train_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
    "test_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "\n",
    "for exp_num in range(NUM_REPEAT):\n",
    "  print(\"Experiment Run Number: \", exp_num)\n",
    "\n",
    "  # number of training rounds\n",
    "  rounds = 50\n",
    "  # client fraction\n",
    "  C = 0.1\n",
    "  # number of clients\n",
    "  K = 100\n",
    "  # number of training passes on local dataset for each round\n",
    "  E = 10\n",
    "  # batch size\n",
    "  batch_size = 10\n",
    "  # learning Rate\n",
    "  lr=0.05\n",
    "  # dict containing different type of data partition\n",
    "  data_dict = iid_partition(mnist_data_train, 100)\n",
    "  # load model\n",
    "  mnist_mlp = MNIST_2NN()\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    mnist_mlp.cuda()\n",
    "\n",
    "  mnist_mlp_iid_trained, train_loss, test_accuracy, test_loss = training(mnist_mlp, rounds, batch_size, lr, mnist_data_train, mnist_data_test, data_dict, C, K, E, \"MNIST MLP on IID Dataset\", \"orange\")\n",
    "  \n",
    "  train_loss_multiple_runs[exp_num] = train_loss\n",
    "  test_accuracy_multiple_runs[exp_num] = test_accuracy\n",
    "  test_loss_multiple_runs[exp_num] = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTBsL3-72PPd"
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9G4j5L62OrS",
    "outputId": "e74613f0-7e49-4865-b430-223d32519658"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "acc, loss = testing(mnist_mlp_iid_trained, mnist_data_test, 128, criterion, num_classes, classes_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWCdJFRCL_f2"
   },
   "outputs": [],
   "source": [
    "hyperparams = {'rounds': rounds,\n",
    "               'C': C,\n",
    "               'K': K,\n",
    "               'E': E,\n",
    "               'batch_size': batch_size,\n",
    "               'lr': lr,\n",
    "               }\n",
    "\n",
    "log_dict['MNIST MLP on IID'] = {'train_loss': train_loss_multiple_runs, \n",
    "                                'test_loss': test_loss_multiple_runs, \n",
    "                                'test_accuracy': test_accuracy_multiple_runs,\n",
    "                                'hyperparams': hyperparams,\n",
    "                                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8jzEyO0iywz"
   },
   "source": [
    "## MNIST MLP on Non IID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJFepr3y2bF-"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EBKO44Hgi1Uh",
    "outputId": "7d0db2bf-fa03-4916-cb89-c2055290bdbb"
   },
   "outputs": [],
   "source": [
    "train_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "test_accuracy_multiple_runs = [None] * NUM_REPEAT\n",
    "test_loss_multiple_runs = [None] * NUM_REPEAT\n",
    "\n",
    "for exp_num in range(NUM_REPEAT):\n",
    "  print(\"Experiment Run Number: \", exp_num)\n",
    "  \n",
    "  # number of training rounds\n",
    "  rounds = 50\n",
    "  # client fraction\n",
    "  C = 0.1\n",
    "  # number of clients\n",
    "  K = 100\n",
    "  # number of training passes on local dataset for each roung\n",
    "  E = 10\n",
    "  # batch size\n",
    "  batch_size = 10\n",
    "  # learning Rate\n",
    "  lr=0.05\n",
    "  # dict containing different type of data partition\n",
    "  data_dict = non_iid_partition(mnist_data_train, 100, 200, 300, 2)\n",
    "  # load model\n",
    "  mnist_mlp = MNIST_2NN()\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    mnist_mlp.cuda()\n",
    "\n",
    "  mnist_mlp_non_iid_trained, train_loss, test_accuracy, test_loss = training(mnist_mlp, rounds, batch_size, lr, mnist_data_train, mnist_data_test, data_dict, C, K, E, \"MNIST MLP on Non-IID Dataset\", \"green\")\n",
    "\n",
    "  train_loss_multiple_runs[exp_num] = train_loss\n",
    "  test_accuracy_multiple_runs[exp_num] = test_accuracy\n",
    "  test_loss_multiple_runs[exp_num] = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmuQYPbF2mes"
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0tKMlJyF2nGN",
    "outputId": "93f7b3e3-8165-4dc8-d40c-579225a8aa33"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "acc, loss = testing(mnist_mlp_non_iid_trained, mnist_data_test, 128, criterion, num_classes, classes_test, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1W5krYcSMQiu"
   },
   "outputs": [],
   "source": [
    "hyperparams = {'rounds': rounds,\n",
    "               'C': C,\n",
    "               'K': K,\n",
    "               'E': E,\n",
    "               'batch_size': batch_size,\n",
    "               'lr': lr,\n",
    "               }\n",
    "\n",
    "log_dict['MNIST MLP on Non IID'] = {'train_loss': train_loss_multiple_runs, \n",
    "                                'test_loss': test_loss_multiple_runs, \n",
    "                                'test_accuracy': test_accuracy_multiple_runs,\n",
    "                                'hyperparams': hyperparams,\n",
    "                                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emS_SaRAP6TZ"
   },
   "source": [
    "## Pickle Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "soAN38JoP0c1"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "with open(path + 'Local_Round_FedAvg_10.pkl', 'wb') as file:\n",
    "  pickle.dump(log_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UBy-JswSoCJ",
    "outputId": "13b2bc77-3443-45b3-ee10-9e2265548afa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MNIST CNN on IID': {'hyperparams': {'C': 0.1,\n",
       "   'E': 10,\n",
       "   'K': 100,\n",
       "   'batch_size': 10,\n",
       "   'lr': 0.05,\n",
       "   'rounds': 50},\n",
       "  'test_accuracy': [[95.82278481012658,\n",
       "    97.0886075949367,\n",
       "    97.72151898734177,\n",
       "    98.22784810126582,\n",
       "    98.22784810126582,\n",
       "    98.35443037974683,\n",
       "    99.11392405063292,\n",
       "    98.9873417721519,\n",
       "    99.11392405063292,\n",
       "    98.86075949367088,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696],\n",
       "   [95.82278481012658,\n",
       "    96.83544303797468,\n",
       "    97.34177215189874,\n",
       "    98.10126582278481,\n",
       "    98.48101265822785,\n",
       "    98.22784810126582,\n",
       "    98.48101265822785,\n",
       "    98.73417721518987,\n",
       "    98.73417721518987,\n",
       "    98.9873417721519,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.74683544303798,\n",
       "    99.36708860759494,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696],\n",
       "   [95.44303797468355,\n",
       "    97.21518987341773,\n",
       "    97.46835443037975,\n",
       "    98.10126582278481,\n",
       "    98.35443037974683,\n",
       "    98.35443037974683,\n",
       "    98.60759493670886,\n",
       "    98.86075949367088,\n",
       "    98.86075949367088,\n",
       "    98.9873417721519,\n",
       "    99.24050632911393,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.11392405063292,\n",
       "    99.24050632911393,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.62025316455696,\n",
       "    99.49367088607595,\n",
       "    99.62025316455696,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595],\n",
       "   [95.0632911392405,\n",
       "    96.70886075949367,\n",
       "    96.70886075949367,\n",
       "    97.72151898734177,\n",
       "    97.84810126582279,\n",
       "    98.35443037974683,\n",
       "    98.35443037974683,\n",
       "    98.60759493670886,\n",
       "    98.86075949367088,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.24050632911393,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.49367088607595,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.49367088607595,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494],\n",
       "   [96.32911392405063,\n",
       "    97.0886075949367,\n",
       "    97.84810126582279,\n",
       "    98.10126582278481,\n",
       "    98.35443037974683,\n",
       "    98.48101265822785,\n",
       "    98.86075949367088,\n",
       "    98.9873417721519,\n",
       "    98.9873417721519,\n",
       "    99.24050632911393,\n",
       "    98.9873417721519,\n",
       "    99.11392405063292,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    99.24050632911393,\n",
       "    99.11392405063292,\n",
       "    99.24050632911393,\n",
       "    98.9873417721519,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.11392405063292,\n",
       "    99.11392405063292,\n",
       "    99.36708860759494,\n",
       "    99.24050632911393,\n",
       "    99.62025316455696,\n",
       "    99.24050632911393,\n",
       "    99.36708860759494]],\n",
       "  'test_loss': [[0.1769070187807083,\n",
       "    0.09903227711592336,\n",
       "    0.07946322233737446,\n",
       "    0.0679114653597935,\n",
       "    0.06064332182582293,\n",
       "    0.055946064434446455,\n",
       "    0.05077990834624652,\n",
       "    0.04427789069645223,\n",
       "    0.04301057471334934,\n",
       "    0.04204429463154665,\n",
       "    0.04013470559676207,\n",
       "    0.036700842851454946,\n",
       "    0.03663840707320487,\n",
       "    0.03420260622876958,\n",
       "    0.03505825899468509,\n",
       "    0.03567547427408472,\n",
       "    0.03373693464267944,\n",
       "    0.034679060305242634,\n",
       "    0.03340388855037945,\n",
       "    0.03335776209940159,\n",
       "    0.03369852075695999,\n",
       "    0.03289110397836048,\n",
       "    0.03229339898114995,\n",
       "    0.031441204498139995,\n",
       "    0.031222003915799722,\n",
       "    0.030562434707799183,\n",
       "    0.028661499328520586,\n",
       "    0.028481904957814184,\n",
       "    0.028146627688708032,\n",
       "    0.030134675855952447,\n",
       "    0.02763151373870969,\n",
       "    0.026987039675464983,\n",
       "    0.029739553741710188,\n",
       "    0.028526439353577734,\n",
       "    0.027699505836383104,\n",
       "    0.0285968581590183,\n",
       "    0.028315841985138833,\n",
       "    0.027802646408660803,\n",
       "    0.028987049497846328,\n",
       "    0.02629339449610161,\n",
       "    0.028099452074923828,\n",
       "    0.026511846145302297,\n",
       "    0.02502864158257289,\n",
       "    0.0262985370257451,\n",
       "    0.026210907931560723,\n",
       "    0.02597544769562628,\n",
       "    0.02377900591370217,\n",
       "    0.024679565323171528,\n",
       "    0.025049374171201906,\n",
       "    0.02602678955242891],\n",
       "   [0.19523917877078056,\n",
       "    0.10551625347859227,\n",
       "    0.08674061290672398,\n",
       "    0.06953547085267783,\n",
       "    0.05957924941251112,\n",
       "    0.05251350055449293,\n",
       "    0.04733635469258152,\n",
       "    0.04538800903859665,\n",
       "    0.04269812132796178,\n",
       "    0.042989207550016,\n",
       "    0.04457764659894601,\n",
       "    0.04311295594123185,\n",
       "    0.03998703654734054,\n",
       "    0.03972006925948699,\n",
       "    0.03708676499009471,\n",
       "    0.03782450246567569,\n",
       "    0.03623758606982556,\n",
       "    0.03531578647860369,\n",
       "    0.0346967443615049,\n",
       "    0.03316168291689792,\n",
       "    0.03244650635842263,\n",
       "    0.030956604620531288,\n",
       "    0.029936855018384813,\n",
       "    0.02977804936246248,\n",
       "    0.029749352085563806,\n",
       "    0.03038465407233398,\n",
       "    0.029002223027943,\n",
       "    0.030113527435500782,\n",
       "    0.02978576849561214,\n",
       "    0.02717958250872052,\n",
       "    0.027076441197548956,\n",
       "    0.02662568965798164,\n",
       "    0.02583106082218601,\n",
       "    0.026742322642685758,\n",
       "    0.02626461453673155,\n",
       "    0.026035346861309473,\n",
       "    0.026534421663345346,\n",
       "    0.026913387135777293,\n",
       "    0.02690557228214311,\n",
       "    0.026232789428751402,\n",
       "    0.02629802736358861,\n",
       "    0.025000300098780553,\n",
       "    0.026499111103506084,\n",
       "    0.026118672233281903,\n",
       "    0.026184814676162068,\n",
       "    0.0261476963192177,\n",
       "    0.026537584051787143,\n",
       "    0.026316808684336685,\n",
       "    0.024843461307808002,\n",
       "    0.02552316502045801],\n",
       "   [0.21493696172833443,\n",
       "    0.10266092305417406,\n",
       "    0.08109421174828021,\n",
       "    0.06641362515541696,\n",
       "    0.058032064697914755,\n",
       "    0.05043387511052424,\n",
       "    0.05281136684461235,\n",
       "    0.04752896805958298,\n",
       "    0.04463457823583012,\n",
       "    0.04052534161525982,\n",
       "    0.03897363588303051,\n",
       "    0.0398881193791065,\n",
       "    0.036930620667297624,\n",
       "    0.03606843994733499,\n",
       "    0.03551517506736127,\n",
       "    0.033066981595490506,\n",
       "    0.034256494551809735,\n",
       "    0.030994688326772303,\n",
       "    0.03188821534715462,\n",
       "    0.0311712300033927,\n",
       "    0.029904182811539795,\n",
       "    0.030851257386644283,\n",
       "    0.029778744316363008,\n",
       "    0.029220889175586854,\n",
       "    0.028210362868117227,\n",
       "    0.0298108165833557,\n",
       "    0.029528001695470995,\n",
       "    0.029027694878672447,\n",
       "    0.02835033884782897,\n",
       "    0.02860966694724284,\n",
       "    0.02882309571466867,\n",
       "    0.029518839564577775,\n",
       "    0.02917175385959563,\n",
       "    0.0290607609517554,\n",
       "    0.02612246127016442,\n",
       "    0.02692927294344581,\n",
       "    0.02684469533940064,\n",
       "    0.02721027102965372,\n",
       "    0.027324963841634325,\n",
       "    0.026527494462820506,\n",
       "    0.026481588440807172,\n",
       "    0.026801458319627,\n",
       "    0.027744856587511687,\n",
       "    0.02686565236109723,\n",
       "    0.027727050575546263,\n",
       "    0.027677005490907323,\n",
       "    0.02738790995737663,\n",
       "    0.026580769002504893,\n",
       "    0.026704122671380538,\n",
       "    0.026289033862828545],\n",
       "   [0.20668872607946395,\n",
       "    0.11562620883136987,\n",
       "    0.0883102937585907,\n",
       "    0.07461030931511195,\n",
       "    0.06701885799106967,\n",
       "    0.05907901015366079,\n",
       "    0.05720207070143952,\n",
       "    0.0542494074452843,\n",
       "    0.05176746603951324,\n",
       "    0.046295788369124055,\n",
       "    0.04269806202415275,\n",
       "    0.04181860452513247,\n",
       "    0.03594419484467944,\n",
       "    0.036762367323756914,\n",
       "    0.03488361160495551,\n",
       "    0.03396686376815778,\n",
       "    0.03399568917681754,\n",
       "    0.032708604462617225,\n",
       "    0.03140084415997262,\n",
       "    0.030961664620654483,\n",
       "    0.03306778178918175,\n",
       "    0.032296115777114755,\n",
       "    0.030789479759340974,\n",
       "    0.029326543303616198,\n",
       "    0.02871111614664478,\n",
       "    0.027733373973249036,\n",
       "    0.029220274069751757,\n",
       "    0.030437442806032777,\n",
       "    0.030137118218487195,\n",
       "    0.02892234516249664,\n",
       "    0.028934593128319103,\n",
       "    0.029781218576975518,\n",
       "    0.02650294750191897,\n",
       "    0.02751677278677562,\n",
       "    0.027552503879395226,\n",
       "    0.026694184074839724,\n",
       "    0.026553052995036706,\n",
       "    0.026187796513565627,\n",
       "    0.026392802614160247,\n",
       "    0.02703069954627317,\n",
       "    0.02756077700586211,\n",
       "    0.02702910330987197,\n",
       "    0.026843579568792394,\n",
       "    0.025984409628807135,\n",
       "    0.024109764783112223,\n",
       "    0.025158638537036676,\n",
       "    0.025374523891137323,\n",
       "    0.02552379954373864,\n",
       "    0.025233546689001195,\n",
       "    0.025212671506901096],\n",
       "   [0.2098904814839363,\n",
       "    0.10520787134687416,\n",
       "    0.08880633456196811,\n",
       "    0.07232505089063489,\n",
       "    0.06597233633130091,\n",
       "    0.061998823192089186,\n",
       "    0.05714895488015318,\n",
       "    0.05141392032255244,\n",
       "    0.045958353895662,\n",
       "    0.047541112519732635,\n",
       "    0.040312744541327265,\n",
       "    0.04025287879461321,\n",
       "    0.03840263371505789,\n",
       "    0.036514274808893424,\n",
       "    0.0371310630518903,\n",
       "    0.03636474375617854,\n",
       "    0.03409015775754851,\n",
       "    0.03311549269056486,\n",
       "    0.03150943853571626,\n",
       "    0.031220519628841793,\n",
       "    0.031094508182765095,\n",
       "    0.02950563282604198,\n",
       "    0.03005197015421472,\n",
       "    0.03016765163075961,\n",
       "    0.027837469907342892,\n",
       "    0.029326974346381394,\n",
       "    0.02883294354076179,\n",
       "    0.028514824757912085,\n",
       "    0.02854654659237367,\n",
       "    0.029530033856060164,\n",
       "    0.026591457272741537,\n",
       "    0.026597078681575112,\n",
       "    0.027530101132293386,\n",
       "    0.02594626796323173,\n",
       "    0.026938944186628396,\n",
       "    0.027674253647714978,\n",
       "    0.02621150757670621,\n",
       "    0.02595481855315411,\n",
       "    0.026351806735547326,\n",
       "    0.026408431676040345,\n",
       "    0.026019155522286563,\n",
       "    0.025259039565152578,\n",
       "    0.026277477958281996,\n",
       "    0.026091080867878917,\n",
       "    0.02645388129116291,\n",
       "    0.0263195899800181,\n",
       "    0.024696403981785534,\n",
       "    0.024477943627710556,\n",
       "    0.02497745327973207,\n",
       "    0.025192796960855458]],\n",
       "  'train_loss': [[0.2751585264064016,\n",
       "    0.06710829781227137,\n",
       "    0.04368256385423336,\n",
       "    0.03369487668701857,\n",
       "    0.030207383945935633,\n",
       "    0.021073842999380876,\n",
       "    0.02259137899945082,\n",
       "    0.0211367949521094,\n",
       "    0.01622700088629486,\n",
       "    0.014234946906726182,\n",
       "    0.0144495671320457,\n",
       "    0.014752843455419115,\n",
       "    0.013294696431645447,\n",
       "    0.014827732692710713,\n",
       "    0.011825522163093958,\n",
       "    0.013184328897661319,\n",
       "    0.010569893873865525,\n",
       "    0.010523859053374052,\n",
       "    0.007604192492291745,\n",
       "    0.011830775748320298,\n",
       "    0.011286092588486334,\n",
       "    0.008372646396519311,\n",
       "    0.006926798643515543,\n",
       "    0.010095563263877911,\n",
       "    0.007787089790263643,\n",
       "    0.007777301803075778,\n",
       "    0.008789572972733787,\n",
       "    0.006436989880213792,\n",
       "    0.0075654106800663685,\n",
       "    0.007450550982610366,\n",
       "    0.008610135632558266,\n",
       "    0.006207327548894937,\n",
       "    0.006519507438018328,\n",
       "    0.009046221897713843,\n",
       "    0.0050188090246486875,\n",
       "    0.005316824336278652,\n",
       "    0.007247983381902717,\n",
       "    0.0053967165795114885,\n",
       "    0.006503724695239571,\n",
       "    0.005930197503298292,\n",
       "    0.00627892687073088,\n",
       "    0.004881945273766829,\n",
       "    0.0058068663980978315,\n",
       "    0.004980471486458117,\n",
       "    0.0065458244171826534,\n",
       "    0.005898955241561559,\n",
       "    0.0053762851817793144,\n",
       "    0.004946855704018846,\n",
       "    0.003945811639989541,\n",
       "    0.0062027055145224335],\n",
       "   [0.27680783944534015,\n",
       "    0.06126414462001469,\n",
       "    0.04014568774203855,\n",
       "    0.03484573740025438,\n",
       "    0.021732602995910327,\n",
       "    0.022221931165898042,\n",
       "    0.024490351829071687,\n",
       "    0.018483871266884982,\n",
       "    0.017215411279790406,\n",
       "    0.016244713516843527,\n",
       "    0.017459655704963495,\n",
       "    0.01700374668417463,\n",
       "    0.013000780756500175,\n",
       "    0.0145488899536196,\n",
       "    0.01232818977512674,\n",
       "    0.011316653083960399,\n",
       "    0.012794027795553237,\n",
       "    0.010449775616049237,\n",
       "    0.009537726672345918,\n",
       "    0.009230571016400214,\n",
       "    0.012178520825891995,\n",
       "    0.006813493762503478,\n",
       "    0.0083886181389403,\n",
       "    0.00864024404188778,\n",
       "    0.008370281703855984,\n",
       "    0.008934277024601263,\n",
       "    0.007270804428493252,\n",
       "    0.006613525732895864,\n",
       "    0.008323519002662892,\n",
       "    0.008112410976496245,\n",
       "    0.008724141812624731,\n",
       "    0.006471410267659773,\n",
       "    0.008821454492602802,\n",
       "    0.006124365923411979,\n",
       "    0.008727166390580122,\n",
       "    0.004608650827246208,\n",
       "    0.006854712864278129,\n",
       "    0.004519312450235765,\n",
       "    0.007962586526326755,\n",
       "    0.004675531171783816,\n",
       "    0.007632536760588888,\n",
       "    0.006920905791715811,\n",
       "    0.0054619296590559824,\n",
       "    0.006206291410286453,\n",
       "    0.004744065444534292,\n",
       "    0.005021952906600861,\n",
       "    0.0058793490882737606,\n",
       "    0.003765384567143602,\n",
       "    0.005852086595095247,\n",
       "    0.0047598008232051445],\n",
       "   [0.2809966708360132,\n",
       "    0.07327133318486183,\n",
       "    0.040397599114981454,\n",
       "    0.03264016874216759,\n",
       "    0.026919155249299047,\n",
       "    0.025583164734543595,\n",
       "    0.020988337414493897,\n",
       "    0.017469916270904867,\n",
       "    0.020092382294561964,\n",
       "    0.017753384760182338,\n",
       "    0.016371495773468678,\n",
       "    0.013635144546397204,\n",
       "    0.012119377263346335,\n",
       "    0.016309053364008418,\n",
       "    0.012684609089752228,\n",
       "    0.010811338378759371,\n",
       "    0.013161137325593689,\n",
       "    0.011446927151156038,\n",
       "    0.013030100343744319,\n",
       "    0.010526008850354707,\n",
       "    0.009366007015736127,\n",
       "    0.00870299708536905,\n",
       "    0.01289623766954307,\n",
       "    0.007432940702612756,\n",
       "    0.009618358098262595,\n",
       "    0.00916725385464452,\n",
       "    0.007318041738516849,\n",
       "    0.006716143263454942,\n",
       "    0.008521349257240166,\n",
       "    0.007848602270295515,\n",
       "    0.00664331385992257,\n",
       "    0.006553769818303659,\n",
       "    0.008050212121285184,\n",
       "    0.0054725167482440615,\n",
       "    0.009883546732138353,\n",
       "    0.00464042537877278,\n",
       "    0.00824650595580885,\n",
       "    0.0057640524768837334,\n",
       "    0.005616339922744448,\n",
       "    0.0061958964842568475,\n",
       "    0.006034843506966493,\n",
       "    0.007023077156567911,\n",
       "    0.00460820856926369,\n",
       "    0.0045764853825398105,\n",
       "    0.002878496853201172,\n",
       "    0.005205141698629992,\n",
       "    0.0049448586850858395,\n",
       "    0.005316819786785737,\n",
       "    0.006340470808772332,\n",
       "    0.0034800842571358196],\n",
       "   [0.27092440389843364,\n",
       "    0.06992934012698064,\n",
       "    0.04312548275562439,\n",
       "    0.03170558913672859,\n",
       "    0.0314711681119236,\n",
       "    0.022902421975544957,\n",
       "    0.020835552534669628,\n",
       "    0.020496188613432633,\n",
       "    0.013637726331694137,\n",
       "    0.017988898863123156,\n",
       "    0.01646182026019736,\n",
       "    0.014292562233065085,\n",
       "    0.014564625221741678,\n",
       "    0.012730012533614055,\n",
       "    0.011922988480804628,\n",
       "    0.011262379543604279,\n",
       "    0.011536282640327116,\n",
       "    0.016837605267640613,\n",
       "    0.011778855299478723,\n",
       "    0.009067598746347728,\n",
       "    0.011318365009783215,\n",
       "    0.009384674466534638,\n",
       "    0.01026006097173936,\n",
       "    0.010512335344407308,\n",
       "    0.00782880639980502,\n",
       "    0.007672973884725982,\n",
       "    0.00869540375230568,\n",
       "    0.006179430023452098,\n",
       "    0.00896613039292717,\n",
       "    0.007612745422959243,\n",
       "    0.006550567163135579,\n",
       "    0.009044875536327528,\n",
       "    0.010820424218779106,\n",
       "    0.006099171462603853,\n",
       "    0.007329452259247735,\n",
       "    0.007953049522772485,\n",
       "    0.005352014657808516,\n",
       "    0.005144539866238324,\n",
       "    0.009413626307154524,\n",
       "    0.00529636588310701,\n",
       "    0.005812035884795438,\n",
       "    0.007887440272312154,\n",
       "    0.003551733639603929,\n",
       "    0.005116695003173049,\n",
       "    0.007535966369909478,\n",
       "    0.006327454203569891,\n",
       "    0.0030936668293444066,\n",
       "    0.004535918382092838,\n",
       "    0.006557628731902751,\n",
       "    0.005686432054276083],\n",
       "   [0.28632072630243977,\n",
       "    0.07011788063528586,\n",
       "    0.03666758617421835,\n",
       "    0.03994774765089689,\n",
       "    0.026031987484226016,\n",
       "    0.020368128372806348,\n",
       "    0.024339944357057205,\n",
       "    0.0176278884070876,\n",
       "    0.020309288767629944,\n",
       "    0.01373715020942016,\n",
       "    0.01899150883959594,\n",
       "    0.012758924089214915,\n",
       "    0.015128205427925437,\n",
       "    0.01468657347853827,\n",
       "    0.011925946487114212,\n",
       "    0.010378636259338613,\n",
       "    0.013010511275425166,\n",
       "    0.011485791325224282,\n",
       "    0.012875284339963395,\n",
       "    0.01152531683118731,\n",
       "    0.009728094908716187,\n",
       "    0.008954735668544194,\n",
       "    0.007876272092160925,\n",
       "    0.00612609363017715,\n",
       "    0.010356120149488338,\n",
       "    0.007853268132700323,\n",
       "    0.0073321287239526325,\n",
       "    0.007962617526214025,\n",
       "    0.00879340129956278,\n",
       "    0.004921559304336515,\n",
       "    0.010315343924948529,\n",
       "    0.00653128252379178,\n",
       "    0.007951972338896909,\n",
       "    0.007034891020429044,\n",
       "    0.0076438602009054005,\n",
       "    0.007521752042940574,\n",
       "    0.007142818489108435,\n",
       "    0.006188048621691993,\n",
       "    0.005803867072151084,\n",
       "    0.005853777980671545,\n",
       "    0.006199496091266512,\n",
       "    0.005167966744472328,\n",
       "    0.004365031608269767,\n",
       "    0.006145660964121947,\n",
       "    0.00521372030532882,\n",
       "    0.005785472068341423,\n",
       "    0.005031721307129379,\n",
       "    0.0069583549343690005,\n",
       "    0.004932474808993523,\n",
       "    0.002955383936271135]]},\n",
       " 'MNIST CNN on Non IID': {'hyperparams': {'C': 0.1,\n",
       "   'E': 10,\n",
       "   'K': 100,\n",
       "   'batch_size': 10,\n",
       "   'lr': 0.05,\n",
       "   'rounds': 50},\n",
       "  'test_accuracy': [[20.253164556962027,\n",
       "    62.40506329113924,\n",
       "    69.24050632911393,\n",
       "    70.88607594936708,\n",
       "    84.30379746835443,\n",
       "    80.75949367088607,\n",
       "    80.88607594936708,\n",
       "    82.78481012658227,\n",
       "    87.9746835443038,\n",
       "    93.79746835443038,\n",
       "    86.58227848101266,\n",
       "    95.18987341772151,\n",
       "    95.9493670886076,\n",
       "    95.56962025316456,\n",
       "    93.54430379746836,\n",
       "    95.44303797468355,\n",
       "    96.45569620253164,\n",
       "    97.0886075949367,\n",
       "    94.68354430379746,\n",
       "    96.9620253164557,\n",
       "    96.58227848101266,\n",
       "    97.34177215189874,\n",
       "    95.9493670886076,\n",
       "    97.34177215189874,\n",
       "    96.20253164556962,\n",
       "    94.30379746835443,\n",
       "    97.72151898734177,\n",
       "    97.21518987341773,\n",
       "    96.07594936708861,\n",
       "    96.45569620253164,\n",
       "    97.34177215189874,\n",
       "    96.9620253164557,\n",
       "    95.56962025316456,\n",
       "    96.70886075949367,\n",
       "    96.58227848101266,\n",
       "    98.73417721518987,\n",
       "    96.58227848101266,\n",
       "    98.22784810126582,\n",
       "    98.35443037974683,\n",
       "    98.35443037974683,\n",
       "    97.72151898734177,\n",
       "    98.86075949367088,\n",
       "    96.32911392405063,\n",
       "    97.46835443037975,\n",
       "    97.84810126582279,\n",
       "    98.60759493670886,\n",
       "    99.24050632911393,\n",
       "    98.86075949367088,\n",
       "    98.9873417721519,\n",
       "    98.9873417721519],\n",
       "   [24.303797468354432,\n",
       "    46.075949367088604,\n",
       "    61.265822784810126,\n",
       "    52.91139240506329,\n",
       "    76.9620253164557,\n",
       "    82.27848101265823,\n",
       "    86.70886075949367,\n",
       "    93.0379746835443,\n",
       "    92.0253164556962,\n",
       "    95.18987341772151,\n",
       "    92.40506329113924,\n",
       "    91.39240506329114,\n",
       "    90.50632911392405,\n",
       "    96.45569620253164,\n",
       "    94.17721518987342,\n",
       "    94.81012658227849,\n",
       "    95.56962025316456,\n",
       "    95.69620253164557,\n",
       "    96.07594936708861,\n",
       "    96.58227848101266,\n",
       "    96.70886075949367,\n",
       "    94.81012658227849,\n",
       "    94.30379746835443,\n",
       "    95.9493670886076,\n",
       "    96.70886075949367,\n",
       "    97.72151898734177,\n",
       "    98.60759493670886,\n",
       "    97.46835443037975,\n",
       "    98.22784810126582,\n",
       "    97.72151898734177,\n",
       "    98.35443037974683,\n",
       "    97.84810126582279,\n",
       "    98.10126582278481,\n",
       "    98.35443037974683,\n",
       "    98.48101265822785,\n",
       "    98.35443037974683,\n",
       "    98.35443037974683,\n",
       "    98.60759493670886,\n",
       "    98.48101265822785,\n",
       "    98.60759493670886,\n",
       "    98.48101265822785,\n",
       "    98.35443037974683,\n",
       "    98.73417721518987,\n",
       "    98.86075949367088,\n",
       "    98.73417721518987,\n",
       "    98.86075949367088,\n",
       "    98.73417721518987,\n",
       "    98.73417721518987,\n",
       "    98.9873417721519,\n",
       "    98.86075949367088],\n",
       "   [18.227848101265824,\n",
       "    18.10126582278481,\n",
       "    56.835443037974684,\n",
       "    67.59493670886076,\n",
       "    75.9493670886076,\n",
       "    90.37974683544304,\n",
       "    85.56962025316456,\n",
       "    87.34177215189874,\n",
       "    92.53164556962025,\n",
       "    80.12658227848101,\n",
       "    93.67088607594937,\n",
       "    94.30379746835443,\n",
       "    95.18987341772151,\n",
       "    96.20253164556962,\n",
       "    93.54430379746836,\n",
       "    96.32911392405063,\n",
       "    94.9367088607595,\n",
       "    95.56962025316456,\n",
       "    97.59493670886076,\n",
       "    96.20253164556962,\n",
       "    96.32911392405063,\n",
       "    96.07594936708861,\n",
       "    97.9746835443038,\n",
       "    97.34177215189874,\n",
       "    97.21518987341773,\n",
       "    97.9746835443038,\n",
       "    97.9746835443038,\n",
       "    97.72151898734177,\n",
       "    98.35443037974683,\n",
       "    97.84810126582279,\n",
       "    97.46835443037975,\n",
       "    97.59493670886076,\n",
       "    98.10126582278481,\n",
       "    97.9746835443038,\n",
       "    98.22784810126582,\n",
       "    97.72151898734177,\n",
       "    98.35443037974683,\n",
       "    98.22784810126582,\n",
       "    98.73417721518987,\n",
       "    97.59493670886076,\n",
       "    96.9620253164557,\n",
       "    98.22784810126582,\n",
       "    98.73417721518987,\n",
       "    98.9873417721519,\n",
       "    97.9746835443038,\n",
       "    98.9873417721519,\n",
       "    99.11392405063292,\n",
       "    97.46835443037975,\n",
       "    98.35443037974683,\n",
       "    98.86075949367088],\n",
       "   [29.240506329113924,\n",
       "    47.34177215189873,\n",
       "    48.9873417721519,\n",
       "    61.139240506329116,\n",
       "    77.84810126582279,\n",
       "    77.9746835443038,\n",
       "    81.77215189873418,\n",
       "    83.41772151898734,\n",
       "    85.31645569620254,\n",
       "    93.54430379746836,\n",
       "    95.56962025316456,\n",
       "    94.17721518987342,\n",
       "    90.12658227848101,\n",
       "    95.0632911392405,\n",
       "    94.17721518987342,\n",
       "    95.9493670886076,\n",
       "    96.83544303797468,\n",
       "    96.58227848101266,\n",
       "    96.32911392405063,\n",
       "    97.59493670886076,\n",
       "    97.0886075949367,\n",
       "    98.10126582278481,\n",
       "    97.21518987341773,\n",
       "    95.0632911392405,\n",
       "    98.48101265822785,\n",
       "    94.30379746835443,\n",
       "    98.73417721518987,\n",
       "    98.22784810126582,\n",
       "    97.59493670886076,\n",
       "    97.84810126582279,\n",
       "    97.0886075949367,\n",
       "    97.9746835443038,\n",
       "    97.9746835443038,\n",
       "    97.72151898734177,\n",
       "    97.9746835443038,\n",
       "    98.35443037974683,\n",
       "    98.60759493670886,\n",
       "    98.48101265822785,\n",
       "    98.35443037974683,\n",
       "    98.35443037974683,\n",
       "    98.60759493670886,\n",
       "    97.9746835443038,\n",
       "    98.86075949367088,\n",
       "    98.9873417721519,\n",
       "    97.84810126582279,\n",
       "    98.73417721518987,\n",
       "    98.9873417721519,\n",
       "    98.48101265822785,\n",
       "    99.24050632911393,\n",
       "    98.73417721518987],\n",
       "   [12.278481012658228,\n",
       "    24.68354430379747,\n",
       "    43.037974683544306,\n",
       "    62.65822784810127,\n",
       "    73.41772151898734,\n",
       "    82.65822784810126,\n",
       "    73.54430379746836,\n",
       "    68.48101265822785,\n",
       "    80.25316455696202,\n",
       "    83.79746835443038,\n",
       "    83.29113924050633,\n",
       "    94.0506329113924,\n",
       "    94.68354430379746,\n",
       "    87.46835443037975,\n",
       "    94.68354430379746,\n",
       "    84.30379746835443,\n",
       "    92.65822784810126,\n",
       "    96.70886075949367,\n",
       "    88.10126582278481,\n",
       "    87.72151898734177,\n",
       "    90.25316455696202,\n",
       "    89.62025316455696,\n",
       "    95.0632911392405,\n",
       "    92.9113924050633,\n",
       "    97.34177215189874,\n",
       "    95.56962025316456,\n",
       "    95.69620253164557,\n",
       "    96.45569620253164,\n",
       "    95.44303797468355,\n",
       "    95.9493670886076,\n",
       "    98.35443037974683,\n",
       "    98.10126582278481,\n",
       "    97.59493670886076,\n",
       "    97.9746835443038,\n",
       "    98.35443037974683,\n",
       "    98.10126582278481,\n",
       "    97.0886075949367,\n",
       "    96.32911392405063,\n",
       "    97.46835443037975,\n",
       "    98.60759493670886,\n",
       "    98.10126582278481,\n",
       "    98.22784810126582,\n",
       "    98.60759493670886,\n",
       "    97.21518987341773,\n",
       "    97.0886075949367,\n",
       "    96.70886075949367,\n",
       "    98.48101265822785,\n",
       "    97.59493670886076,\n",
       "    98.22784810126582,\n",
       "    98.60759493670886]],\n",
       "  'test_loss': [[2.0689556293487548,\n",
       "    1.736101571083069,\n",
       "    0.9723292611122132,\n",
       "    0.8257440708160401,\n",
       "    0.5592529003620148,\n",
       "    0.48982332797050476,\n",
       "    0.7152421183109283,\n",
       "    0.4561609885931015,\n",
       "    0.30422684535980227,\n",
       "    0.2135374899327755,\n",
       "    0.33305617876052857,\n",
       "    0.1527027723699808,\n",
       "    0.15005532059073448,\n",
       "    0.1457638788074255,\n",
       "    0.19015368984937667,\n",
       "    0.16052452442348003,\n",
       "    0.1535167486190796,\n",
       "    0.10207696719020605,\n",
       "    0.13358684555888176,\n",
       "    0.09459865484014153,\n",
       "    0.09323406512141227,\n",
       "    0.08960907938331365,\n",
       "    0.10658761765137315,\n",
       "    0.07750639696121216,\n",
       "    0.1000865833491087,\n",
       "    0.15224873656630517,\n",
       "    0.08114131038039922,\n",
       "    0.08077846393883228,\n",
       "    0.10524737669676543,\n",
       "    0.09891628094092011,\n",
       "    0.08239640629757196,\n",
       "    0.07735141555024311,\n",
       "    0.12789369680471718,\n",
       "    0.10719324751347303,\n",
       "    0.10445555611848831,\n",
       "    0.0553358416646719,\n",
       "    0.08320808489322662,\n",
       "    0.06520148724317551,\n",
       "    0.065883069389686,\n",
       "    0.06185564043968916,\n",
       "    0.06377268083244562,\n",
       "    0.056125887023471294,\n",
       "    0.10324323297739028,\n",
       "    0.09534728131294251,\n",
       "    0.06986305173933506,\n",
       "    0.05055845413370989,\n",
       "    0.052813772687455636,\n",
       "    0.05330841936499346,\n",
       "    0.0569393301198259,\n",
       "    0.04849203179525211],\n",
       "   [2.2217685302734376,\n",
       "    1.602509794807434,\n",
       "    1.1567535836219787,\n",
       "    1.429989273071289,\n",
       "    0.7930545471668243,\n",
       "    0.49262388570308685,\n",
       "    0.45723566627502443,\n",
       "    0.277774187541008,\n",
       "    0.25754299186468127,\n",
       "    0.1867617939889431,\n",
       "    0.2140059184372425,\n",
       "    0.22891661015748976,\n",
       "    0.23546933255195618,\n",
       "    0.1297194303303957,\n",
       "    0.17369082570672034,\n",
       "    0.14739162335395814,\n",
       "    0.13570444695800543,\n",
       "    0.1226574543043971,\n",
       "    0.1200542866230011,\n",
       "    0.10952961697131396,\n",
       "    0.10295479838252068,\n",
       "    0.12986802995800972,\n",
       "    0.14372271497249603,\n",
       "    0.12130515520572663,\n",
       "    0.09592362531274558,\n",
       "    0.0810811913087964,\n",
       "    0.06650958956256509,\n",
       "    0.08195705036073923,\n",
       "    0.07166217147707939,\n",
       "    0.0764700187318027,\n",
       "    0.05990184022933245,\n",
       "    0.07866721309889108,\n",
       "    0.06549933273931965,\n",
       "    0.07149201076552272,\n",
       "    0.06161916751340032,\n",
       "    0.07226821449659765,\n",
       "    0.06573893114924431,\n",
       "    0.06870635089613497,\n",
       "    0.053478222969546914,\n",
       "    0.054247412214428185,\n",
       "    0.06093817472755909,\n",
       "    0.05457887287214398,\n",
       "    0.04745276061743498,\n",
       "    0.042919506560638546,\n",
       "    0.04493604005463421,\n",
       "    0.043927232804894446,\n",
       "    0.051951308956742284,\n",
       "    0.04655716579072178,\n",
       "    0.0408302393168211,\n",
       "    0.04576541264615953],\n",
       "   [2.2325249240875245,\n",
       "    2.004249538421631,\n",
       "    1.280712834739685,\n",
       "    0.8049835003852844,\n",
       "    0.7134485340118408,\n",
       "    0.32395536510944367,\n",
       "    0.4922534396648407,\n",
       "    0.3171104661345482,\n",
       "    0.22292884277105332,\n",
       "    0.4770589249610901,\n",
       "    0.2366157354593277,\n",
       "    0.1664390948534012,\n",
       "    0.14957280485630037,\n",
       "    0.1370398206949234,\n",
       "    0.17260705696344375,\n",
       "    0.13067556772232056,\n",
       "    0.11548547396659851,\n",
       "    0.13316368431597947,\n",
       "    0.09521337493360042,\n",
       "    0.11371382140815257,\n",
       "    0.11120311254262924,\n",
       "    0.10073108713366091,\n",
       "    0.08943489115536213,\n",
       "    0.08228115219660104,\n",
       "    0.08556621458381414,\n",
       "    0.07372644712720067,\n",
       "    0.07683605518378317,\n",
       "    0.06498175223469735,\n",
       "    0.06161050593778491,\n",
       "    0.06730682412311435,\n",
       "    0.07367463682219386,\n",
       "    0.07823456361610442,\n",
       "    0.0726261323923245,\n",
       "    0.06165711926277727,\n",
       "    0.057917035869136456,\n",
       "    0.05589291758164763,\n",
       "    0.052683185691200195,\n",
       "    0.05552984034717083,\n",
       "    0.050914919301867485,\n",
       "    0.07147006985265762,\n",
       "    0.083477932792902,\n",
       "    0.08215935980975628,\n",
       "    0.05022596247084439,\n",
       "    0.04716193663552404,\n",
       "    0.06811619984209538,\n",
       "    0.05551378978900611,\n",
       "    0.05072443006634712,\n",
       "    0.07951700145155191,\n",
       "    0.056063669342175125,\n",
       "    0.0451651464369148],\n",
       "   [2.120135450363159,\n",
       "    1.6778202396392823,\n",
       "    1.5583040866851807,\n",
       "    1.11625015335083,\n",
       "    0.7213679007530213,\n",
       "    0.7000604876518249,\n",
       "    0.46085054626464844,\n",
       "    0.5044112827301025,\n",
       "    0.34750364694595337,\n",
       "    0.19233824237585068,\n",
       "    0.14260402155816554,\n",
       "    0.1730954826116562,\n",
       "    0.2543843898773193,\n",
       "    0.12834891547858715,\n",
       "    0.14441190523356198,\n",
       "    0.11658525056540966,\n",
       "    0.08853676647096873,\n",
       "    0.0924829984433949,\n",
       "    0.1138647197932005,\n",
       "    0.08050716630443931,\n",
       "    0.08545231851488352,\n",
       "    0.07364040589630604,\n",
       "    0.07966229724287986,\n",
       "    0.1234931661516428,\n",
       "    0.06514530021883547,\n",
       "    0.15099786420166492,\n",
       "    0.05651724679023028,\n",
       "    0.053753561423718926,\n",
       "    0.07633744806051254,\n",
       "    0.06406546380985528,\n",
       "    0.09467758579850197,\n",
       "    0.06737189831528813,\n",
       "    0.06513761349469423,\n",
       "    0.07447291218787432,\n",
       "    0.05896976684778929,\n",
       "    0.05694871577210724,\n",
       "    0.05267396389320493,\n",
       "    0.05160315046533942,\n",
       "    0.056438782018050554,\n",
       "    0.05316750269727782,\n",
       "    0.05358818309949711,\n",
       "    0.05984316837750375,\n",
       "    0.05887491346714087,\n",
       "    0.04587646578550339,\n",
       "    0.07525433095395564,\n",
       "    0.0494802556887269,\n",
       "    0.03861705912612379,\n",
       "    0.0628237991134869,\n",
       "    0.04374754488673061,\n",
       "    0.04371527785360813],\n",
       "   [2.2760480911254883,\n",
       "    1.7707060277938842,\n",
       "    1.6202389938354491,\n",
       "    1.0208618412017822,\n",
       "    0.8098142660140991,\n",
       "    0.4533729320526123,\n",
       "    0.8515695455551148,\n",
       "    0.8050028548240662,\n",
       "    0.48061771001815795,\n",
       "    0.5537333018302918,\n",
       "    0.48176908445358274,\n",
       "    0.21492089682817458,\n",
       "    0.16570784521102905,\n",
       "    0.2898276589393616,\n",
       "    0.22160697807073593,\n",
       "    0.40598577947616576,\n",
       "    0.21125369980335235,\n",
       "    0.1314983626008034,\n",
       "    0.2408541711449623,\n",
       "    0.3032036853790283,\n",
       "    0.23872689673900604,\n",
       "    0.22516004447937013,\n",
       "    0.13516930868327617,\n",
       "    0.18630990076065063,\n",
       "    0.09685718948096037,\n",
       "    0.130630480492115,\n",
       "    0.11831855542063713,\n",
       "    0.11523875867724419,\n",
       "    0.14262946181297304,\n",
       "    0.12112420808076858,\n",
       "    0.061077914415299896,\n",
       "    0.07378535098433495,\n",
       "    0.080172548609972,\n",
       "    0.07791562170088291,\n",
       "    0.0705103595636785,\n",
       "    0.06321537059992552,\n",
       "    0.07198954460248351,\n",
       "    0.1073354538552463,\n",
       "    0.0775747403562069,\n",
       "    0.0502731558278203,\n",
       "    0.06887051222957671,\n",
       "    0.06200238651409745,\n",
       "    0.0533069805726409,\n",
       "    0.08870990549325942,\n",
       "    0.09428818714618682,\n",
       "    0.08986269738078118,\n",
       "    0.06241091412603855,\n",
       "    0.08747206720486284,\n",
       "    0.06750252692215145,\n",
       "    0.05307375021092594]],\n",
       "  'train_loss': [[0.042660705626225676,\n",
       "    0.03450827225845075,\n",
       "    0.019574450244091584,\n",
       "    0.013154874768932448,\n",
       "    0.014000767645680218,\n",
       "    0.008938741588615,\n",
       "    0.011985070315847507,\n",
       "    0.007233238564745485,\n",
       "    0.01136607043136208,\n",
       "    0.012571158586864847,\n",
       "    0.004605976596381686,\n",
       "    0.0076871443449385134,\n",
       "    0.008720573397628693,\n",
       "    0.008061211209391825,\n",
       "    0.004341370473545438,\n",
       "    0.005884617560766904,\n",
       "    0.008912595019890465,\n",
       "    0.00528053602815444,\n",
       "    0.0037928754158387617,\n",
       "    0.005211577204187197,\n",
       "    0.004284109615619855,\n",
       "    0.00398431454524767,\n",
       "    0.007497890395306729,\n",
       "    0.005030275541113528,\n",
       "    0.0068119387270811444,\n",
       "    0.003892465646956508,\n",
       "    0.002257201954600198,\n",
       "    0.005773933462874178,\n",
       "    0.003068074006702527,\n",
       "    0.0028027919778140704,\n",
       "    0.003435853873416988,\n",
       "    0.0036972268855859853,\n",
       "    0.0021976825377640293,\n",
       "    0.004502546419432839,\n",
       "    0.005290952949363958,\n",
       "    0.0038555266828033978,\n",
       "    0.007578679455439083,\n",
       "    0.00408723133680711,\n",
       "    0.0029970936129740918,\n",
       "    0.005018078426656273,\n",
       "    0.0031673117757378437,\n",
       "    0.004019554833622768,\n",
       "    0.0016624392827534798,\n",
       "    0.005896526026064255,\n",
       "    0.0031572618482142586,\n",
       "    0.0028893576578120743,\n",
       "    0.002807857904516016,\n",
       "    0.0021800810891041723,\n",
       "    0.002121553403596501,\n",
       "    0.0036387365152349823],\n",
       "   [0.06554663245085793,\n",
       "    0.03612378989500721,\n",
       "    0.01769536160755445,\n",
       "    0.01614351212104237,\n",
       "    0.02179577157986666,\n",
       "    0.007433245393507801,\n",
       "    0.012116686672054719,\n",
       "    0.013316436598393188,\n",
       "    0.011287908919563133,\n",
       "    0.009191739535914536,\n",
       "    0.011042378329408652,\n",
       "    0.0054574939785061475,\n",
       "    0.0077771057746909514,\n",
       "    0.010385269839143837,\n",
       "    0.009705030055682993,\n",
       "    0.005470457553932237,\n",
       "    0.006115367261018417,\n",
       "    0.008679232157222872,\n",
       "    0.0054913881739821774,\n",
       "    0.004577693849920249,\n",
       "    0.004254587312809458,\n",
       "    0.007853899381701787,\n",
       "    0.005203837353924237,\n",
       "    0.005414549023026578,\n",
       "    0.00600820139523506,\n",
       "    0.0069518291141208695,\n",
       "    0.007640169947499946,\n",
       "    0.004037657298256073,\n",
       "    0.004040840658463646,\n",
       "    0.003554864933773739,\n",
       "    0.007444036743843217,\n",
       "    0.0034443573157237465,\n",
       "    0.004953143620800168,\n",
       "    0.003588070259613732,\n",
       "    0.004416641713295125,\n",
       "    0.003035373828433645,\n",
       "    0.005377577167151934,\n",
       "    0.0045822783608508175,\n",
       "    0.004893281468764826,\n",
       "    0.003950214451877842,\n",
       "    0.004069351574475601,\n",
       "    0.0029056346344600798,\n",
       "    0.0027657941504562263,\n",
       "    0.002278998059487256,\n",
       "    0.0017906015010576526,\n",
       "    0.0058477601205076625,\n",
       "    0.0046241343081879155,\n",
       "    0.004829891026655058,\n",
       "    0.005199763869970354,\n",
       "    0.003278619565361203],\n",
       "   [0.05495690412369384,\n",
       "    0.0314492807563556,\n",
       "    0.028582377777884756,\n",
       "    0.018050427275772486,\n",
       "    0.014867169665362884,\n",
       "    0.007011771845689958,\n",
       "    0.012197639886145724,\n",
       "    0.0063320068466490185,\n",
       "    0.009083031261377227,\n",
       "    0.007749930404231775,\n",
       "    0.014132733587113616,\n",
       "    0.007260611006057247,\n",
       "    0.007277031415564223,\n",
       "    0.010128477334623817,\n",
       "    0.005480159088417047,\n",
       "    0.006932104221101157,\n",
       "    0.006444160795145833,\n",
       "    0.008465773885798267,\n",
       "    0.006668686988922962,\n",
       "    0.006594270187319512,\n",
       "    0.005140026307949847,\n",
       "    0.003456756725467128,\n",
       "    0.006562327820504145,\n",
       "    0.0036109140879301916,\n",
       "    0.007130156335118837,\n",
       "    0.004815051885301965,\n",
       "    0.0032806034531148607,\n",
       "    0.005016190487275247,\n",
       "    0.005628509438869388,\n",
       "    0.004550057148774638,\n",
       "    0.004584826546878916,\n",
       "    0.002768969471655712,\n",
       "    0.00344907479076857,\n",
       "    0.0038615981031670287,\n",
       "    0.003631909409532464,\n",
       "    0.003798218513838364,\n",
       "    0.0029759373257337942,\n",
       "    0.003614120265672545,\n",
       "    0.003920101286982564,\n",
       "    0.0017666837726175015,\n",
       "    0.0031583199728328084,\n",
       "    0.002689340661886969,\n",
       "    0.003984080919467848,\n",
       "    0.004117519942941872,\n",
       "    0.005520238302342949,\n",
       "    0.0030382366223630397,\n",
       "    0.004618754333388928,\n",
       "    0.002176138519847303,\n",
       "    0.00385431904791654,\n",
       "    0.0044759059743699325],\n",
       "   [0.0458991737246899,\n",
       "    0.04615716312608527,\n",
       "    0.019064184040935198,\n",
       "    0.019566643762894246,\n",
       "    0.010942242801674171,\n",
       "    0.009258401397537981,\n",
       "    0.01281066030167025,\n",
       "    0.009480769097845617,\n",
       "    0.011702925360377514,\n",
       "    0.0066815899996488775,\n",
       "    0.006835353696035777,\n",
       "    0.007971033019506059,\n",
       "    0.012930364241542976,\n",
       "    0.004977921088914569,\n",
       "    0.0034782867851691603,\n",
       "    0.006759615934848693,\n",
       "    0.005969868445346922,\n",
       "    0.007534223241043321,\n",
       "    0.006050695467119678,\n",
       "    0.004037475678537336,\n",
       "    0.005519967957124608,\n",
       "    0.00511109688119752,\n",
       "    0.006360883165619292,\n",
       "    0.0043762864060738465,\n",
       "    0.004296826244040778,\n",
       "    0.0027067727945335523,\n",
       "    0.005656395642949624,\n",
       "    0.00432844327008372,\n",
       "    0.0034959730456935447,\n",
       "    0.003310125779187362,\n",
       "    0.0058855312247755795,\n",
       "    0.004116994418067279,\n",
       "    0.002881429880521951,\n",
       "    0.00603128613931249,\n",
       "    0.003825991369385625,\n",
       "    0.003106045564895055,\n",
       "    0.0026837187117997956,\n",
       "    0.001962838006002115,\n",
       "    0.004720652774225137,\n",
       "    0.0017639465697860228,\n",
       "    0.0034925644250897838,\n",
       "    0.0013576455604189955,\n",
       "    0.0027061483657626366,\n",
       "    0.0042316010338046325,\n",
       "    0.0031497320498412988,\n",
       "    0.0020444760087760683,\n",
       "    0.0023474570230602653,\n",
       "    0.002923234192777716,\n",
       "    0.004971182666497797,\n",
       "    0.0034518887619949105],\n",
       "   [0.06904416013389994,\n",
       "    0.03491321564578688,\n",
       "    0.02751457570361915,\n",
       "    0.0136902562728014,\n",
       "    0.009096363166322192,\n",
       "    0.010752287194313006,\n",
       "    0.009806541845950527,\n",
       "    0.012011987163791014,\n",
       "    0.008192094842246584,\n",
       "    0.010131868660805352,\n",
       "    0.00624606598504278,\n",
       "    0.007950871068564146,\n",
       "    0.005623924405311867,\n",
       "    0.0052864763026856535,\n",
       "    0.004465500890801613,\n",
       "    0.00704813683095445,\n",
       "    0.010266339009778446,\n",
       "    0.004307414350752684,\n",
       "    0.006738091889054042,\n",
       "    0.005648730479656512,\n",
       "    0.00540647985663633,\n",
       "    0.00527639641292447,\n",
       "    0.004555365546199076,\n",
       "    0.004025483218526519,\n",
       "    0.0037704695387139097,\n",
       "    0.006178406485925196,\n",
       "    0.005153150148888881,\n",
       "    0.0034772843059753533,\n",
       "    0.007556822708317616,\n",
       "    0.003293378173603934,\n",
       "    0.002808910897522553,\n",
       "    0.005947095192366904,\n",
       "    0.00336395350394541,\n",
       "    0.0037695791327188345,\n",
       "    0.0031049765304663624,\n",
       "    0.006599109339890871,\n",
       "    0.005541458641968701,\n",
       "    0.004221923803278528,\n",
       "    0.0027602325470097683,\n",
       "    0.0028822494584772353,\n",
       "    0.002985137910214936,\n",
       "    0.0031548147994242444,\n",
       "    0.002345212633884666,\n",
       "    0.003939823693881496,\n",
       "    0.004497374036440937,\n",
       "    0.004084980009164422,\n",
       "    0.0034421726912410864,\n",
       "    0.0035434796413926833,\n",
       "    0.003240946200951917,\n",
       "    0.0031241644489895777]]},\n",
       " 'MNIST MLP on IID': {'hyperparams': {'C': 0.1,\n",
       "   'E': 10,\n",
       "   'K': 100,\n",
       "   'batch_size': 10,\n",
       "   'lr': 0.05,\n",
       "   'rounds': 50},\n",
       "  'test_accuracy': [[92.15189873417721,\n",
       "    94.17721518987342,\n",
       "    94.43037974683544,\n",
       "    94.81012658227849,\n",
       "    94.9367088607595,\n",
       "    95.31645569620254,\n",
       "    95.31645569620254,\n",
       "    95.9493670886076,\n",
       "    96.20253164556962,\n",
       "    96.20253164556962,\n",
       "    96.07594936708861,\n",
       "    96.32911392405063,\n",
       "    96.58227848101266,\n",
       "    96.9620253164557,\n",
       "    96.83544303797468,\n",
       "    96.83544303797468,\n",
       "    96.9620253164557,\n",
       "    96.58227848101266,\n",
       "    97.21518987341773,\n",
       "    97.0886075949367,\n",
       "    97.21518987341773,\n",
       "    97.21518987341773,\n",
       "    97.21518987341773,\n",
       "    97.59493670886076,\n",
       "    97.59493670886076,\n",
       "    97.34177215189874,\n",
       "    97.72151898734177,\n",
       "    97.59493670886076,\n",
       "    97.84810126582279,\n",
       "    97.34177215189874,\n",
       "    97.84810126582279,\n",
       "    98.22784810126582,\n",
       "    97.72151898734177,\n",
       "    98.22784810126582,\n",
       "    97.9746835443038,\n",
       "    98.10126582278481,\n",
       "    97.59493670886076,\n",
       "    97.84810126582279,\n",
       "    97.84810126582279,\n",
       "    97.84810126582279,\n",
       "    97.72151898734177,\n",
       "    97.72151898734177,\n",
       "    97.84810126582279,\n",
       "    98.22784810126582,\n",
       "    97.9746835443038,\n",
       "    97.9746835443038,\n",
       "    98.35443037974683,\n",
       "    97.72151898734177,\n",
       "    97.72151898734177,\n",
       "    98.22784810126582],\n",
       "   [90.75949367088607,\n",
       "    93.92405063291139,\n",
       "    94.30379746835443,\n",
       "    94.68354430379746,\n",
       "    94.9367088607595,\n",
       "    95.56962025316456,\n",
       "    96.07594936708861,\n",
       "    96.07594936708861,\n",
       "    95.69620253164557,\n",
       "    96.45569620253164,\n",
       "    96.32911392405063,\n",
       "    96.70886075949367,\n",
       "    96.45569620253164,\n",
       "    96.45569620253164,\n",
       "    96.70886075949367,\n",
       "    96.70886075949367,\n",
       "    97.21518987341773,\n",
       "    97.34177215189874,\n",
       "    97.59493670886076,\n",
       "    97.46835443037975,\n",
       "    98.10126582278481,\n",
       "    97.34177215189874,\n",
       "    97.34177215189874,\n",
       "    97.21518987341773,\n",
       "    97.34177215189874,\n",
       "    97.59493670886076,\n",
       "    97.21518987341773,\n",
       "    97.46835443037975,\n",
       "    97.21518987341773,\n",
       "    97.34177215189874,\n",
       "    97.21518987341773,\n",
       "    97.34177215189874,\n",
       "    97.34177215189874,\n",
       "    97.21518987341773,\n",
       "    97.59493670886076,\n",
       "    97.59493670886076,\n",
       "    97.34177215189874,\n",
       "    97.84810126582279,\n",
       "    97.34177215189874,\n",
       "    97.46835443037975,\n",
       "    97.46835443037975,\n",
       "    97.46835443037975,\n",
       "    97.21518987341773,\n",
       "    97.72151898734177,\n",
       "    97.84810126582279,\n",
       "    97.9746835443038,\n",
       "    97.84810126582279,\n",
       "    97.9746835443038,\n",
       "    97.84810126582279,\n",
       "    98.10126582278481],\n",
       "   [91.64556962025317,\n",
       "    93.41772151898734,\n",
       "    94.30379746835443,\n",
       "    94.43037974683544,\n",
       "    94.68354430379746,\n",
       "    95.0632911392405,\n",
       "    95.9493670886076,\n",
       "    95.82278481012658,\n",
       "    95.9493670886076,\n",
       "    96.32911392405063,\n",
       "    96.32911392405063,\n",
       "    96.45569620253164,\n",
       "    96.58227848101266,\n",
       "    96.58227848101266,\n",
       "    96.45569620253164,\n",
       "    96.83544303797468,\n",
       "    96.83544303797468,\n",
       "    97.34177215189874,\n",
       "    97.0886075949367,\n",
       "    97.0886075949367,\n",
       "    97.0886075949367,\n",
       "    96.9620253164557,\n",
       "    96.83544303797468,\n",
       "    96.83544303797468,\n",
       "    96.9620253164557,\n",
       "    97.46835443037975,\n",
       "    97.0886075949367,\n",
       "    97.0886075949367,\n",
       "    97.0886075949367,\n",
       "    97.0886075949367,\n",
       "    97.0886075949367,\n",
       "    97.34177215189874,\n",
       "    97.59493670886076,\n",
       "    97.84810126582279,\n",
       "    97.46835443037975,\n",
       "    97.72151898734177,\n",
       "    97.59493670886076,\n",
       "    97.34177215189874,\n",
       "    97.34177215189874,\n",
       "    97.59493670886076,\n",
       "    97.72151898734177,\n",
       "    97.72151898734177,\n",
       "    97.84810126582279,\n",
       "    97.72151898734177,\n",
       "    97.72151898734177,\n",
       "    97.84810126582279,\n",
       "    97.9746835443038,\n",
       "    97.9746835443038,\n",
       "    97.72151898734177,\n",
       "    98.35443037974683],\n",
       "   [91.26582278481013,\n",
       "    93.54430379746836,\n",
       "    94.17721518987342,\n",
       "    94.17721518987342,\n",
       "    95.31645569620254,\n",
       "    95.9493670886076,\n",
       "    96.07594936708861,\n",
       "    95.9493670886076,\n",
       "    96.20253164556962,\n",
       "    96.07594936708861,\n",
       "    96.45569620253164,\n",
       "    96.45569620253164,\n",
       "    96.58227848101266,\n",
       "    96.70886075949367,\n",
       "    96.45569620253164,\n",
       "    96.70886075949367,\n",
       "    97.0886075949367,\n",
       "    96.83544303797468,\n",
       "    97.0886075949367,\n",
       "    97.0886075949367,\n",
       "    97.0886075949367,\n",
       "    97.21518987341773,\n",
       "    96.9620253164557,\n",
       "    96.9620253164557,\n",
       "    97.0886075949367,\n",
       "    97.46835443037975,\n",
       "    97.0886075949367,\n",
       "    97.46835443037975,\n",
       "    97.0886075949367,\n",
       "    97.21518987341773,\n",
       "    97.21518987341773,\n",
       "    97.34177215189874,\n",
       "    97.34177215189874,\n",
       "    97.46835443037975,\n",
       "    97.21518987341773,\n",
       "    97.46835443037975,\n",
       "    97.59493670886076,\n",
       "    97.34177215189874,\n",
       "    98.10126582278481,\n",
       "    97.84810126582279,\n",
       "    97.46835443037975,\n",
       "    97.46835443037975,\n",
       "    97.84810126582279,\n",
       "    98.10126582278481,\n",
       "    98.10126582278481,\n",
       "    98.10126582278481,\n",
       "    97.84810126582279,\n",
       "    97.9746835443038,\n",
       "    97.9746835443038,\n",
       "    98.10126582278481],\n",
       "   [91.39240506329114,\n",
       "    93.79746835443038,\n",
       "    95.18987341772151,\n",
       "    94.68354430379746,\n",
       "    94.9367088607595,\n",
       "    95.9493670886076,\n",
       "    95.9493670886076,\n",
       "    96.20253164556962,\n",
       "    95.82278481012658,\n",
       "    96.45569620253164,\n",
       "    96.32911392405063,\n",
       "    96.9620253164557,\n",
       "    96.83544303797468,\n",
       "    97.0886075949367,\n",
       "    96.58227848101266,\n",
       "    96.70886075949367,\n",
       "    96.83544303797468,\n",
       "    97.0886075949367,\n",
       "    97.34177215189874,\n",
       "    97.46835443037975,\n",
       "    97.21518987341773,\n",
       "    97.72151898734177,\n",
       "    97.34177215189874,\n",
       "    97.0886075949367,\n",
       "    97.21518987341773,\n",
       "    97.46835443037975,\n",
       "    97.46835443037975,\n",
       "    97.59493670886076,\n",
       "    97.59493670886076,\n",
       "    97.0886075949367,\n",
       "    97.34177215189874,\n",
       "    97.59493670886076,\n",
       "    97.72151898734177,\n",
       "    97.84810126582279,\n",
       "    97.72151898734177,\n",
       "    97.72151898734177,\n",
       "    97.72151898734177,\n",
       "    97.9746835443038,\n",
       "    97.72151898734177,\n",
       "    98.22784810126582,\n",
       "    97.9746835443038,\n",
       "    97.84810126582279,\n",
       "    97.72151898734177,\n",
       "    97.84810126582279,\n",
       "    98.35443037974683,\n",
       "    98.35443037974683,\n",
       "    98.48101265822785,\n",
       "    98.35443037974683,\n",
       "    98.35443037974683,\n",
       "    98.35443037974683]],\n",
       "  'test_loss': [[0.3248507313132286,\n",
       "    0.26938695085570213,\n",
       "    0.24068555022403598,\n",
       "    0.21423552065342666,\n",
       "    0.1875092912260443,\n",
       "    0.17503183328211308,\n",
       "    0.1708953746955842,\n",
       "    0.15603411198332906,\n",
       "    0.1503832942403853,\n",
       "    0.14224967153863982,\n",
       "    0.13492291602641343,\n",
       "    0.1311779676907696,\n",
       "    0.13004700065515937,\n",
       "    0.12619001378882677,\n",
       "    0.12057543986053205,\n",
       "    0.11319402423305437,\n",
       "    0.10942779652155005,\n",
       "    0.11122455281428993,\n",
       "    0.10971589596783743,\n",
       "    0.10703674499662592,\n",
       "    0.10807169904476031,\n",
       "    0.10425751894423738,\n",
       "    0.10125029800022022,\n",
       "    0.102272339505516,\n",
       "    0.10315702347960323,\n",
       "    0.1006648329405114,\n",
       "    0.09759744305089116,\n",
       "    0.09810666757435538,\n",
       "    0.09581816454199142,\n",
       "    0.09270704038504045,\n",
       "    0.09506823366577737,\n",
       "    0.09577044622130924,\n",
       "    0.09192423030594364,\n",
       "    0.09275254190938431,\n",
       "    0.09569518436943764,\n",
       "    0.09428598449339624,\n",
       "    0.09536432437319309,\n",
       "    0.0918722216494847,\n",
       "    0.09285917443252402,\n",
       "    0.09118672710277606,\n",
       "    0.09229036936330959,\n",
       "    0.09046584906610661,\n",
       "    0.09047211318925256,\n",
       "    0.08916125207090518,\n",
       "    0.08827215108694508,\n",
       "    0.09031896461069118,\n",
       "    0.08939699692259309,\n",
       "    0.08895562065101695,\n",
       "    0.08844206988828082,\n",
       "    0.08853859550169436],\n",
       "   [0.3378390926718712,\n",
       "    0.27015439505651595,\n",
       "    0.23438065526634455,\n",
       "    0.20435790624320507,\n",
       "    0.1846977689607069,\n",
       "    0.17818727747872473,\n",
       "    0.15823081371635198,\n",
       "    0.14847528419215233,\n",
       "    0.14714101351089776,\n",
       "    0.13809985739551484,\n",
       "    0.1327034524600953,\n",
       "    0.12616634486261755,\n",
       "    0.12049020426645875,\n",
       "    0.12104798691868782,\n",
       "    0.11763765863059089,\n",
       "    0.11294484746847301,\n",
       "    0.11514405601341278,\n",
       "    0.11562071828767657,\n",
       "    0.11035499260537326,\n",
       "    0.10639673577295616,\n",
       "    0.10416167385359294,\n",
       "    0.10026604987005704,\n",
       "    0.09834364370261318,\n",
       "    0.10189288901016116,\n",
       "    0.09546575271436013,\n",
       "    0.09768088597382885,\n",
       "    0.09624367692358791,\n",
       "    0.09368889628297183,\n",
       "    0.09206409088219517,\n",
       "    0.09662170899047051,\n",
       "    0.09221084195063449,\n",
       "    0.09063672388730337,\n",
       "    0.08760858457685682,\n",
       "    0.08993145452290191,\n",
       "    0.09087997025350923,\n",
       "    0.09137658834097674,\n",
       "    0.09232257836603676,\n",
       "    0.09199552012407221,\n",
       "    0.09146882874309085,\n",
       "    0.08878430040418753,\n",
       "    0.09046005169376439,\n",
       "    0.0898899981983006,\n",
       "    0.08961511292082723,\n",
       "    0.08812638760274276,\n",
       "    0.08841065776580363,\n",
       "    0.08816046996219083,\n",
       "    0.08815371951684356,\n",
       "    0.08749346727096709,\n",
       "    0.08856375869590993,\n",
       "    0.08630373757877387],\n",
       "   [0.3364133652925491,\n",
       "    0.28720996180027725,\n",
       "    0.24630297811701893,\n",
       "    0.21841174987405537,\n",
       "    0.19406556394547225,\n",
       "    0.17658563824482262,\n",
       "    0.16407683480158447,\n",
       "    0.15877321874853223,\n",
       "    0.1454856861717999,\n",
       "    0.13792401563674211,\n",
       "    0.13178716602809729,\n",
       "    0.12774202365241943,\n",
       "    0.12561667932616546,\n",
       "    0.1234689930417575,\n",
       "    0.11745775944082998,\n",
       "    0.11075208172872662,\n",
       "    0.11013410570034757,\n",
       "    0.10556535214991308,\n",
       "    0.1017289143661037,\n",
       "    0.10354716144665145,\n",
       "    0.10330013719168492,\n",
       "    0.10368448696751148,\n",
       "    0.10029701158162206,\n",
       "    0.09830744606107474,\n",
       "    0.09922969191027806,\n",
       "    0.09584246370899491,\n",
       "    0.0936545808891533,\n",
       "    0.09540742920369376,\n",
       "    0.09243571980004199,\n",
       "    0.09082024742220528,\n",
       "    0.08912526300386525,\n",
       "    0.08925376634104178,\n",
       "    0.08722188892228296,\n",
       "    0.08788321343078279,\n",
       "    0.08540474108476191,\n",
       "    0.0863603653249098,\n",
       "    0.0865280753439758,\n",
       "    0.08519925678777508,\n",
       "    0.08368517119181343,\n",
       "    0.08627221408113837,\n",
       "    0.08520747105279006,\n",
       "    0.08418832890256309,\n",
       "    0.08515011391355656,\n",
       "    0.08436375266010873,\n",
       "    0.08360500783775933,\n",
       "    0.08423019349602982,\n",
       "    0.08330906575659755,\n",
       "    0.0848300243982696,\n",
       "    0.08179374065359589,\n",
       "    0.08215474168051733],\n",
       "   [0.34480443749427797,\n",
       "    0.28203559458255767,\n",
       "    0.24068483888804912,\n",
       "    0.206675897423923,\n",
       "    0.19118378508388997,\n",
       "    0.17356874076500536,\n",
       "    0.1622109260553494,\n",
       "    0.15594333327952772,\n",
       "    0.1518954072220251,\n",
       "    0.14312174407932907,\n",
       "    0.13929769882932305,\n",
       "    0.13272507113590837,\n",
       "    0.1283761634632945,\n",
       "    0.12558471842333674,\n",
       "    0.11878073824830353,\n",
       "    0.12141075409152545,\n",
       "    0.12066070839203893,\n",
       "    0.11782477997327223,\n",
       "    0.10959322171467356,\n",
       "    0.10294323925110511,\n",
       "    0.10453747095866128,\n",
       "    0.10659921883330681,\n",
       "    0.10406458996646106,\n",
       "    0.10343805713327602,\n",
       "    0.10311881358763203,\n",
       "    0.10033453854369,\n",
       "    0.09916481368970126,\n",
       "    0.09702365548105445,\n",
       "    0.0984536264252616,\n",
       "    0.09736492780497065,\n",
       "    0.09395526356180199,\n",
       "    0.09136535726762376,\n",
       "    0.09119851277526468,\n",
       "    0.09178806748264469,\n",
       "    0.09177599361487665,\n",
       "    0.09077089084356557,\n",
       "    0.08950518018931616,\n",
       "    0.0893087056821445,\n",
       "    0.08771191551724915,\n",
       "    0.08554093419255224,\n",
       "    0.08854552707234398,\n",
       "    0.08863555370150134,\n",
       "    0.08536816610745154,\n",
       "    0.08602034017682308,\n",
       "    0.08658916591089219,\n",
       "    0.0842900916997809,\n",
       "    0.08431673438705038,\n",
       "    0.08381074141580612,\n",
       "    0.08422235240179579,\n",
       "    0.08209299267845926],\n",
       "   [0.33804945058822633,\n",
       "    0.2717673426449299,\n",
       "    0.23206808126568795,\n",
       "    0.20711622412465513,\n",
       "    0.18537128895893693,\n",
       "    0.16975842800959945,\n",
       "    0.15795275962017477,\n",
       "    0.15249818964954465,\n",
       "    0.1435635506685823,\n",
       "    0.13816532347630708,\n",
       "    0.13216511832587421,\n",
       "    0.12828578463355078,\n",
       "    0.12184537582737394,\n",
       "    0.12097257600510493,\n",
       "    0.11621803740411997,\n",
       "    0.11128649039650336,\n",
       "    0.11395554696125909,\n",
       "    0.1074235015526414,\n",
       "    0.10636009645229205,\n",
       "    0.10480688233785332,\n",
       "    0.10570360252810643,\n",
       "    0.10521879832306877,\n",
       "    0.10571514374427497,\n",
       "    0.10782199932839721,\n",
       "    0.10166028150720521,\n",
       "    0.09867313798703253,\n",
       "    0.09918782191271894,\n",
       "    0.09885102113196627,\n",
       "    0.09725694205835461,\n",
       "    0.09654860583380796,\n",
       "    0.09379825498042628,\n",
       "    0.09498179306741804,\n",
       "    0.09237138985875062,\n",
       "    0.09209044973736163,\n",
       "    0.09290204809650313,\n",
       "    0.09072367244292982,\n",
       "    0.09140012271008455,\n",
       "    0.09037303081140853,\n",
       "    0.08899646681023296,\n",
       "    0.08566052319040755,\n",
       "    0.08693554995424929,\n",
       "    0.08667409702253062,\n",
       "    0.08593689153281739,\n",
       "    0.08652188242225675,\n",
       "    0.08531095936584752,\n",
       "    0.08838203190400963,\n",
       "    0.08512579736694752,\n",
       "    0.08422705688703572,\n",
       "    0.08406381597151631,\n",
       "    0.08446711352040584]],\n",
       "  'train_loss': [[0.2410788935219316,\n",
       "    0.08557262864812099,\n",
       "    0.06233777954071775,\n",
       "    0.04977783540471743,\n",
       "    0.04574994505007574,\n",
       "    0.044513546864858346,\n",
       "    0.0372596252997652,\n",
       "    0.03477108828317327,\n",
       "    0.031282074029931516,\n",
       "    0.03013050887540521,\n",
       "    0.03176170145904113,\n",
       "    0.025111832831803366,\n",
       "    0.023860617912463262,\n",
       "    0.02154939360981787,\n",
       "    0.030089047810307114,\n",
       "    0.025757208571473178,\n",
       "    0.019827378799828616,\n",
       "    0.01790720148162423,\n",
       "    0.021051451767462332,\n",
       "    0.021520780007458672,\n",
       "    0.019238617598919063,\n",
       "    0.016737186629115514,\n",
       "    0.019657718046980563,\n",
       "    0.015712817748876378,\n",
       "    0.016430054649510733,\n",
       "    0.016402427831691706,\n",
       "    0.0154135652621662,\n",
       "    0.015573537223607789,\n",
       "    0.015220113224290949,\n",
       "    0.013414749797349787,\n",
       "    0.012786110138799853,\n",
       "    0.014823203737529772,\n",
       "    0.013770227573579008,\n",
       "    0.011815968168068264,\n",
       "    0.014504901907887594,\n",
       "    0.01402592034844009,\n",
       "    0.011902296515404259,\n",
       "    0.01315123104913111,\n",
       "    0.010411600004630117,\n",
       "    0.011442587847131107,\n",
       "    0.012272445948437327,\n",
       "    0.015479009091528901,\n",
       "    0.011293684896002305,\n",
       "    0.009404652491537501,\n",
       "    0.0098482409429612,\n",
       "    0.007697901473837865,\n",
       "    0.009606832531934595,\n",
       "    0.009371015657487559,\n",
       "    0.010584821413282205,\n",
       "    0.007198007877819259],\n",
       "   [0.2957059248589333,\n",
       "    0.08163232661748043,\n",
       "    0.0689942211715376,\n",
       "    0.051816296571930556,\n",
       "    0.04798694579277919,\n",
       "    0.03846280776544275,\n",
       "    0.03889144717571283,\n",
       "    0.03867157198505629,\n",
       "    0.03204046296619905,\n",
       "    0.03182900719141344,\n",
       "    0.02924731327896446,\n",
       "    0.02656490178460505,\n",
       "    0.02651795025379806,\n",
       "    0.02453183452129648,\n",
       "    0.023304648936269965,\n",
       "    0.023333669488702753,\n",
       "    0.0173885013542919,\n",
       "    0.023246234342377388,\n",
       "    0.020866694684821295,\n",
       "    0.023406265946210168,\n",
       "    0.02035294836864379,\n",
       "    0.020442414948846965,\n",
       "    0.019053651509986898,\n",
       "    0.018058829715785273,\n",
       "    0.016002122251902506,\n",
       "    0.016976754210728,\n",
       "    0.019964346566102922,\n",
       "    0.014974143221365277,\n",
       "    0.01636067633110341,\n",
       "    0.014075020918279677,\n",
       "    0.014907721246159042,\n",
       "    0.013016415786455052,\n",
       "    0.0164032047794419,\n",
       "    0.014051633571862083,\n",
       "    0.012730048578348242,\n",
       "    0.013227577752211783,\n",
       "    0.008752209581122012,\n",
       "    0.01375376555406344,\n",
       "    0.01169747918580655,\n",
       "    0.010848800378897776,\n",
       "    0.009848843621074885,\n",
       "    0.012199633197866029,\n",
       "    0.010042531752205701,\n",
       "    0.011410485913479417,\n",
       "    0.011261970108865153,\n",
       "    0.0077404549163722055,\n",
       "    0.007148749153701522,\n",
       "    0.009439267474850185,\n",
       "    0.009181885191932271,\n",
       "    0.010953733155931279],\n",
       "   [0.23259293996364433,\n",
       "    0.08330657186465396,\n",
       "    0.06620805747575711,\n",
       "    0.051042850719964006,\n",
       "    0.04771066800881393,\n",
       "    0.03714997696289387,\n",
       "    0.04169191805367154,\n",
       "    0.03403636521697899,\n",
       "    0.035771637684458485,\n",
       "    0.032247934826023845,\n",
       "    0.03054713930464641,\n",
       "    0.023078429755696562,\n",
       "    0.023454717815703133,\n",
       "    0.023020964733696095,\n",
       "    0.027116603764331254,\n",
       "    0.022267408143780355,\n",
       "    0.01662228236830335,\n",
       "    0.01890167814260034,\n",
       "    0.022697398401308307,\n",
       "    0.017323938317859756,\n",
       "    0.021879856739088606,\n",
       "    0.014760875200223705,\n",
       "    0.019561845593415682,\n",
       "    0.016508722291884322,\n",
       "    0.015946788423035152,\n",
       "    0.017177057517437484,\n",
       "    0.013265080147117752,\n",
       "    0.013293330349658789,\n",
       "    0.017948417970338294,\n",
       "    0.01732562348297062,\n",
       "    0.015389822171220622,\n",
       "    0.010902415374701066,\n",
       "    0.013046266786999097,\n",
       "    0.01105872634650646,\n",
       "    0.013097673510117014,\n",
       "    0.013590252863711369,\n",
       "    0.01183432795504579,\n",
       "    0.013501695623255063,\n",
       "    0.015031338094749155,\n",
       "    0.013581857511346173,\n",
       "    0.012776705748987491,\n",
       "    0.008730183203001418,\n",
       "    0.011206925656953956,\n",
       "    0.010342528893972518,\n",
       "    0.00895556947836575,\n",
       "    0.00938157051152463,\n",
       "    0.00805560618431497,\n",
       "    0.012191626480529499,\n",
       "    0.009363701185599638,\n",
       "    0.008900775339434614],\n",
       "   [0.33141807561569536,\n",
       "    0.08425167137758156,\n",
       "    0.06617521242315708,\n",
       "    0.05216635924072033,\n",
       "    0.047477683519823885,\n",
       "    0.03879120023809719,\n",
       "    0.04091960062683545,\n",
       "    0.03383136275036653,\n",
       "    0.026630196498596437,\n",
       "    0.028221662896544864,\n",
       "    0.025859507186913256,\n",
       "    0.026986580061674426,\n",
       "    0.02461346207050618,\n",
       "    0.026377711803811127,\n",
       "    0.02650333353002543,\n",
       "    0.02233697441851973,\n",
       "    0.015611676283485018,\n",
       "    0.01995951405564745,\n",
       "    0.020867668452766247,\n",
       "    0.023044719595147947,\n",
       "    0.018747362087899413,\n",
       "    0.0153962155761347,\n",
       "    0.01664120974743389,\n",
       "    0.015030952911534959,\n",
       "    0.018503840167099155,\n",
       "    0.013978607239329222,\n",
       "    0.015439206747943343,\n",
       "    0.015430119514448542,\n",
       "    0.015558847812502797,\n",
       "    0.009951537723255455,\n",
       "    0.017177754376447824,\n",
       "    0.01601985799074322,\n",
       "    0.013470265939946801,\n",
       "    0.016372697052955538,\n",
       "    0.013368622537919442,\n",
       "    0.014939445295953285,\n",
       "    0.009409064141124763,\n",
       "    0.010583897889814637,\n",
       "    0.01342217015922387,\n",
       "    0.01198548036869924,\n",
       "    0.011275548731949043,\n",
       "    0.013610127967018276,\n",
       "    0.011866182341689216,\n",
       "    0.014102053116015997,\n",
       "    0.012782108267444215,\n",
       "    0.012955633828444632,\n",
       "    0.010583833162692343,\n",
       "    0.010122803952628838,\n",
       "    0.010549047070496956,\n",
       "    0.010445583249598059],\n",
       "   [0.27333599750023374,\n",
       "    0.09235973346964663,\n",
       "    0.06533252627275761,\n",
       "    0.051516743899288474,\n",
       "    0.04581749550524576,\n",
       "    0.0444112493907904,\n",
       "    0.037851520658551484,\n",
       "    0.03310655514005945,\n",
       "    0.03469178665632035,\n",
       "    0.03214298097211595,\n",
       "    0.027615682016886623,\n",
       "    0.02523532095991989,\n",
       "    0.025523533275461512,\n",
       "    0.027518455391886316,\n",
       "    0.025899777660009855,\n",
       "    0.022243719696645486,\n",
       "    0.017888909482827935,\n",
       "    0.02410549296254362,\n",
       "    0.02201417933782349,\n",
       "    0.021845791646786815,\n",
       "    0.018864377965986213,\n",
       "    0.01827760521834557,\n",
       "    0.01364725173554795,\n",
       "    0.01524510177881198,\n",
       "    0.01879119504911426,\n",
       "    0.016189613686891487,\n",
       "    0.018808818327036462,\n",
       "    0.012198692092802929,\n",
       "    0.01615688008902244,\n",
       "    0.012814700770084564,\n",
       "    0.013481516938818402,\n",
       "    0.012983896212974305,\n",
       "    0.013197453017238833,\n",
       "    0.013386874390380398,\n",
       "    0.011393725979628927,\n",
       "    0.014868303302113205,\n",
       "    0.011355884939193183,\n",
       "    0.01342402469952592,\n",
       "    0.011694059536550694,\n",
       "    0.011130787351041505,\n",
       "    0.013842849754329808,\n",
       "    0.010820415772194527,\n",
       "    0.011761904872034353,\n",
       "    0.01215684747865306,\n",
       "    0.012354453844997284,\n",
       "    0.007312754235402159,\n",
       "    0.009394225661980522,\n",
       "    0.008748332632341902,\n",
       "    0.01182704391124704,\n",
       "    0.009112540383307286]]},\n",
       " 'MNIST MLP on Non IID': {'hyperparams': {'C': 0.1,\n",
       "   'E': 10,\n",
       "   'K': 100,\n",
       "   'batch_size': 10,\n",
       "   'lr': 0.05,\n",
       "   'rounds': 50},\n",
       "  'test_accuracy': [[21.0126582278481,\n",
       "    30.88607594936709,\n",
       "    45.822784810126585,\n",
       "    53.54430379746835,\n",
       "    74.68354430379746,\n",
       "    80.0,\n",
       "    63.924050632911396,\n",
       "    81.39240506329114,\n",
       "    87.0886075949367,\n",
       "    83.67088607594937,\n",
       "    78.73417721518987,\n",
       "    87.34177215189874,\n",
       "    83.92405063291139,\n",
       "    90.50632911392405,\n",
       "    91.0126582278481,\n",
       "    87.9746835443038,\n",
       "    91.0126582278481,\n",
       "    90.63291139240506,\n",
       "    89.49367088607595,\n",
       "    90.88607594936708,\n",
       "    93.41772151898734,\n",
       "    86.20253164556962,\n",
       "    89.36708860759494,\n",
       "    93.92405063291139,\n",
       "    91.0126582278481,\n",
       "    91.64556962025317,\n",
       "    86.32911392405063,\n",
       "    86.20253164556962,\n",
       "    88.9873417721519,\n",
       "    92.27848101265823,\n",
       "    86.07594936708861,\n",
       "    91.26582278481013,\n",
       "    92.78481012658227,\n",
       "    94.17721518987342,\n",
       "    93.92405063291139,\n",
       "    93.79746835443038,\n",
       "    93.67088607594937,\n",
       "    93.92405063291139,\n",
       "    92.27848101265823,\n",
       "    94.17721518987342,\n",
       "    92.53164556962025,\n",
       "    93.54430379746836,\n",
       "    90.37974683544304,\n",
       "    92.53164556962025,\n",
       "    90.63291139240506,\n",
       "    94.55696202531645,\n",
       "    91.77215189873418,\n",
       "    95.56962025316456,\n",
       "    95.56962025316456,\n",
       "    94.68354430379746],\n",
       "   [46.32911392405063,\n",
       "    46.075949367088604,\n",
       "    48.9873417721519,\n",
       "    64.17721518987342,\n",
       "    59.74683544303797,\n",
       "    85.82278481012658,\n",
       "    67.84810126582279,\n",
       "    69.74683544303798,\n",
       "    77.59493670886076,\n",
       "    85.0632911392405,\n",
       "    83.41772151898734,\n",
       "    90.12658227848101,\n",
       "    87.9746835443038,\n",
       "    88.86075949367088,\n",
       "    89.62025316455696,\n",
       "    88.9873417721519,\n",
       "    89.24050632911393,\n",
       "    88.9873417721519,\n",
       "    88.9873417721519,\n",
       "    90.88607594936708,\n",
       "    88.60759493670886,\n",
       "    81.26582278481013,\n",
       "    91.89873417721519,\n",
       "    88.9873417721519,\n",
       "    92.9113924050633,\n",
       "    91.89873417721519,\n",
       "    91.51898734177215,\n",
       "    91.51898734177215,\n",
       "    92.9113924050633,\n",
       "    91.26582278481013,\n",
       "    94.0506329113924,\n",
       "    92.65822784810126,\n",
       "    95.0632911392405,\n",
       "    92.53164556962025,\n",
       "    94.9367088607595,\n",
       "    93.41772151898734,\n",
       "    90.63291139240506,\n",
       "    94.43037974683544,\n",
       "    94.0506329113924,\n",
       "    95.31645569620254,\n",
       "    95.44303797468355,\n",
       "    92.65822784810126,\n",
       "    91.89873417721519,\n",
       "    95.18987341772151,\n",
       "    93.41772151898734,\n",
       "    95.56962025316456,\n",
       "    95.31645569620254,\n",
       "    95.0632911392405,\n",
       "    91.89873417721519,\n",
       "    91.89873417721519],\n",
       "   [32.151898734177216,\n",
       "    40.88607594936709,\n",
       "    56.45569620253165,\n",
       "    61.265822784810126,\n",
       "    62.53164556962025,\n",
       "    70.12658227848101,\n",
       "    80.25316455696202,\n",
       "    83.0379746835443,\n",
       "    79.49367088607595,\n",
       "    86.9620253164557,\n",
       "    86.58227848101266,\n",
       "    89.11392405063292,\n",
       "    89.49367088607595,\n",
       "    83.41772151898734,\n",
       "    88.9873417721519,\n",
       "    86.07594936708861,\n",
       "    85.82278481012658,\n",
       "    89.49367088607595,\n",
       "    91.64556962025317,\n",
       "    86.70886075949367,\n",
       "    90.0,\n",
       "    90.12658227848101,\n",
       "    89.11392405063292,\n",
       "    86.70886075949367,\n",
       "    90.63291139240506,\n",
       "    87.59493670886076,\n",
       "    91.77215189873418,\n",
       "    91.64556962025317,\n",
       "    90.12658227848101,\n",
       "    91.0126582278481,\n",
       "    92.40506329113924,\n",
       "    90.88607594936708,\n",
       "    86.9620253164557,\n",
       "    89.11392405063292,\n",
       "    94.30379746835443,\n",
       "    93.67088607594937,\n",
       "    91.0126582278481,\n",
       "    92.78481012658227,\n",
       "    91.89873417721519,\n",
       "    88.60759493670886,\n",
       "    93.67088607594937,\n",
       "    88.10126582278481,\n",
       "    92.65822784810126,\n",
       "    93.29113924050633,\n",
       "    95.69620253164557,\n",
       "    92.40506329113924,\n",
       "    91.64556962025317,\n",
       "    94.9367088607595,\n",
       "    92.0253164556962,\n",
       "    88.10126582278481],\n",
       "   [33.41772151898734,\n",
       "    34.936708860759495,\n",
       "    55.69620253164557,\n",
       "    68.35443037974683,\n",
       "    58.48101265822785,\n",
       "    71.77215189873418,\n",
       "    72.78481012658227,\n",
       "    77.0886075949367,\n",
       "    81.26582278481013,\n",
       "    86.20253164556962,\n",
       "    73.16455696202532,\n",
       "    83.54430379746836,\n",
       "    88.10126582278481,\n",
       "    88.35443037974683,\n",
       "    84.9367088607595,\n",
       "    89.11392405063292,\n",
       "    89.24050632911393,\n",
       "    90.12658227848101,\n",
       "    90.12658227848101,\n",
       "    87.46835443037975,\n",
       "    84.43037974683544,\n",
       "    84.30379746835443,\n",
       "    90.12658227848101,\n",
       "    88.86075949367088,\n",
       "    84.81012658227849,\n",
       "    86.70886075949367,\n",
       "    86.32911392405063,\n",
       "    89.49367088607595,\n",
       "    93.79746835443038,\n",
       "    87.9746835443038,\n",
       "    91.39240506329114,\n",
       "    94.0506329113924,\n",
       "    94.30379746835443,\n",
       "    94.68354430379746,\n",
       "    93.0379746835443,\n",
       "    89.62025316455696,\n",
       "    92.78481012658227,\n",
       "    92.78481012658227,\n",
       "    92.53164556962025,\n",
       "    91.77215189873418,\n",
       "    91.77215189873418,\n",
       "    93.79746835443038,\n",
       "    92.53164556962025,\n",
       "    94.43037974683544,\n",
       "    94.17721518987342,\n",
       "    93.41772151898734,\n",
       "    93.16455696202532,\n",
       "    91.26582278481013,\n",
       "    95.31645569620254,\n",
       "    91.77215189873418],\n",
       "   [31.645569620253166,\n",
       "    27.848101265822784,\n",
       "    44.81012658227848,\n",
       "    65.69620253164557,\n",
       "    82.27848101265823,\n",
       "    83.67088607594937,\n",
       "    85.69620253164557,\n",
       "    83.67088607594937,\n",
       "    83.0379746835443,\n",
       "    86.9620253164557,\n",
       "    89.36708860759494,\n",
       "    88.35443037974683,\n",
       "    84.17721518987342,\n",
       "    88.48101265822785,\n",
       "    90.88607594936708,\n",
       "    89.87341772151899,\n",
       "    87.0886075949367,\n",
       "    91.39240506329114,\n",
       "    90.88607594936708,\n",
       "    87.0886075949367,\n",
       "    86.70886075949367,\n",
       "    87.9746835443038,\n",
       "    91.51898734177215,\n",
       "    92.27848101265823,\n",
       "    94.30379746835443,\n",
       "    91.64556962025317,\n",
       "    92.15189873417721,\n",
       "    92.27848101265823,\n",
       "    93.79746835443038,\n",
       "    92.53164556962025,\n",
       "    90.88607594936708,\n",
       "    93.41772151898734,\n",
       "    92.53164556962025,\n",
       "    91.13924050632912,\n",
       "    93.79746835443038,\n",
       "    94.17721518987342,\n",
       "    91.51898734177215,\n",
       "    94.55696202531645,\n",
       "    92.53164556962025,\n",
       "    92.27848101265823,\n",
       "    92.78481012658227,\n",
       "    96.07594936708861,\n",
       "    92.9113924050633,\n",
       "    95.82278481012658,\n",
       "    94.0506329113924,\n",
       "    95.82278481012658,\n",
       "    93.16455696202532,\n",
       "    93.29113924050633,\n",
       "    94.30379746835443,\n",
       "    95.18987341772151]],\n",
       "  'test_loss': [[3.2934228218078614,\n",
       "    2.5883201515197753,\n",
       "    1.6543202547073363,\n",
       "    1.7307588743209839,\n",
       "    1.0050747878074646,\n",
       "    0.8199475685119629,\n",
       "    1.1841038164138793,\n",
       "    0.5980448506355286,\n",
       "    0.4394598982095718,\n",
       "    0.5018466475486756,\n",
       "    0.5900130473613739,\n",
       "    0.3666877955675125,\n",
       "    0.451176312494278,\n",
       "    0.3137271153211594,\n",
       "    0.2964402710914612,\n",
       "    0.3583333957910538,\n",
       "    0.2952238154888153,\n",
       "    0.27823999969959257,\n",
       "    0.3067282650589943,\n",
       "    0.2756366723060608,\n",
       "    0.2170648746728897,\n",
       "    0.358918372297287,\n",
       "    0.30907208305597306,\n",
       "    0.21967991786003113,\n",
       "    0.2671801598787308,\n",
       "    0.2219459727048874,\n",
       "    0.3874765370368958,\n",
       "    0.40214405188560487,\n",
       "    0.29316137576103213,\n",
       "    0.24090889861583709,\n",
       "    0.3475607740402222,\n",
       "    0.24792579519748686,\n",
       "    0.1869011845946312,\n",
       "    0.16424763147830962,\n",
       "    0.166161401116848,\n",
       "    0.17367568697333335,\n",
       "    0.1963889586210251,\n",
       "    0.19992047786712647,\n",
       "    0.21501385318636895,\n",
       "    0.15972089519500732,\n",
       "    0.18353962387442588,\n",
       "    0.1900057356595993,\n",
       "    0.2507585160970688,\n",
       "    0.19397489385008812,\n",
       "    0.2333379539489746,\n",
       "    0.17197089093327522,\n",
       "    0.23727468529939652,\n",
       "    0.14628261949419974,\n",
       "    0.15935524304509163,\n",
       "    0.161716834628582],\n",
       "   [3.104198776626587,\n",
       "    2.2131814521789552,\n",
       "    1.9526010009765624,\n",
       "    1.2582466075897216,\n",
       "    1.3081710649490357,\n",
       "    0.5235606809139252,\n",
       "    0.8324925982475281,\n",
       "    0.9019473128795624,\n",
       "    0.671421819972992,\n",
       "    0.5093335064411163,\n",
       "    0.5534094200372696,\n",
       "    0.3533559451580048,\n",
       "    0.355283073425293,\n",
       "    0.3644638207912445,\n",
       "    0.3380949775457382,\n",
       "    0.30330966901779177,\n",
       "    0.2870009360313416,\n",
       "    0.35701698322296144,\n",
       "    0.3221560030937195,\n",
       "    0.27612020933628084,\n",
       "    0.3325274708747864,\n",
       "    0.4450624210238457,\n",
       "    0.23394998156428337,\n",
       "    0.2948323369562626,\n",
       "    0.20719558845758437,\n",
       "    0.2537402585625648,\n",
       "    0.24778999954462053,\n",
       "    0.26252483912706376,\n",
       "    0.21549111915826796,\n",
       "    0.23328598160743713,\n",
       "    0.17865390188694,\n",
       "    0.22750861656665802,\n",
       "    0.17562943359017372,\n",
       "    0.24499917552471162,\n",
       "    0.1752063353896141,\n",
       "    0.1931134439110756,\n",
       "    0.2619528482794762,\n",
       "    0.1915587815284729,\n",
       "    0.19135365169644356,\n",
       "    0.14953251929283143,\n",
       "    0.16134404458403587,\n",
       "    0.2393001701593399,\n",
       "    0.2575257083177567,\n",
       "    0.17389559469223023,\n",
       "    0.2225833116054535,\n",
       "    0.14633278616070747,\n",
       "    0.15843906297683716,\n",
       "    0.15972094242572785,\n",
       "    0.24578683668375015,\n",
       "    0.21309466970562935],\n",
       "   [3.2754574241638186,\n",
       "    2.1200834312438963,\n",
       "    1.780775852394104,\n",
       "    1.1973972043991088,\n",
       "    1.18878082447052,\n",
       "    1.0116556953430176,\n",
       "    0.5914692818164825,\n",
       "    0.5025621202468872,\n",
       "    0.594012400341034,\n",
       "    0.4210924189090729,\n",
       "    0.4257457583665848,\n",
       "    0.32507463791370395,\n",
       "    0.32090276041030885,\n",
       "    0.448854688167572,\n",
       "    0.3511755140066147,\n",
       "    0.39770171394348147,\n",
       "    0.4348321150779724,\n",
       "    0.2867581461429596,\n",
       "    0.24893247791528703,\n",
       "    0.45950159969329835,\n",
       "    0.311840358710289,\n",
       "    0.2578997296452522,\n",
       "    0.3259135175228119,\n",
       "    0.3600000165939331,\n",
       "    0.28212893664836886,\n",
       "    0.33780190024375917,\n",
       "    0.2295034856557846,\n",
       "    0.22817581133842468,\n",
       "    0.26061540888547896,\n",
       "    0.2532504912137985,\n",
       "    0.2287013495206833,\n",
       "    0.25862081367969514,\n",
       "    0.35653810744285586,\n",
       "    0.30697847480773927,\n",
       "    0.23098361518383026,\n",
       "    0.21940652000308036,\n",
       "    0.25069723707437513,\n",
       "    0.22660469297170638,\n",
       "    0.2356775574207306,\n",
       "    0.33698314793109896,\n",
       "    0.18731167744398117,\n",
       "    0.3316873684048653,\n",
       "    0.23339338363409043,\n",
       "    0.1882738183259964,\n",
       "    0.14842481055855752,\n",
       "    0.21919099254608154,\n",
       "    0.21534875062704087,\n",
       "    0.1713424399137497,\n",
       "    0.22686594871282578,\n",
       "    0.3813911184310913],\n",
       "   [3.308970509338379,\n",
       "    2.505068730545044,\n",
       "    1.7572049907684326,\n",
       "    1.1836989162445068,\n",
       "    1.2857654779434204,\n",
       "    0.8583740795135498,\n",
       "    0.8512718927383423,\n",
       "    0.8281086952209472,\n",
       "    0.5234473892211914,\n",
       "    0.45344197874069214,\n",
       "    0.7648207420349121,\n",
       "    0.4533322723388672,\n",
       "    0.4005232582092285,\n",
       "    0.39031220111846926,\n",
       "    0.4416619561910629,\n",
       "    0.33052233085632327,\n",
       "    0.3013895672559738,\n",
       "    0.28131382961273194,\n",
       "    0.2887169661045074,\n",
       "    0.3401607595443726,\n",
       "    0.43108431301116945,\n",
       "    0.41038278965950015,\n",
       "    0.30728992785215375,\n",
       "    0.31499539009332655,\n",
       "    0.456125572347641,\n",
       "    0.3972618703842163,\n",
       "    0.4159207398891449,\n",
       "    0.3045714326381683,\n",
       "    0.21827424252033234,\n",
       "    0.32188261787891387,\n",
       "    0.2338770180106163,\n",
       "    0.19020545583963394,\n",
       "    0.19224756795167924,\n",
       "    0.20783267359733582,\n",
       "    0.25352452650666235,\n",
       "    0.2959786410808563,\n",
       "    0.2353510598421097,\n",
       "    0.22451036076545716,\n",
       "    0.20790569393634797,\n",
       "    0.2504319306850433,\n",
       "    0.2225740165233612,\n",
       "    0.2075215801715851,\n",
       "    0.20867311223745347,\n",
       "    0.17873893680572508,\n",
       "    0.17396015136241913,\n",
       "    0.20321080160140992,\n",
       "    0.20435041296482087,\n",
       "    0.2283479024887085,\n",
       "    0.15387174677848817,\n",
       "    0.22540752828121186],\n",
       "   [3.1648748039245604,\n",
       "    2.3407725902557375,\n",
       "    2.303530979156494,\n",
       "    1.1153209789276124,\n",
       "    0.6551060503482818,\n",
       "    0.5610954285621643,\n",
       "    0.45835457220077513,\n",
       "    0.5465251795291901,\n",
       "    0.537635084104538,\n",
       "    0.4534230456829071,\n",
       "    0.36206395144462583,\n",
       "    0.3368954835891724,\n",
       "    0.45300154190063474,\n",
       "    0.35535158343315126,\n",
       "    0.30355994050502777,\n",
       "    0.35113229150772096,\n",
       "    0.36940209159851073,\n",
       "    0.27558853130340577,\n",
       "    0.2720876528143883,\n",
       "    0.3381537664413452,\n",
       "    0.38795585038661956,\n",
       "    0.3572600389480591,\n",
       "    0.2555208859682083,\n",
       "    0.22911678377389907,\n",
       "    0.21890819724798202,\n",
       "    0.24805773323774338,\n",
       "    0.23798713133335114,\n",
       "    0.24564072330594064,\n",
       "    0.21800484287142755,\n",
       "    0.2198715269625187,\n",
       "    0.30386200450658796,\n",
       "    0.2158077120065689,\n",
       "    0.23469958646297454,\n",
       "    0.24931953874230384,\n",
       "    0.19882868645787238,\n",
       "    0.18158389101028444,\n",
       "    0.2457328546643257,\n",
       "    0.18176837548613548,\n",
       "    0.22085472140312196,\n",
       "    0.21837033648490906,\n",
       "    0.21125697033405305,\n",
       "    0.16124889230728148,\n",
       "    0.21115974349528552,\n",
       "    0.1551307640314102,\n",
       "    0.18939165943861008,\n",
       "    0.14508258306384086,\n",
       "    0.2227524017572403,\n",
       "    0.22606216924190523,\n",
       "    0.17055404257178305,\n",
       "    0.14275769340097905]],\n",
       "  'train_loss': [[0.07849164038672843,\n",
       "    0.035129265798139866,\n",
       "    0.022288947235614545,\n",
       "    0.020539525970065132,\n",
       "    0.02260912629675938,\n",
       "    0.012345552147826228,\n",
       "    0.012937808392145861,\n",
       "    0.008305114190895645,\n",
       "    0.013250018742799796,\n",
       "    0.01502409664369217,\n",
       "    0.01816146199618988,\n",
       "    0.01155475257075242,\n",
       "    0.010071909363585959,\n",
       "    0.009029721698424524,\n",
       "    0.010704242372196526,\n",
       "    0.006561795007113534,\n",
       "    0.009173254736668997,\n",
       "    0.009591114757260693,\n",
       "    0.011983435651609146,\n",
       "    0.005358075002631936,\n",
       "    0.007224157363562059,\n",
       "    0.007220179650604766,\n",
       "    0.005252334215598781,\n",
       "    0.0055114141118895454,\n",
       "    0.006463987514542531,\n",
       "    0.00690999483204659,\n",
       "    0.007445866407396247,\n",
       "    0.007135131531686069,\n",
       "    0.007998875410038743,\n",
       "    0.0057244509871527925,\n",
       "    0.007871492998802071,\n",
       "    0.008244027400321628,\n",
       "    0.009346073610912154,\n",
       "    0.00707237325931293,\n",
       "    0.003911091650928246,\n",
       "    0.00918687744435382,\n",
       "    0.008562955414130793,\n",
       "    0.007184192235310774,\n",
       "    0.008463269582975571,\n",
       "    0.006245426560680404,\n",
       "    0.005928693657908464,\n",
       "    0.006881548342153864,\n",
       "    0.004383536948972635,\n",
       "    0.003856633374937362,\n",
       "    0.005925424402109188,\n",
       "    0.005018551911200474,\n",
       "    0.005204368829614691,\n",
       "    0.004321818733574834,\n",
       "    0.008869700615512207,\n",
       "    0.00386978484095733],\n",
       "   [0.06958786803917676,\n",
       "    0.02470357989465926,\n",
       "    0.021251031400570226,\n",
       "    0.017769601352782695,\n",
       "    0.012678251806242588,\n",
       "    0.020506467637607263,\n",
       "    0.02855264968196728,\n",
       "    0.012137351376851003,\n",
       "    0.013340723065243415,\n",
       "    0.01826449139260842,\n",
       "    0.01669557726363721,\n",
       "    0.009864344164859895,\n",
       "    0.014924684829463167,\n",
       "    0.011931922258004726,\n",
       "    0.006958099776486678,\n",
       "    0.00789624023803059,\n",
       "    0.012544142774465916,\n",
       "    0.00927072207099231,\n",
       "    0.007592123753886497,\n",
       "    0.005023066080082838,\n",
       "    0.007873161239928591,\n",
       "    0.005947530893644535,\n",
       "    0.007320872941400827,\n",
       "    0.006681173358179746,\n",
       "    0.01022248827597418,\n",
       "    0.004213469298232513,\n",
       "    0.006410865003049053,\n",
       "    0.009866064684181233,\n",
       "    0.006512320777528745,\n",
       "    0.007642124551877622,\n",
       "    0.008940555642346604,\n",
       "    0.004493298495761111,\n",
       "    0.0048691671787964,\n",
       "    0.005293456494095549,\n",
       "    0.006160472465220204,\n",
       "    0.008523373558358836,\n",
       "    0.006667946256868324,\n",
       "    0.00682989707709661,\n",
       "    0.004400096872786012,\n",
       "    0.0029169294288455662,\n",
       "    0.009192968572353629,\n",
       "    0.004034757602331757,\n",
       "    0.006994345621338964,\n",
       "    0.003707818468141512,\n",
       "    0.00348302662173932,\n",
       "    0.006942466044887854,\n",
       "    0.007250368287847584,\n",
       "    0.002534229435374365,\n",
       "    0.006534600067271938,\n",
       "    0.0042661438615996874],\n",
       "   [0.05498655917283439,\n",
       "    0.03269504073336925,\n",
       "    0.02145158759335509,\n",
       "    0.01986072657883206,\n",
       "    0.013640462848176752,\n",
       "    0.01692763821844582,\n",
       "    0.016297333874803005,\n",
       "    0.00902896719527519,\n",
       "    0.020003739516519216,\n",
       "    0.016498870469537887,\n",
       "    0.008827871000588343,\n",
       "    0.011788239716069451,\n",
       "    0.014170005319700602,\n",
       "    0.010210843964335014,\n",
       "    0.00781149294858525,\n",
       "    0.011437651654581,\n",
       "    0.006559530447494674,\n",
       "    0.00963950979047543,\n",
       "    0.004223994678651961,\n",
       "    0.012520260225687873,\n",
       "    0.007924965388330998,\n",
       "    0.004565335072508661,\n",
       "    0.005425011946495815,\n",
       "    0.010783616905258869,\n",
       "    0.007277262286093877,\n",
       "    0.005303688548727555,\n",
       "    0.004802053723070187,\n",
       "    0.009544713042385822,\n",
       "    0.0055080096422644356,\n",
       "    0.0040972675608925094,\n",
       "    0.006423869250336037,\n",
       "    0.006328774155202198,\n",
       "    0.008788954963531789,\n",
       "    0.004531542590614179,\n",
       "    0.010022778487552755,\n",
       "    0.005268771395540136,\n",
       "    0.004079612681883419,\n",
       "    0.006463209807858685,\n",
       "    0.009013022661149719,\n",
       "    0.0049054848143701615,\n",
       "    0.0048786287883649995,\n",
       "    0.0039000266838775445,\n",
       "    0.004514218367952069,\n",
       "    0.008454969228595389,\n",
       "    0.005783688695110202,\n",
       "    0.004887210210510656,\n",
       "    0.0030516848633787078,\n",
       "    0.005265571016564452,\n",
       "    0.004801554474047205,\n",
       "    0.006316082069808831],\n",
       "   [0.04892896394374005,\n",
       "    0.013825936676088913,\n",
       "    0.0351677234864181,\n",
       "    0.01935511833976488,\n",
       "    0.014419821701579802,\n",
       "    0.019978278428837686,\n",
       "    0.012442141424219302,\n",
       "    0.017720889237019644,\n",
       "    0.013689382443279724,\n",
       "    0.013535640531657575,\n",
       "    0.011420966036188526,\n",
       "    0.015994693474994694,\n",
       "    0.01104279824026387,\n",
       "    0.012947331622939786,\n",
       "    0.01276531049174393,\n",
       "    0.009149777815816626,\n",
       "    0.008530976222518642,\n",
       "    0.007510255961361257,\n",
       "    0.009086389408086752,\n",
       "    0.006530193967939073,\n",
       "    0.012124267971138521,\n",
       "    0.01012716842619036,\n",
       "    0.006478321979846491,\n",
       "    0.006165697227169543,\n",
       "    0.007656339481968685,\n",
       "    0.0060531354514483495,\n",
       "    0.0043424231121123435,\n",
       "    0.005431478484992151,\n",
       "    0.0050418905670992515,\n",
       "    0.008322959044522963,\n",
       "    0.006343967958405706,\n",
       "    0.008030856883433513,\n",
       "    0.0033204655153766414,\n",
       "    0.0033017625282681685,\n",
       "    0.00514094393148255,\n",
       "    0.005094051554850382,\n",
       "    0.00637865244230654,\n",
       "    0.009357100474817758,\n",
       "    0.005364934167312022,\n",
       "    0.0026887137556587557,\n",
       "    0.005088796711358934,\n",
       "    0.005718723422975275,\n",
       "    0.0032042850078556943,\n",
       "    0.00891871080261156,\n",
       "    0.0027519604856324174,\n",
       "    0.006710235801474652,\n",
       "    0.005522951495800754,\n",
       "    0.0038422240881114088,\n",
       "    0.004139424566815266,\n",
       "    0.005136927369609755],\n",
       "   [0.05641435122936754,\n",
       "    0.02865290003306937,\n",
       "    0.020282868059707894,\n",
       "    0.027142719664559705,\n",
       "    0.014245983322892092,\n",
       "    0.014983630331982255,\n",
       "    0.015741763793848274,\n",
       "    0.0064207360785119546,\n",
       "    0.016570640481139882,\n",
       "    0.010330967982860432,\n",
       "    0.018069994784076907,\n",
       "    0.010511986633181607,\n",
       "    0.008551083290662,\n",
       "    0.010286165485982806,\n",
       "    0.013938072491190634,\n",
       "    0.01137014585975043,\n",
       "    0.005639617235552596,\n",
       "    0.006819046078794898,\n",
       "    0.009749967967702956,\n",
       "    0.009307927959581713,\n",
       "    0.00940429622977138,\n",
       "    0.006335043915557924,\n",
       "    0.006073973135802656,\n",
       "    0.008674814195619536,\n",
       "    0.007960655264649084,\n",
       "    0.009344648969572593,\n",
       "    0.006009967908875298,\n",
       "    0.0045171294634073714,\n",
       "    0.005533735471463875,\n",
       "    0.007551562354767452,\n",
       "    0.010123100998834941,\n",
       "    0.007088836063800524,\n",
       "    0.006268455499504347,\n",
       "    0.005614955734808744,\n",
       "    0.0040643028452954835,\n",
       "    0.0073751165421613255,\n",
       "    0.005074329232023101,\n",
       "    0.00566927271439468,\n",
       "    0.005466788458550245,\n",
       "    0.006352010955827192,\n",
       "    0.0058628589922728085,\n",
       "    0.003675885776593843,\n",
       "    0.003139146286026978,\n",
       "    0.006660592808860817,\n",
       "    0.004562966566578746,\n",
       "    0.004080227491408328,\n",
       "    0.0031379643945482713,\n",
       "    0.005526595274126444,\n",
       "    0.007517831301989452,\n",
       "    0.006714473991224346]]}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "with open(path + 'Local_Round_FedAvg_10.pkl', 'rb') as file:\n",
    "  log_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "se_04JMaVJPg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99.62025316455696, 99.62025316455696, 99.49367088607595, 99.36708860759494, 99.36708860759494]\n"
     ]
    }
   ],
   "source": [
    "print([test_acc[-1] for test_acc in log_dict['MNIST CNN on IID']['test_accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "5sv0rv-G4Sqa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[98.9873417721519, 98.86075949367088, 98.86075949367088, 98.73417721518987, 98.60759493670886]\n"
     ]
    }
   ],
   "source": [
    "print([test_acc[-1] for test_acc in log_dict['MNIST CNN on Non IID']['test_accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "b2a2_Az-4nal"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[98.22784810126582, 98.10126582278481, 98.35443037974683, 98.10126582278481, 98.35443037974683]\n"
     ]
    }
   ],
   "source": [
    "print([test_acc[-1] for test_acc in log_dict['MNIST MLP on IID']['test_accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "YoRqZyvD4p2K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94.68354430379746, 91.89873417721519, 88.10126582278481, 91.77215189873418, 95.18987341772151]\n"
     ]
    }
   ],
   "source": [
    "print([test_acc[-1] for test_acc in log_dict['MNIST MLP on Non IID']['test_accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B87eGKJnAHIh"
   },
   "outputs": [],
   "source": [
    "output.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "FederatedAveraging",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "QSGD",
   "language": "python",
   "name": "qsgd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
